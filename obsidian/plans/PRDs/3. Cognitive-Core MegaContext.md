---
tags:
  - plans
  - prd
summary: Training and evaluation roadmap for the Cognitive-Core MegaContext model that externalizes knowledge into composable MegaContexts.
---
# Cognitive-Core MegaContext — Training and Evaluation Plan

The **Cognitive-Core MegaContext Language Model (C²MC-LLM)** extends the [[Cognitive Core]] concept by relying on a persistent, composable set of MegaContexts rather than memorizing facts in its weights. This plan covers training, inference, and evaluation requirements so the core model depends on external memory built with [[MegaContext]] infrastructure.

---

## 1. Motivation

Traditional LLMs embed knowledge directly into weights, which is costly to update, prone to drift, and memory-inefficient. C²MC-LLM externalizes knowledge into persistent [[MegaContext Tree|MegaContexts]] (MCs) built with [[GistNet]] summaries. Benefits include:
- **Instant updates:** refresh a Core Knowledge MC without touching model weights.
- **Smaller reasoning core:** concentrate parameters on abstraction and planning.
- **Integrated memory:** blend retrieval and focus via [[LensNet]] and the [[Focus Allocator]].

Once deployed, the model ships with a Core Knowledge MegaContext (CK-MC) and expects to operate with a composite MC that can also include:
- **CK-MC:** canonical factual and procedural knowledge.
- **Session-MC:** live conversation or task memory.
- **Recent-Events MC:** rolling summaries of news or sensor feeds.
- **Domain MCs:** organizational or application-specific corpora.
- **File-Watcher MC:** live-ingested files or repositories.

All MCs are fused into a single [[Working Context]] through focus operations.

---

## 2. Architecture Components

| Component | Role |
|-----------|------|
| [[GistNet]] | Summarizes token spans into LOD1/LOD2 gists for every MC. |
| [[LensNet]] | Scores spans for expansion or compression using ΔNLL supervision. |
| MegaContexts (MCs) | Persistent hierarchical trees (CK, Session, Recent Events, etc.). |
| [[Focus Allocator]] | Selects spans across MCs while maintaining budget and contiguity constraints. |
| Base model (core) | Transformer trained for next-token and gist prediction using [[MegaPrediction]] heads. |

---

## 3. Core Knowledge MegaContext (CK-MC)

1. **Dataset:** Curated factual corpus (~100M tokens) representing stable, high-value knowledge.
2. **Processing:** Chunk into 32-token spans, summarize via [[GistNet]], and optionally prune into a sparse hierarchy (LOD1/LOD2 anchors).
3. **Initial CK Working Context (CKWC):** Use [[LensNet]] + [[Focus Allocator]] to assemble a starter CKWC of size `C_L` (e.g., 4k–8k tokens). The CKWC is refocused per query as the composite WC evolves.

---

## 4. Training Initialization

| Module | Source | Frozen? |
|--------|--------|---------|
| Token embeddings | MC-LLM checkpoint | ✗ |
| [[GistNet]] | MC-LLM | ✗ (slow fine-tune) |
| [[LensNet]] | MC-LLM | ✗ (slow fine-tune) |
| Transformer trunk | Random | ✗ |
| Prediction heads (LOD0/LOD1) | Random | ✗ |

Rationale: retain existing compression/focus semantics while forcing the new trunk to depend on external MCs.

---

## 5. Training Loop

Each batch samples a full context `C` and constructs a **composite WC** mixing CK-MC and Session-MC spans under a shared budget `C₂`.

```
Full context C
 ├── Load CKWC (size C_L) and session context
 ├── Allocate joint CK+Session WC (length C₂)
 │    ├── CK spans = summarized (LOD1/LOD2)
 │    ├── Session spans = LOD0 + gists
 │    └── Focus Allocator enforces budget + diversity constraints
 ├── Evaluate base model
 │    ├── LOD0 NLL (token prediction)
 │    ├── LOD1/LOD2 regression (gist targets)
 │    └── Backprop through core + [[GistNet]]
 ├── LensNet refinement
 │    ├── Identify ideal WC(s) via regularized argmin
 │    └── Update focus scores using ΔNLL supervision
 └── CK reliance margin
      ├── Compute losses with CK spans dropped
      └── Enforce L_drop > L_full + margin m
```

This loop fits naturally within the JT1/JT2/JT3 cadence from [[Alternating Optimization]].

---

## 6. Composite Focus Allocator

### Objective
Maximize total utility while respecting the joint WC budget `C₂`:
```
maximize  Σ (v_i − penalty_i)  subject to Σ w_i ≤ C₂
```
where each span `i` has cost `w_i`, predicted benefit `v_i` (ΔNLL reduction), and penalty terms for hysteresis, redundancy, and legality.

### Implementation Notes
- **Utility estimates:** blend [[LensNet]] scores with ΔNLL EMA.
- **Diversity:** penalize over-selection from any single MC to avoid starvation.
- **Session floor:** reserve ≥50% of the budget for live session content.
- **Edit budget:** limit to four expand/collapse actions per iteration.
- **Deduplication:** remove identical WCs produced via different edit sequences.

---

## 7. Sparse MegaContexts

To scale beyond dense storage, use sparse anchor trees:

| Feature | Description |
|---------|-------------|
| Anchors | High-level LOD1/LOD2 summaries with domain and provenance metadata. |
| Progressive fill | Expand anchors lazily, reconstructing LOD0 on demand. |
| Cache | Store expanded spans (embeddings + KV) for reuse. |
| Consistency | Penalize drift between refreshed spans and stored anchors. |
| Versioning | Track checkpoint IDs so CK updates never invalidate history. |

---

## 8. Loss Terms

| Loss | Description |
|------|-------------|
| LOD0 NLL | Standard next-token loss enforced by MegaPrediction. |
| LOD1/LOD2 regression | Cosine/MSE between predicted and reference gists. |
| CK reliance margin | Drop-test ensuring CK spans materially affect loss. |
| Focus utility reward | Reward expansions that reduce ΔNLL. |
| Hysteresis penalty | Discourage focus oscillation across MCs. |

---

## 9. Validation and Evaluation

Compare three configurations under equal `C₂` budgets:

| Model | Context composition | MC access | Expected behavior |
|-------|---------------------|-----------|-------------------|
| LLM | Last `C₂` LOD0 tokens | None | Baseline without MegaContext. |
| MC-LLM | LOD0 + session gists | Session MC only | Compression-focused baseline. |
| C²MC-LLM | CK + session (and optional extra MCs) | Composite MC | Adaptive focus across knowledge tiers. |

Key metrics:
- Perplexity (ΔNLL) on CK-dependent and cross-MC spans.
- CK Reliance Index (CKRI) = `(L_drop − L_full) / L_full`.
- Focus attribution: proportion of expansions per MC.
- Expansion utility: % of focus edits that reduce loss.
- MC update responsiveness: accuracy delta immediately after a CK refresh.
- MC “poison” tests: robustness to conflicting or adversarial CK entries.

---

## 10. Practical Considerations

- Maintain ≥50% session share to ground reasoning in live context.
- Keep compute fair: all variants must share the same `C₂` budget and inference stack.
- Use Gaussian RoPE + ALiBi from [[Positional Encoding]] to align CK and session spans.
- Cache CKWC embeddings and KV tensors to amortize loading.
- Add safety hooks (e.g., contradiction detectors) to flag conflicts between prompt and CK content.
- Design the API so new MC types (Recent Events, File Watcher, etc.) plug into the same ingestion tooling.

---

## 11. Success Criteria

1. **CK reliance:** removing CK spans increases ΔNLL significantly.
2. **Instant adaptability:** CK updates improve performance without finetuning.
3. **Compute parity:** matches MC-LLM compute while delivering better factual accuracy.
4. **Externalized knowledge:** catastrophic drop without CK implies facts truly live in MegaContexts.
5. **Cross-MC reasoning:** model combines CK, session, and auxiliary MCs coherently.

---

## 12. Next Steps

1. Implement a composite [[Focus Allocator]] with multi-MC awareness.
2. Add sparse MC infrastructure, metadata, and caching.
3. Integrate the CK reliance margin and multi-MC supervision into the end-to-end loop.
4. Build a multi-MC evaluation harness comparing LLM vs. MC-LLM vs. C²MC-LLM.
5. Extend support for Recent Events, Domain Knowledge, and File Watcher MCs.

---
tags:
  - plans
  - prd
summary: Training plan for the MegaPrediction multi-LOD decoding head that forecasts gist vectors alongside standard token logits.
---
# MegaPrediction: Multi-LOD Decoding Head

**Scope:** Extend the end-to-end [[MegaContext]] loop with a continuous LOD1 prediction head (and optional LOD2) so the decoder can forecast gist vectors before emitting raw tokens.
**Goal:** Enrich the latent space with an explicit level-of-detail axis, enabling gist-first decoding, constrained search, and coarse-to-fine planning while preserving LOD0 accuracy.

Terminology: **LOD0** = raw tokens; **LOD1** = 32→1 gists; **LOD2** = 32 LOD1 gists (1024→1).
Assumes PyTorch and the existing [[MegaPrediction]] infrastructure.

---

## 1. Design Summary

We add a lightweight LOD1 head (and later an LOD2 head) on top of the shared transformer trunk. The new head predicts the continuous gist vector(s) corresponding to the next block(s) of tokens under teacher forcing:
- The **LOD0 head** (token logits) trains with next-token NLL.
- The **LOD1 head** trains with gist regression (cosine or MSE) over aligned horizons.
- The trunk receives both gradients with the LOD1 path initially under-weighted.

Why start with continuous gists?
- Mirrors how [[Working Context]] already carries [[GistNet]] embeddings.
- Avoids introducing a new Latentese vocabulary until the regression path stabilizes.
- Keeps inference compatible with existing token softmax heads and [[LoRA]] adapters.

---

## 2. Architecture Options

```
(a) Dual heads off shared trunk (baseline)
Trunk ──► Token LM head (tied unembedding)  ──► LOD0 NLL
   └──► LOD1 head (MLP/projection)           ──► LOD1 regression
```

```
(b) Last-k block specialization (if interference)
Trunk ──► Blocks[-k:]tok ─► Token head  ──► LOD0 NLL
   └──► Blocks[-k:]lod ─► LOD1 head    ──► LOD1 regression
```

```
(c) Adapter/LoRA gating (parameter-efficient)
Trunk ──► shared blocks + LOD-conditioned adapters ─► Heads
```

Recommended path:
- Begin with **(a)**.
- If LOD0 accuracy regresses, switch to **(b)** with `k = 1–2`.
- Use **(c)** for parameter-efficient experiments or to specialize per domain.

**LOD1 head (continuous):**
- **Input:** pooled hidden(s) aligned to the next 32-token block.
- **Body:** LayerNorm → small MLP (d→d or d→d/2→d with GELU/SiLU).
- **Output:** d-dimensional gist vector (same space as [[GistNet]] outputs).
- Respect positional semantics: when using Gaussian damping from [[Positional Encoding]], feed matching signals into the head.

---

## 3. Losses and Gradient Routing

Let `H` denote the teacher-forced horizon; choose `H % 32 == 0` for LOD1 blocks.

- **Token loss (LOD0):**
  `L_tok = mean_t NLL(y_t | WC)` over the final `H` positions.

- **LOD1 regression loss:**
  Partition the `H` targets into `K = H/32` blocks. For each block `b`:
  1. Ground truth gist `g_true[b] = GistNet(embed(tokens[b*32:(b+1)*32]))`.
  2. Predicted gist (two paths):
     - **Head-direct:** read trunk hidden(s) at the block start → LOD1 head → `g_pred_head[b]`.
     - **Soft-from-logits:** softmax over vocab for the 32 positions, form expected embeddings `E[p] @ W_emb`, and run [[GistNet]] → `g_pred_soft[b]`.
  3. Distance: `dist = 1 − cosine(g_pred, g_true)` (or MSE).
  4. `L_lod1 = mean_b dist`.

- **Consistency (optional):** encourage agreement between both prediction paths:
  `L_cons = mean_b dist(g_pred_head[b], g_pred_soft[b])`.

- **Routing:**
  - Token head: `L_tok` only.
  - LOD1 head: `L_lod1 (+ L_cons)` only.
  - Shared trunk: `L_tok + α1·L_lod1 (+ αc·L_cons)`.

Typical weights: `α1 ∈ [0.01, 0.05]`, `αc ∈ [0.005, 0.02]`.

**Stop-gradient warm-up:** For stability during the first few epochs, freeze either the [[GistNet]] targets or the trunk activations feeding the head (`detach()`), then unfreeze once losses settle.

---

## 4. Reading Positions for LOD1

To predict the **next** gist block during teacher forcing:
1. Build `[WC || H ground-truth tokens]`.
2. Run a single forward pass; compute losses only on the last `H` positions.
3. For each 32-token block `b`, read:
   - **Option A (default):** hidden state at the first token of block `b`.
   - **Option B:** mean/attention pool across the 32-token window before projection.

Option A is deterministic and sufficient; Option B is useful when the gist head underfits.

---

## 5. Integration with End-to-End Training

The MegaPrediction head plugs into **Step 4 (Evaluate Base + GistNet)** of the [[MegaContext: End-to-End Training Strategy|end-to-end plan]]:
1. Construct `[WC || H ground-truth tokens]`; forward once.
2. Compute `L_tok` on final `H` positions.
3. Compute `L_lod1` (and `L_cons`, `L_lod2` if enabled).
4. Backpropagate: the trunk receives the combined loss; heads receive their respective gradients.
5. [[LensNet]] supervision still uses ΔNLL; optionally log whether expansions reduced LOD1 error to guide focus rewards.

---

## 6. Inference Applications (Continuous Version)

Even before quantization, LOD1 predictions unlock new decoding modes:
1. **Gist-first speculative decoding:** predict `g_pred`, track the running soft gist during generation, and penalize token logits that diverge from the target until convergence.
2. **Constrained search / vocab pruning:** down-weight token clusters whose embeddings would push the gist away from `g_pred`, reducing compute.
3. **Coarse-to-fine beam pruning:** rank beams by gist agreement and prune early, then expand survivors with standard token decoding.

Quantizing the gist head later (Latentese v1) makes these pathways categorical and faster.

---

## 7. Hyperparameters and Schedules

- `H`: 32–64 initially, 128–256 after convergence; 1024 for optional LOD2.
- `α1`: anneal from 0.01 → 0.05 as regression stabilizes.
- `αc`: grow from 0 → 0.02 to blend soft/logit paths if needed.
- **Warm-up:** keep the gist target frozen for 1–3% of total steps or until `L_lod1` plateaus.
- **Last-k specialization:** enable only if `L_tok` regresses; start with `k = 1`.
- **Pooling choice:** stick with single-token reads unless regression underfits.

---

## 8. Failure Modes and Mitigations

| Failure mode | Mitigation |
|--------------|------------|
| LOD0 accuracy drops | Lower `α1`, enable last-k specialization, or balance gradients. |
| LOD1 collapse (low variance) | Add variance regularization or light dropout in the gist head. |
| LOD1 ↔ GistNet drift | Extend stop-grad warm-up or temporarily freeze [[GistNet]]. |
| Positional mismatch | Ensure the head receives the same Gaussian damping described in [[Positional Encoding]]. |

---

## 9. Minimal PyTorch Sketch (Packed Forward)

```python
# wc_ids: [W] token ids for the working context
# gt_ids: [H] ground-truth next tokens

wc_emb = model.embed_tokens(wc_ids)         # [W, d]
gt_emb = model.embed_tokens(gt_ids)         # [H, d]
seq_emb = torch.cat([wc_emb, gt_emb], dim=0)

logits, hiddens = model.forward_from_embeds(
    seq_emb, return_hiddens=True
)  # logits: [W+H, V], hiddens: [W+H, d]

logits_H = logits[-H:]
token_loss = F.cross_entropy(
    logits_H.view(-1, logits_H.size(-1)),
    gt_ids.view(-1)
)

assert H % 32 == 0
K = H // 32
block_starts = torch.arange(0, H, 32, device=logits.device)
abs_starts = (seq_emb.size(0) - H) + block_starts
h_reads = hiddens[abs_starts]              # [K, d]
g_pred = lod1_head(h_reads)                # [K, d]

with torch.no_grad():
    gt_blocks = gt_emb.view(K, 32, -1)
    g_true = torch.stack([gistnet(block) for block in gt_blocks], dim=0)

g_pred = F.normalize(g_pred, dim=-1)
g_true = F.normalize(g_true, dim=-1)
lod1_loss = (1.0 - (g_pred * g_true).sum(dim=-1)).mean()

total = token_loss + alpha1 * lod1_loss
total.backward()
optimizer.step()
optimizer.zero_grad()
```

---

## 10. Roadmap to “Latentese v1”

1. Train a VQ codebook (EMA or Gumbel-VQ) over LOD1 vectors.
2. Add a categorical head predicting code IDs; combine cross-entropy with continuous distillation.
3. Translate corpora into LOD1 tokens and train a Latentese LM for gist-only decoding.
4. Deploy categorical decoding for coarse-to-fine planning and faster inference.

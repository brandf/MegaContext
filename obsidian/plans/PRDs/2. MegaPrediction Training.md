# MegaPrediction: Multi‑LOD Decoding Head

**Scope:** Add a *continuous* LOD1 prediction path (and optional LOD2) to the existing end‑to‑end MegaContext training loop.
**Goal:** Enrich the latent space with an explicit *level‑of‑detail (LOD)* axis and unlock inference acceleration (gist‑first, constrained search) while preserving LOD0 token accuracy.

Terminology: **LOD0** = raw tokens; **LOD1** = 32→1 gists; **LOD2** = 32 LOD1 → 1 (1024→1).
Assumes PyTorch throughout.

---

## 1) Design summary

We add a lightweight **LOD1 head** (and later a LOD2 head) on top of the shared transformer trunk. The head predicts the **continuous gist vector(s)** for the next block(s) of tokens under teacher forcing. Losses are routed so:
- **LOD0 head** (token) trains with next‑token NLL.
- **LOD1 head** trains with gist regression (cosine or MSE) over aligned horizons.
- The **trunk** receives both gradients (LOD1 weighted small at first).

### Why continuous first?
- Matches how WCs already carry gists (continuous embeddings).
- No new vocabulary or unembedding required.
- Easier to stabilize; later we can add a VQ codebook (“Latentese”) for categorical LOD1 prediction.

---

## 2) Architecture options

```
(a) Dual heads off shared trunk (baseline)
Trunk ──► Token LM head (tied unembedding)  ──► LOD0 NLL
   └──► LOD1 head (MLP/proj)                 ──► LOD1 regression
```

```
(b) Last‑k block specialization (if interference)
Trunk ──► Blocks[-k:]_tok ─► Token head  ──► LOD0 NLL
   └──► Blocks[-k:]_lod ─► LOD1 head    ──► LOD1 regression
```

```
(c) Adapter/LoRA gating (parameter‑efficient)
Trunk ──► shared blocks + LOD‑conditioned adapters ─► Heads
```

**Start with (a)**. If LOD0 accuracy dips, switch to **(b)** with k=1–2.

**LOD1 head (continuous):**
- Input: pooled hidden(s) at positions aligned to the next 32 tokens (see §4).
- Body: LN → MLP(d→d) or small (d→d/2→d) with GELU/SiLU.
- Output: d‑dim gist vector(s) (same dimension as GistNet summaries).

> If using Gaussian RoPE (or any LOD positional damping), ensure LOD1 head sees the same damped position semantics as GistNet when constructing targets.

---

## 3) Losses & gradient routing

Let `H` be the horizon; choose `H % 32 == 0` for LOD1.

- **Token loss (LOD0):**
  `L_tok = mean_t NLL(y_t | WC)` over the last `H` positions (teacher forcing).

- **LOD1 regression loss (continuous):**
  Partition the `H` targets into K = `H/32` blocks of 32. For each block:
  - **Ground truth LOD1**: `g1_true[b] = GistNet(embed(tokens[b*32:(b+1)*32]))` (no grad into tokens; grad into GistNet if enabled).
  - **Predicted LOD1**: two equivalent ways:
    1) **Head‑direct**: read trunk hidden(s) at the block start (or pooled over the block window) → LOD1 head → `g1_pred[b]`.
    2) **Soft‑from‑logits**: take softmax over vocab for the 32 positions, form expected embeddings `E[p] @ W_emb`, run GistNet over that 32×d to get `g1_pred[b]`.
  - **Distance**: `dist = 1 − cosine(g1_pred[b], g1_true[b])` (or MSE).
  - `L_lod1 = mean_b dist`.

- **Optional consistency:** encourage agreement between both ways of predicting LOD1 (if you use both 1) and 2)):
  `L_cons = mean_b dist(g1_pred_head[b], g1_pred_soft[b])`.

- **Routing:**
  - Token head: `L_tok` only.
  - LOD1 head: `L_lod1 (+ L_cons)` only.
  - Trunk: `L_tok + α1 * L_lod1 (+ αc * L_cons)`.

**Typical weights:** `α1 ∈ [0.01, 0.05]`, `αc ∈ [0.005, 0.02]` to start.

**Stop‑grad warm‑up (stability):**
- For the first few K steps/epochs, compute `g1_true = GistNet(...)` under `torch.no_grad()` to freeze the target, or feed `trunk_out.detach()` into the LOD1 head. Unclamp later for full end‑to‑end.

PyTorch patterns:
```python
# Freeze GistNet target:
with torch.no_grad():
    g1_true = gistnet(ground_truth_embeds)  # no grad into GistNet

# Freeze trunk path into LOD1 head (early warm-up only):
g1_pred = lod1_head(trunk_hidden_for_block.detach())
```

---

## 4) Where does the LOD1 head “read” from?

We want the LOD1 head to predict the **next** block gist(s). During teacher forcing:
- Build one packed sequence: `[WC || H ground‑truth tokens]`.
- Run a single forward; compute losses only on the last H positions.
- For each 32‑token block `b`, read a consistent **query position**:
  - **Option A (simple):** use the **hidden at the first token** *of that block* (i.e., the token right *before* you predict that 32‑span). This is analogous to next‑token prediction but at the block scale.
  - **Option B (richer):** pool the hidden states over the block window (mean or attention‑pool) before projecting with the LOD1 head.

Either is fine; start with **Option A** for determinism.

---

## 5) Integration into the v3 training loop

This slots into **Step 4: Evaluate Base + GistNet** (no changes to Steps 1–3 or 5).

**Per evaluated WC:**
1. Construct `[WC || H gt tokens]`; forward once.
2. Compute `L_tok` on the last H positions.
3. Compute `L_lod1` over K = H/32 blocks (and `L_cons` if enabled).
4. Backprop: trunk gets `L_tok + α1 * L_lod1 (+ αc * L_cons)`. Heads get their respective losses.

**LensNet / ΔNLL:** unchanged. Optionally record whether **LOD1 error** dropped more when certain spans were expanded—this can be a tiny auxiliary reward to Lens focus.

---

## 6) Inference: practical benefits (continuous version)

Even without quantization you can use LOD1 predictions to accelerate or stabilize decoding:

1) **Gist‑first speculative decoding (blockwise):**
   - Predict `g1_pred` for the next 32 tokens.
   - During token decode, keep a running soft gist over the partial 32‑span and **penalize logits** that push the soft gist away from `g1_pred`. Accept tokens greedily/beam‑wise until the soft gist matches the target within a tolerance.

2) **Constrained search / vocab pruning:**
   - Use the LOD1 target to down‑weight token clusters whose embeddings diverge from the target gist.
   - Yields latency reductions with mild accuracy impact.

3) **Coarse‑to‑fine beam pruning:**
   - Rank beams via block‑level gist agreement; prune early, then let surviving beams expand.

(Quantized LOD1 later makes all of this snappier and more categorical.)

---

## 7) Hyperparameters & schedules

- **Horizon `H`**: 32–64 initially (then 128–256); ensure `H % 32 == 0` for LOD1 blocks. Add **H=1024** occasionally if you want LOD2 (costly).
- **Weights**: `α1=0.01→0.05` (anneal up), `αc=0→0.02`.
- **Warm‑up**: stop‑grad for 1–3% of total steps or until LOD1 loss stabilizes.
- **Last‑k specialization**: enable only if LOD0 dips; try `k=1` first.
- **Pooling** (if used): mean‑pool or single‑token read; keep it simple until needed.

---

## 8) Failure modes & mitigations

- **LOD0 accuracy drops** → lower `α1`, add last‑k specialization, or apply grad‑norm balancing on shared layers.
- **LOD1 collapse (low variance)** → add variance reg. on predicted gists; consider small dropout in the LOD1 head.
- **Unstable LOD1↔GistNet chase** → stop‑grad warm‑up; freeze GistNet for a phase; later unfreeze for end‑to‑end.
- **Position mismatch** → ensure the same positional damping (Gaussian RoPE) in both target construction and the LOD1 head’s path.

---

## 9) Minimal PyTorch sketch (training step, packed forward)

```python
# wc_ids: [W] token ids for the WC (use your mixed-LOD embed path if different)
# gt_ids: [H] ground-truth token ids
# model returns (logits, hiddens) for the concatenated sequence
wc_emb = model.embed_tokens(wc_ids)                # [W, d]
gt_emb = model.embed_tokens(gt_ids)                # [H, d]
seq_emb = torch.cat([wc_emb, gt_emb], dim=0)       # [W+H, d]

logits, hiddens = model.forward_from_embeds(seq_emb, return_hiddens=True)  # logits: [W+H, V], hiddens: [W+H, d]

# Token loss on last H positions
logits_H = logits[-H:]                             # [H, V]
L_tok = F.cross_entropy(logits_H.view(-1, logits_H.size(-1)), gt_ids.view(-1))

# LOD1 blocks
assert H % 32 == 0
K = H // 32
block_starts = torch.arange(0, H, 32, device=logits.device)
abs_starts  = (seq_emb.size(0) - H) + block_starts    # positions in packed seq
h_reads = hiddens[abs_starts]                         # [K, d]
g1_pred = lod1_head(h_reads)                          # [K, d]

# Ground-truth LOD1 (freeze target during warm-up if desired)
with torch.no_grad():
    gt_blocks = gt_emb.view(K, 32, -1)                # [K, 32, d]
    g1_true = torch.stack([gistnet(block) for block in gt_blocks], dim=0)  # [K, d]

# Cosine loss
g1_pred_n = F.normalize(g1_pred, dim=-1)
g1_true_n = F.normalize(g1_true, dim=-1)
L_lod1 = (1.0 - (g1_pred_n * g1_true_n).sum(dim=-1)).mean()

total = L_tok + alpha1 * L_lod1
total.backward()
optimizer.step(); optimizer.zero_grad()
```

---

## 10) Roadmap to “Latentese v1” (quantized LOD1)

- Learn a **VQ codebook** for LOD1 (EMA or Gumbel‑VQ).
- Add a **classification head** predicting code IDs; combine CE with the continuous regression (distillation).
- Translate corpora to **LOD1 tokens**; train a Latentese LM; iterate (v2, v3).
- Inference: categorical **block decoding**, coarse beam search, tight pruning.

---

## 11) What changes in the main E2E doc?

Only **Step 4** gains LOD1 losses; **LensNet** and ΔNLL remain as‑is.
Your repo should contain this file alongside the v3 training spec.

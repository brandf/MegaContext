i'm planning on doing some e2e training runs and ablations tomorrow, so what can we do to polish things up so that goes smoothly? how about you do a sweeping review of the project documentation and code. put the results in a md file under obsidian/reviews.
the analysis should include potential bugs, potential performance issues, feature and architectural suggestions, consistency and comment quality feedback, major gaps in testing (note i can only test locally on my laptop with mocked/cpu tests) etc. i'm mainly interested in the MC related stuff this project is about. we're basing it on nanochat, and we don't plan on making major changes to that, so keep the review focused on MC and the integration with nanochat. think about it from the perspective of a professional AI research who is starting to work in this project.  think about what would they need/expect out of a 10/10 AI project, and include a plan to get us there it in the review doc.
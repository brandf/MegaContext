model:
  hidden_size: 960
  block_size: 32
  num_heads: 15
  mlp_ratio: 4.0
  rope_base: 10000.0
training:
  batch_size: 2
  lr: 0.001
  device: cuda
  seed: 0
  run_name: gistnet-example
  window_tokens: 512
  phases:
    - name: pooling-pretrain
      objective: pooling_mse
      max_steps: 200
      lr: 0.001
    - name: delta-finetune
      objective: delta_nll
      max_steps: 200
      lr: 0.0005
  base_model:
    name: HuggingFaceTB/SmolLM2-360M-Instruct
    torch_dtype: auto
    device: cuda

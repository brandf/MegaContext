{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {
    "id": "7fb27b941602401d91542211134fc71a"
   },
   "source": [
    "# MegaContext Research Console\n",
    "\n",
    "Interactively prepare datasets, train GistNet with PyTorch Lightning, and capture experiment artifacts. Key docs: [GistNet](https://brandf.github.io/MegaContext/architecture/components/GistNet), [GistNet Training](https://brandf.github.io/MegaContext/architecture/components/GistNet%20Training), [Telemetry](https://brandf.github.io/MegaContext/ops/Telemetry), [Alternating Optimization](https://brandf.github.io/MegaContext/ops/Alternating%20Optimization).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xSTPy-psxIwA",
   "metadata": {
    "id": "xSTPy-psxIwA"
   },
   "source": [
    "## Quick Start\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad6bc6d",
   "metadata": {
    "cellView": "form",
    "collapsed": true,
    "id": "9ad6bc6d",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "repo_url = 'https://github.com/brandf/MegaContext.git'\n",
    "workspace = Path('/content/MegaContext')\n",
    "if not workspace.exists():\n",
    "    !git clone $repo_url $workspace\n",
    "else:\n",
    "    !git -C $workspace pull --ff-only\n",
    "%cd /content/MegaContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8173e5ba",
   "metadata": {},
   "source": [
    "### Mount google drive (only on colab, uncomment below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab1efac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1defa149",
   "metadata": {
    "id": "1defa149"
   },
   "source": [
    "### Setup Dependencies (PIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ygEM0yrdxIwB",
   "metadata": {
    "cellView": "form",
    "collapsed": true,
    "id": "ygEM0yrdxIwB",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from IPython import get_ipython\n",
    "\n",
    "try:\n",
    "    import ipywidgets as widgets  # type: ignore\n",
    "except ImportError:  # pragma: no cover\n",
    "    widgets = None\n",
    "\n",
    "print('Python executable:', sys.executable)\n",
    "shell = get_ipython()\n",
    "if shell is None:\n",
    "    raise RuntimeError('IPython shell not found; cannot run pip installs.')\n",
    "\n",
    "QUIET_PIP = os.environ.get('MEGACONTEXT_PIP_QUIET', '1').strip().lower() not in {'0', 'false', 'no'}\n",
    "\n",
    "def pip_install(args: str, *, quiet: bool | None = None) -> None:\n",
    "    effective_quiet = QUIET_PIP if quiet is None else quiet\n",
    "    extra_flags = ['--disable-pip-version-check']\n",
    "    if effective_quiet and all(flag not in args for flag in ('-q', '--quiet')):\n",
    "        extra_flags.insert(0, '--quiet')\n",
    "    shell.run_line_magic('pip', f\"{args} {' '.join(extra_flags)}\")\n",
    "\n",
    "def install_dependencies() -> None:\n",
    "    print('Installing MegaContext dependencies...')\n",
    "    for module_name in list(sys.modules):\n",
    "        if module_name.startswith('megacontext') or module_name.startswith('lightning'):\n",
    "            sys.modules.pop(module_name, None)\n",
    "    pip_install('install --upgrade pip setuptools wheel')\n",
    "\n",
    "    cuda_index = os.environ.get('PYTORCH_CUDA_INDEX', 'https://download.pytorch.org/whl/cu121')\n",
    "    pip_install(f'install torch torchvision torchaudio --index-url {cuda_index}')\n",
    "    pip_install('install lightning')\n",
    "    pip_install('install -e .[dev]')\n",
    "    pip_install('install rich>=13')\n",
    "    pip_install('install tensorboard')\n",
    "    # Auto-select widget version based on runtime (Colab vs JupyterLab).\n",
    "    def _running_in_colab() -> bool:\n",
    "        try:\n",
    "            import google.colab  # type: ignore\n",
    "        except ImportError:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    if _running_in_colab():\n",
    "        pip_install('install ipywidgets==7.7.1')\n",
    "    else:\n",
    "        pip_install('install ipywidgets>=8.1,<9')\n",
    "        pip_install('install jupyterlab-widgets>=3.0,<4')\n",
    "    print('Widget target:', 'Colab 7.x' if _running_in_colab() else 'JupyterLab 8.x')\n",
    "\n",
    "install_dependencies()\n",
    "importlib.invalidate_caches()\n",
    "\n",
    "src_path = workspace / 'src'\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.append(str(src_path))\n",
    "try:\n",
    "    import torch\n",
    "    import megacontext  # noqa: F401\n",
    "    import lightning  # noqa: F401\n",
    "except ModuleNotFoundError as exc:\n",
    "    print(f'Missing dependency ({exc}).')\n",
    "\n",
    "print('Torch version:', torch.__version__)\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cPnYvSKFxIwB",
   "metadata": {
    "id": "cPnYvSKFxIwB"
   },
   "source": [
    "## 0. Setup Console\n",
    "\n",
    "Configure experiment settings, storage, logging, and reproducibility before running the pipeline.\n",
    "print('Notebook bootstrap finished — widgets may require refresh if versions mismatch.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "L2sJyG9dxIwB",
   "metadata": {
    "cellView": "form",
    "collapsed": true,
    "id": "L2sJyG9dxIwB",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "import dataclasses\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import yaml\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from megacontext.notebook import (\n",
    "    MetricsTracker,\n",
    "    build_logger,\n",
    "    collect_environment_report,\n",
    "    format_dataset_summary,\n",
    "    format_training_config,\n",
    "    format_training_summary,\n",
    "    iter_config_sections,\n",
    "    render_environment_report,\n",
    "    save_experiment_summary,\n",
    ")\n",
    "from megacontext.gistnet import (\n",
    "    BaseModelSettings,\n",
    "    GistNetConfig,\n",
    "    GistNetTrainingConfig,\n",
    ")\n",
    "\n",
    "\n",
    "try:\n",
    "    import ipywidgets as widgets  # type: ignore\n",
    "except ImportError:  # pragma: no cover\n",
    "    widgets = None\n",
    "\n",
    "\n",
    "def _resolve_path(value):\n",
    "    if value in (None, \"\"):\n",
    "        return None\n",
    "    return Path(value).expanduser().resolve()\n",
    "\n",
    "\n",
    "def launch_setup_console() -> None:\n",
    "    global LOGGER_STATE\n",
    "    if widgets is None:\n",
    "        print('ipywidgets not available — falling back to defaults.')\n",
    "        config_root = Path('configs')\n",
    "        configs = sorted(config_root.glob('*.yaml'))\n",
    "        if not configs:\n",
    "            raise RuntimeError('No experiment configs found under `configs/`.')\n",
    "        selected_config = configs[0]\n",
    "        globals()['EXPERIMENT_CONFIG'] = selected_config\n",
    "        globals()['experiment_cfg'] = yaml.safe_load(selected_config.read_text(encoding='utf-8'))\n",
    "        env_report = collect_environment_report()\n",
    "        print(render_environment_report(env_report))\n",
    "        artifact_root = _resolve_path(os.environ.get('MEGACONTEXT_ARTIFACT_ROOT')) or (Path.cwd() / 'artifacts').resolve()\n",
    "        artifact_root.mkdir(parents=True, exist_ok=True)\n",
    "        globals()['ARTIFACT_ROOT'] = artifact_root\n",
    "        os.environ['MEGACONTEXT_ARTIFACT_ROOT'] = str(artifact_root)\n",
    "        LOGGER_STATE = {\n",
    "            'selection': os.environ.get('MEGACONTEXT_LOGGER', 'none'),\n",
    "            'project': os.environ.get('MEGACONTEXT_LOGGER_PROJECT', 'megacontext-poc'),\n",
    "            'run_name': os.environ.get('MEGACONTEXT_LOGGER_RUN', ''),\n",
    "        }\n",
    "        seed = int(os.environ.get('MEGACONTEXT_SEED', 42))\n",
    "        globals()['RUN_SEED'] = seed\n",
    "        print(f'Selected config: {selected_config}')\n",
    "        print(f'Artifact root: {artifact_root}')\n",
    "        print(f'Seed: {seed}')\n",
    "        return\n",
    "\n",
    "    config_root = Path('configs')\n",
    "    configs = sorted(config_root.glob('*.yaml'))\n",
    "    if not configs:\n",
    "        raise RuntimeError('No experiment configs found under `configs/`.')\n",
    "\n",
    "    selected_config = globals().get('EXPERIMENT_CONFIG', configs[0])\n",
    "    if isinstance(selected_config, str):\n",
    "        selected_config = Path(selected_config)\n",
    "    if selected_config not in configs:\n",
    "        selected_config = configs[0]\n",
    "\n",
    "    artifact_default = _resolve_path(os.environ.get('MEGACONTEXT_ARTIFACT_ROOT')) or (Path.cwd() / 'artifacts').resolve()\n",
    "    artifact_root = _resolve_path(globals().get('ARTIFACT_ROOT')) or artifact_default\n",
    "    artifact_root.mkdir(parents=True, exist_ok=True)\n",
    "    os.environ['MEGACONTEXT_ARTIFACT_ROOT'] = str(artifact_root)\n",
    "    globals()['ARTIFACT_ROOT'] = artifact_root\n",
    "\n",
    "    data_root_initial = _resolve_path(os.environ.get('MEGACONTEXT_DATA_ROOT'))\n",
    "\n",
    "    LOGGER_STATE = globals().get('LOGGER_STATE', {\n",
    "        'selection': 'none',\n",
    "        'project': 'megacontext-poc',\n",
    "        'run_name': '',\n",
    "    })\n",
    "    LOGGER_STATE = {\n",
    "        'selection': LOGGER_STATE.get('selection', 'none'),\n",
    "        'project': LOGGER_STATE.get('project', 'megacontext-poc'),\n",
    "        'run_name': LOGGER_STATE.get('run_name', ''),\n",
    "    }\n",
    "\n",
    "    training_preview = widgets.Output()\n",
    "    training_controls_box = widgets.VBox()\n",
    "\n",
    "    def _resolve_teacher_hidden_size() -> int | None:\n",
    "        dataset_result = globals().get('DATASET_RESULT')\n",
    "        if not dataset_result:\n",
    "            return None\n",
    "        splits = dataset_result.get('splits', {})\n",
    "        split_name = globals().get('SPLIT_NAME')\n",
    "        candidate = None\n",
    "        if split_name and split_name in splits:\n",
    "            candidate = splits[split_name]\n",
    "        elif splits:\n",
    "            first_key = next(iter(splits))\n",
    "            candidate = splits[first_key]\n",
    "        if not candidate:\n",
    "            return None\n",
    "        return candidate.get('teacher_hidden_size')\n",
    "\n",
    "    def _build_training_configs() -> tuple[GistNetConfig | None, GistNetTrainingConfig | None, str | None]:\n",
    "        experiment = globals().get('experiment_cfg')\n",
    "        if not experiment:\n",
    "            return None, None, 'No experiment configuration loaded.'\n",
    "        gistnet_cfg = experiment.get('gistnet', {})\n",
    "        if not gistnet_cfg:\n",
    "            return None, None, 'Experiment config missing `gistnet` section.'\n",
    "        model_dict = deepcopy(gistnet_cfg.get('model', {}))\n",
    "        if model_dict.get('hidden_size') == 'auto':\n",
    "            hidden_size = _resolve_teacher_hidden_size()\n",
    "            if hidden_size:\n",
    "                model_dict['hidden_size'] = hidden_size\n",
    "        try:\n",
    "            model_config = GistNetConfig(**model_dict)\n",
    "        except Exception as exc:  # pragma: no cover - validation feedback\n",
    "            return None, None, str(exc)\n",
    "        training_dict = deepcopy(gistnet_cfg.get('training', {}))\n",
    "        try:\n",
    "            training_config = GistNetTrainingConfig.from_dict(training_dict)\n",
    "        except Exception as exc:  # pragma: no cover - validation feedback\n",
    "            return model_config, None, str(exc)\n",
    "        if training_config.base_model is None:\n",
    "            base_cfg = experiment.get('base_model')\n",
    "            if base_cfg is not None:\n",
    "                training_config = dataclasses.replace(\n",
    "                    training_config, base_model=BaseModelSettings.from_dict(base_cfg)\n",
    "                )\n",
    "        return model_config, training_config, None\n",
    "\n",
    "    def _render_training_preview(message: str | None = None) -> None:\n",
    "        training_preview.clear_output()\n",
    "        error = globals().get('TRAINING_CONFIG_ERROR')\n",
    "        training_config = globals().get('TRAINING_CONFIG')\n",
    "        with training_preview:\n",
    "            if message:\n",
    "                display(Markdown(message))\n",
    "            elif error:\n",
    "                display(Markdown(f\"**Training config unavailable:** {error}\"))\n",
    "            elif training_config:\n",
    "                display(Markdown(format_training_config(training_config)))\n",
    "            else:\n",
    "                display(Markdown('Training configuration will populate after dataset prep.'))\n",
    "\n",
    "    def _rebuild_training_controls() -> None:\n",
    "        if widgets is None:\n",
    "            return\n",
    "        error = globals().get('TRAINING_CONFIG_ERROR')\n",
    "        training_config = globals().get('TRAINING_CONFIG')\n",
    "        model_config = globals().get('MODEL_CONFIG')\n",
    "        if error:\n",
    "            training_controls_box.children = (\n",
    "                widgets.HTML(\n",
    "                    '<b>Training configuration unavailable.</b><br>'\n",
    "                    'Resolve the configuration issues above or run dataset prep.'\n",
    "                ),\n",
    "            )\n",
    "            return\n",
    "        if training_config is None or model_config is None:\n",
    "            training_controls_box.children = (\n",
    "                widgets.HTML('Run dataset prep to populate training configuration.'),\n",
    "            )\n",
    "            return\n",
    "        batch_slider = widgets.IntSlider(\n",
    "            value=int(training_config.batch_size),\n",
    "            min=1,\n",
    "            max=max(1, training_config.batch_size * 2),\n",
    "            step=1,\n",
    "            description='Batch size:',\n",
    "        )\n",
    "        log_every_slider = widgets.IntSlider(\n",
    "            value=max(1, training_config.log_every_n_steps),\n",
    "            min=1,\n",
    "            max=max(1, training_config.log_every_n_steps * 2),\n",
    "            step=1,\n",
    "            description='Log every:',\n",
    "        )\n",
    "        phase_controls = []\n",
    "        phase_boxes = []\n",
    "        for phase in training_config.phases:\n",
    "            steps_slider = widgets.IntSlider(\n",
    "                value=int(phase.max_steps),\n",
    "                min=1,\n",
    "                max=max(int(phase.max_steps), 100),\n",
    "                step=1,\n",
    "                description='Steps',\n",
    "            )\n",
    "            lr_slider = widgets.FloatLogSlider(\n",
    "                value=float(phase.lr),\n",
    "                base=10,\n",
    "                min=-6,\n",
    "                max=0,\n",
    "                step=0.1,\n",
    "                description='LR',\n",
    "            )\n",
    "            window_slider = widgets.IntSlider(\n",
    "                value=int(phase.window_tokens),\n",
    "                min=int(model_config.block_size),\n",
    "                max=max(int(phase.window_tokens), int(model_config.block_size) * 64),\n",
    "                step=int(model_config.block_size),\n",
    "                description='Window',\n",
    "            )\n",
    "            phase_controls.append((phase, (steps_slider, lr_slider, window_slider)))\n",
    "            phase_boxes.append(\n",
    "                widgets.VBox([\n",
    "                    widgets.HTML(f'<b>{phase.name}</b> ({phase.objective})'),\n",
    "                    steps_slider,\n",
    "                    lr_slider,\n",
    "                    window_slider,\n",
    "                ])\n",
    "            )\n",
    "        apply_button = widgets.Button(description='Apply overrides', button_style='success')\n",
    "        overrides_output = widgets.Output()\n",
    "\n",
    "        def _apply_overrides(_):\n",
    "            current_config = globals().get('TRAINING_CONFIG')\n",
    "            if current_config is None:\n",
    "                return\n",
    "            phases = []\n",
    "            for base_phase, controls in phase_controls:\n",
    "                steps_slider, lr_slider, window_slider = controls\n",
    "                phases.append(\n",
    "                    dataclasses.replace(\n",
    "                        base_phase,\n",
    "                        max_steps=int(steps_slider.value),\n",
    "                        lr=float(lr_slider.value),\n",
    "                        window_tokens=int(window_slider.value),\n",
    "                    )\n",
    "                )\n",
    "            new_config = dataclasses.replace(\n",
    "                current_config,\n",
    "                batch_size=int(batch_slider.value),\n",
    "                log_every_n_steps=int(log_every_slider.value),\n",
    "                phases=tuple(phases),\n",
    "            )\n",
    "            globals()['TRAINING_CONFIG'] = new_config\n",
    "            globals()['TRAINING_CONFIG_ERROR'] = None\n",
    "            overrides_output.clear_output()\n",
    "            with overrides_output:\n",
    "                display(Markdown(format_training_config(new_config, heading='Updated Training Config')))\n",
    "            _render_training_preview()\n",
    "            update_summary('Applied training overrides.')\n",
    "\n",
    "        apply_button.on_click(_apply_overrides)\n",
    "        training_controls_box.children = tuple(\n",
    "            [batch_slider, log_every_slider]\n",
    "            + phase_boxes\n",
    "            + [apply_button, overrides_output]\n",
    "        )\n",
    "\n",
    "    def _reset_training_settings(status: str | None = None) -> None:\n",
    "        model_config, training_config, error = _build_training_configs()\n",
    "        globals()['MODEL_CONFIG'] = model_config\n",
    "        globals()['TRAINING_CONFIG'] = training_config\n",
    "        globals()['TRAINING_CONFIG_ERROR'] = error\n",
    "        _render_training_preview()\n",
    "        _rebuild_training_controls()\n",
    "        try:\n",
    "            update_summary(status)\n",
    "        except NameError:\n",
    "            pass\n",
    "\n",
    "    globals()['refresh_training_settings'] = _reset_training_settings\n",
    "\n",
    "\n",
    "    env_report = collect_environment_report()\n",
    "    env_output = widgets.Output()\n",
    "    with env_output:\n",
    "        display(Markdown(render_environment_report(env_report)))\n",
    "\n",
    "    seed_initial = int(globals().get('RUN_SEED', os.environ.get('MEGACONTEXT_SEED', 42)))\n",
    "    state = {\n",
    "        'artifact_root': str(artifact_root),\n",
    "        'data_root': str(data_root_initial) if data_root_initial else '',\n",
    "        'resume_checkpoint': globals().get('RESUME_CHECKPOINT'),\n",
    "        'seed': seed_initial,\n",
    "    }\n",
    "\n",
    "    config_dropdown = widgets.Dropdown(\n",
    "        options=[(cfg.stem, str(cfg)) for cfg in configs],\n",
    "        value=str(selected_config),\n",
    "        description='Experiment:',\n",
    "        layout=widgets.Layout(width='60%'),\n",
    "    )\n",
    "    config_sections_container = widgets.VBox()\n",
    "\n",
    "    def _render_config_sections(cfg):\n",
    "        sections = iter_config_sections(cfg)\n",
    "        if not sections:\n",
    "            config_sections_container.children = (\n",
    "                widgets.HTML(value='<em>No experiment configuration found.</em>'),\n",
    "            )\n",
    "            return\n",
    "        tab_children = []\n",
    "        for title, payload, docs_link in sections:\n",
    "            section_output = widgets.Output()\n",
    "            with section_output:\n",
    "                heading = f\"### {title}\"\n",
    "                if docs_link:\n",
    "                    heading = f\"{heading} ([docs]({docs_link}))\"\n",
    "                yaml_dump = yaml.safe_dump(payload, sort_keys=False)\n",
    "                display(Markdown(f\"{heading}\\n```yaml\\n{yaml_dump}\\n```\"))\n",
    "            tab_children.append(section_output)\n",
    "        tabs = widgets.Tab(children=tab_children)\n",
    "        for idx, (title, _payload, _docs_link) in enumerate(sections):\n",
    "            tabs.set_title(idx, title)\n",
    "        config_sections_container.children = (tabs,)\n",
    "\n",
    "    def _load_config(path: Path) -> None:\n",
    "        cfg = yaml.safe_load(path.read_text(encoding='utf-8'))\n",
    "        globals()['EXPERIMENT_CONFIG'] = path\n",
    "        globals()['experiment_cfg'] = cfg\n",
    "        _render_config_sections(cfg)\n",
    "        _reset_training_settings('Loaded experiment configuration.')\n",
    "\n",
    "\n",
    "    def update_summary(status: str | None = None) -> None:\n",
    "        summary_lines = [\n",
    "            f\"- **Config:** `{globals().get('EXPERIMENT_CONFIG')}`\",\n",
    "            f\"- **Artifact root:** `{state['artifact_root']}`\",\n",
    "        ]\n",
    "        if state['data_root']:\n",
    "            summary_lines.append(f\"- **Data root:** `{state['data_root']}`\")\n",
    "        else:\n",
    "            summary_lines.append('- **Data root:** config-relative')\n",
    "        summary_lines.extend([\n",
    "            f\"- **Logger:** {LOGGER_STATE.get('selection', 'none')}\",\n",
    "            f\"- **Resume checkpoint:** `{state['resume_checkpoint']}`\" if state['resume_checkpoint'] else '- **Resume checkpoint:** none',\n",
    "            f\"- **WANDB token:** {'set' if os.environ.get('WANDB_API_KEY') else 'not set'}\",\n",
    "            f\"- **HF token:** {'set' if os.environ.get('HUGGINGFACE_HUB_TOKEN') else 'not set'}\",\n",
    "            f\"- **Seed:** `{state['seed']}`\",\n",
    "        ])\n",
    "        training_error = globals().get('TRAINING_CONFIG_ERROR')\n",
    "        training_cfg = globals().get('TRAINING_CONFIG')\n",
    "        if training_cfg:\n",
    "            summary_lines.append(f'- **Batch size:** {training_cfg.batch_size}')\n",
    "            summary_lines.append(f'- **Training phases:** {len(training_cfg.phases)}')\n",
    "        elif training_error:\n",
    "            summary_lines.append(f'- **Training config:** {training_error}')\n",
    "        summary_output.clear_output()\n",
    "        with summary_output:\n",
    "            if status:\n",
    "                display(Markdown(status))\n",
    "            display(Markdown('\\n'.join(summary_lines)))\n",
    "\n",
    "    def _on_config_change(change):\n",
    "        if change['name'] != 'value':\n",
    "            return\n",
    "        _load_config(Path(change['new']))\n",
    "\n",
    "    config_dropdown.observe(_on_config_change, names='value')\n",
    "\n",
    "    wandb_input = widgets.Text(\n",
    "        value=os.environ.get('WANDB_API_KEY', ''),\n",
    "        description='W&B token:',\n",
    "        layout=widgets.Layout(width='70%'),\n",
    "        placeholder='Optional',\n",
    "    )\n",
    "    hf_input = widgets.Text(\n",
    "        value=os.environ.get('HUGGINGFACE_HUB_TOKEN', ''),\n",
    "        description='HF token:',\n",
    "        layout=widgets.Layout(width='70%'),\n",
    "        placeholder='Optional',\n",
    "    )\n",
    "\n",
    "    def _sync_wandb(change):\n",
    "        if change['name'] != 'value':\n",
    "            return\n",
    "        os.environ['WANDB_API_KEY'] = change['new'].strip()\n",
    "        update_summary()\n",
    "\n",
    "    def _sync_hf(change):\n",
    "        if change['name'] != 'value':\n",
    "            return\n",
    "        os.environ['HUGGINGFACE_HUB_TOKEN'] = change['new'].strip()\n",
    "        update_summary()\n",
    "\n",
    "    wandb_input.observe(_sync_wandb, names='value')\n",
    "    hf_input.observe(_sync_hf, names='value')\n",
    "\n",
    "    artifact_input = widgets.Text(\n",
    "        value=str(artifact_root),\n",
    "        description='Artifact root:',\n",
    "        layout=widgets.Layout(width='70%'),\n",
    "    )\n",
    "    artifact_apply = widgets.Button(description='Apply', button_style='info', icon='save')\n",
    "    artifact_message = widgets.Output()\n",
    "\n",
    "    data_input = widgets.Text(\n",
    "        value=str(data_root_initial) if data_root_initial else '',\n",
    "        description='Data root:',\n",
    "        layout=widgets.Layout(width='70%'),\n",
    "        placeholder='Optional — defaults to config-relative paths',\n",
    "    )\n",
    "    data_apply = widgets.Button(description='Apply', button_style='info', icon='save')\n",
    "    data_message = widgets.Output()\n",
    "\n",
    "    resume_status = widgets.HTML('Starting a new run.')\n",
    "    resume_dropdown = widgets.Dropdown(\n",
    "        description='Resume from:',\n",
    "        layout=widgets.Layout(width='70%'),\n",
    "    )\n",
    "\n",
    "    summary_output = widgets.Output()\n",
    "    _load_config(selected_config)\n",
    "\n",
    "    def refresh_resume_options() -> None:\n",
    "        runs_dir = Path(state['artifact_root']) / 'gistnet'\n",
    "        runs_dir.mkdir(parents=True, exist_ok=True)\n",
    "        globals()['RUNS_DIR'] = runs_dir\n",
    "        checkpoints = sorted(\n",
    "            runs_dir.glob('**/*.ckpt'),\n",
    "            key=lambda p: p.stat().st_mtime,\n",
    "            reverse=True,\n",
    "        )\n",
    "        options = [('Start fresh', '')]\n",
    "        for path in checkpoints:\n",
    "            try:\n",
    "                label = str(path.relative_to(Path(state['artifact_root'])))\n",
    "            except ValueError:\n",
    "                label = str(path)\n",
    "            options.append((label, str(path)))\n",
    "        current = str(state['resume_checkpoint']) if state['resume_checkpoint'] else ''\n",
    "        values = [value for _, value in options]\n",
    "        if current not in values:\n",
    "            current = ''\n",
    "        resume_dropdown.options = options\n",
    "        resume_dropdown.value = current\n",
    "        if current:\n",
    "            resume_status.value = f'Resuming from <code>{current}</code>'\n",
    "        else:\n",
    "            resume_status.value = 'Starting a new run.'\n",
    "\n",
    "    def _apply_artifact_path(_=None) -> None:\n",
    "        raw = artifact_input.value.strip()\n",
    "        if not raw:\n",
    "            artifact_input.value = state['artifact_root']\n",
    "            update_summary('Artifact root cannot be empty; keeping current path.')\n",
    "            return\n",
    "        new_path = Path(raw).expanduser().resolve()\n",
    "        if str(new_path) == state['artifact_root']:\n",
    "            return\n",
    "        new_path.mkdir(parents=True, exist_ok=True)\n",
    "        state['artifact_root'] = str(new_path)\n",
    "        globals()['ARTIFACT_ROOT'] = new_path\n",
    "        os.environ['MEGACONTEXT_ARTIFACT_ROOT'] = str(new_path)\n",
    "        with artifact_message:\n",
    "            artifact_message.clear_output()\n",
    "            display(Markdown(f'Artifacts stored under `{new_path}`.'))\n",
    "        refresh_resume_options()\n",
    "        update_summary('Artifact root updated.')\n",
    "\n",
    "    artifact_apply.on_click(_apply_artifact_path)\n",
    "\n",
    "    def _apply_data_path(_=None) -> None:\n",
    "        raw = data_input.value.strip()\n",
    "        if not raw:\n",
    "            os.environ.pop('MEGACONTEXT_DATA_ROOT', None)\n",
    "            state['data_root'] = ''\n",
    "            with data_message:\n",
    "                data_message.clear_output()\n",
    "                display(Markdown('Using dataset paths relative to the experiment config.'))\n",
    "            update_summary('Data root cleared.')\n",
    "            return\n",
    "        path = Path(raw).expanduser().resolve()\n",
    "        if str(path) == state['data_root']:\n",
    "            return\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "        os.environ['MEGACONTEXT_DATA_ROOT'] = str(path)\n",
    "        state['data_root'] = str(path)\n",
    "        with data_message:\n",
    "            data_message.clear_output()\n",
    "            display(Markdown(f'Dataset root override set to `{path}`.'))\n",
    "        update_summary('Data root updated.')\n",
    "\n",
    "    data_apply.on_click(_apply_data_path)\n",
    "\n",
    "    def _on_resume_change(change):\n",
    "        if change['name'] != 'value':\n",
    "            return\n",
    "        selected = change['new']\n",
    "        state['resume_checkpoint'] = Path(selected) if selected else None\n",
    "        globals()['RESUME_CHECKPOINT'] = state['resume_checkpoint']\n",
    "        if selected:\n",
    "            resume_status.value = f'Resuming from <code>{selected}</code>'\n",
    "        else:\n",
    "            resume_status.value = 'Starting a new run.'\n",
    "        update_summary()\n",
    "\n",
    "    resume_dropdown.observe(_on_resume_change, names='value')\n",
    "\n",
    "    logger_dropdown = widgets.Dropdown(\n",
    "        options=[('Disabled', 'none'), ('Weights & Biases', 'wandb')],\n",
    "        value=LOGGER_STATE['selection'],\n",
    "        description='Logger:',\n",
    "        layout=widgets.Layout(width='50%'),\n",
    "    )\n",
    "    project_text = widgets.Text(\n",
    "        value=LOGGER_STATE['project'],\n",
    "        description='Project:',\n",
    "        layout=widgets.Layout(width='50%'),\n",
    "    )\n",
    "    run_text = widgets.Text(\n",
    "        value=LOGGER_STATE['run_name'],\n",
    "        placeholder='auto',\n",
    "        description='Run name:',\n",
    "        layout=widgets.Layout(width='50%'),\n",
    "    )\n",
    "    logger_summary = widgets.HTML()\n",
    "\n",
    "    def update_logger_state(_=None):\n",
    "        LOGGER_STATE['selection'] = logger_dropdown.value\n",
    "        LOGGER_STATE['project'] = project_text.value.strip()\n",
    "        LOGGER_STATE['run_name'] = run_text.value.strip()\n",
    "        logger_summary.value = (\n",
    "            f\"<b>Logger:</b> {LOGGER_STATE['selection']}<br>\"\n",
    "            f\"<b>Project:</b> {LOGGER_STATE['project'] or '—'}<br>\"\n",
    "            f\"<b>Run name:</b> {LOGGER_STATE['run_name'] or 'auto'}\"\n",
    "        )\n",
    "        update_summary()\n",
    "\n",
    "    for widget in (logger_dropdown, project_text, run_text):\n",
    "        widget.observe(update_logger_state, names='value')\n",
    "    update_logger_state()\n",
    "\n",
    "    seed_input = widgets.IntText(\n",
    "        value=seed_initial,\n",
    "        description='Global seed:',\n",
    "        layout=widgets.Layout(width='30%'),\n",
    "    )\n",
    "    apply_button = widgets.Button(description='Apply settings', button_style='success', icon='check')\n",
    "\n",
    "    def apply_settings(_=None):\n",
    "        try:\n",
    "            seed = int(seed_input.value)\n",
    "        except (TypeError, ValueError):\n",
    "            seed_input.value = state['seed']\n",
    "            update_summary('Seed must be an integer; reverting to the previous value.')\n",
    "            return\n",
    "        state['seed'] = seed\n",
    "        globals()['RUN_SEED'] = seed\n",
    "        os.environ['MEGACONTEXT_SEED'] = str(seed)\n",
    "        random.seed(seed)\n",
    "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "        np.random.seed(seed)\n",
    "        try:\n",
    "            import torch\n",
    "            torch.manual_seed(seed)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.manual_seed_all(seed)\n",
    "                try:\n",
    "                    torch.backends.cudnn.deterministic = True\n",
    "                    torch.backends.cudnn.benchmark = False\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "        except ImportError:\n",
    "            pass\n",
    "        update_summary(f'Seeded runtime with `{seed}`.')\n",
    "\n",
    "    apply_button.on_click(apply_settings)\n",
    "\n",
    "    refresh_resume_options()\n",
    "    update_summary()\n",
    "\n",
    "    overview_box = widgets.VBox([env_output, summary_output])\n",
    "    experiment_box = widgets.VBox([config_dropdown, config_sections_container])\n",
    "    training_box = widgets.VBox([training_preview, training_controls_box])\n",
    "    storage_box = widgets.VBox([\n",
    "        widgets.HBox([artifact_input, artifact_apply]),\n",
    "        artifact_message,\n",
    "        widgets.HBox([data_input, data_apply]),\n",
    "        data_message,\n",
    "        resume_dropdown,\n",
    "        resume_status,\n",
    "    ])\n",
    "    logging_box = widgets.VBox([logger_dropdown, project_text, run_text, logger_summary])\n",
    "    credentials_box = widgets.VBox([wandb_input, hf_input])\n",
    "    reproducibility_box = widgets.VBox([seed_input, apply_button])\n",
    "\n",
    "    tabs = widgets.Tab(children=[overview_box, experiment_box, training_box, storage_box, logging_box, credentials_box, reproducibility_box])\n",
    "    titles = ['Overview', 'Experiment', 'Training', 'Storage', 'Logging', 'Credentials', 'Reproducibility']\n",
    "    for idx, title in enumerate(titles):\n",
    "        tabs.set_title(idx, title)\n",
    "\n",
    "    display(tabs)\n",
    "    apply_settings()\n",
    "    globals()['SETUP_CONSOLE'] = tabs\n",
    "    print('Setup console ready — adjust settings via the tabs above.')\n",
    "    return tabs\n",
    "\n",
    "\n",
    "console_widget = launch_setup_console()\n",
    "if console_widget is not None:\n",
    "    globals()['SETUP_CONSOLE'] = console_widget\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7623eae2785240b9bd12b16a66d81610",
   "metadata": {
    "id": "7623eae2785240b9bd12b16a66d81610"
   },
   "source": [
    "## 1. Dataset Preparation\n",
    "\n",
    "Runs `tools.prepare_dataset.prepare_dataset_from_config` with tqdm progress bars. Skip this if the shard already exists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fQBMZYf7xIwD",
   "metadata": {
    "cellView": "form",
    "collapsed": true,
    "id": "fQBMZYf7xIwD",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import subprocess\n",
    "import yaml\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def ensure_dataset_assets(config_path: Path) -> None:\n",
    "    raw_config = yaml.safe_load(config_path.read_text())\n",
    "    dataset_cfg = raw_config.get('dataset', raw_config)\n",
    "    base_dir = config_path.parent\n",
    "    splits = dataset_cfg.get('splits', {})\n",
    "    if isinstance(splits, dict):\n",
    "        iterable = splits.items()\n",
    "    else:\n",
    "        iterable = enumerate(splits or [])\n",
    "    missing_patterns = []\n",
    "    for split_name, split in tqdm(iterable, desc='Checking dataset assets'):\n",
    "        if not isinstance(split, dict):\n",
    "            continue\n",
    "        pattern = (base_dir / split['source']).expanduser()\n",
    "        matches = glob.glob(str(pattern), recursive=True)\n",
    "        if matches:\n",
    "            continue\n",
    "        if 'gutenberg' in str(pattern):\n",
    "            print('Downloading Gutenberg subset (one-time download)...')\n",
    "            script_path = Path('tools/download_gutenberg.sh')\n",
    "            if not script_path.exists():\n",
    "                raise FileNotFoundError(\n",
    "                    'Expected tools/download_gutenberg.sh to exist. '\n",
    "                    'Populate the Gutenberg corpus manually or restore the helper script.'\n",
    "                )\n",
    "            glob_str = str(pattern)\n",
    "            stop_idx = len(glob_str)\n",
    "            for token in ('*', '?', '['):\n",
    "                idx = glob_str.find(token)\n",
    "                if idx != -1:\n",
    "                    stop_idx = min(stop_idx, idx)\n",
    "            target_dir = Path(glob_str[:stop_idx]).resolve()\n",
    "            if target_dir.suffix:\n",
    "                target_dir = target_dir.parent\n",
    "            target_dir.mkdir(parents=True, exist_ok=True)\n",
    "            try:\n",
    "                subprocess.check_call(['bash', str(script_path), str(target_dir)])\n",
    "            except FileNotFoundError as err:\n",
    "                raise RuntimeError('`bash` not found while running Gutenberg download script.') from err\n",
    "            except subprocess.CalledProcessError as err:\n",
    "                raise RuntimeError(\n",
    "                    'Gutenberg download script failed. Inspect the output above or run \"bash tools/download_gutenberg.sh\" manually to diagnose.'\n",
    "                ) from err\n",
    "            matches = glob.glob(str(pattern), recursive=True)\n",
    "            if matches:\n",
    "                continue\n",
    "        missing_patterns.append((split_name, pattern))\n",
    "        print(f\"No files matched pattern {pattern}\")\n",
    "    if missing_patterns:\n",
    "        unresolved = '\\n - '.join(f\"{name}: {path}\" for name, path in missing_patterns)\n",
    "        raise FileNotFoundError(\n",
    "            'Dataset assets missing. Patterns without matches:\\n - ' + unresolved\n",
    "        )\n",
    "\n",
    "\n",
    "config_path_obj = EXPERIMENT_CONFIG if isinstance(EXPERIMENT_CONFIG, Path) else Path(EXPERIMENT_CONFIG)\n",
    "ensure_dataset_assets(config_path_obj)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdc8c89c7104fffa095e18ddfef8986",
   "metadata": {
    "cellView": "form",
    "collapsed": true,
    "id": "7cdc8c89c7104fffa095e18ddfef8986",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import yaml\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from megacontext.notebook import format_dataset_summary\n",
    "from tools.prepare_dataset import load_dataset_config, prepare_dataset_from_config\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except ImportError:  # pragma: no cover\n",
    "    tqdm = None\n",
    "\n",
    "CONFIG_MODEL = load_dataset_config(EXPERIMENT_CONFIG)\n",
    "\n",
    "\n",
    "def _resolve_dataset_path(split_name: str) -> Path:\n",
    "    split_cfg = CONFIG_MODEL.splits[split_name]\n",
    "    output_path_cfg = Path(split_cfg.output_path)\n",
    "    if output_path_cfg.is_absolute():\n",
    "        return output_path_cfg\n",
    "    base_dir = EXPERIMENT_CONFIG.parent\n",
    "    default_output = (base_dir / output_path_cfg).resolve()\n",
    "    data_root_override = os.environ.get('MEGACONTEXT_DATA_ROOT')\n",
    "    if data_root_override:\n",
    "        resolved_base = base_dir.resolve()\n",
    "        repo_root = (\n",
    "            resolved_base.parents[1]\n",
    "            if len(resolved_base.parents) >= 2\n",
    "            else resolved_base\n",
    "        )\n",
    "        try:\n",
    "            relative = default_output.relative_to(repo_root)\n",
    "        except ValueError:\n",
    "            relative = default_output.name\n",
    "        return (Path(data_root_override).expanduser().resolve() / relative).resolve()\n",
    "    return default_output\n",
    "\n",
    "\n",
    "expected_outputs = {name: _resolve_dataset_path(name) for name in CONFIG_MODEL.splits}\n",
    "metadata_path = CONFIG_MODEL.metadata_path()\n",
    "if metadata_path.suffix not in {'.yaml', '.yml'}:\n",
    "    metadata_path = metadata_path.with_suffix('.yaml')\n",
    "metadata_path = metadata_path.resolve()\n",
    "force_rebuild = os.environ.get('MEGACONTEXT_FORCE_DATA_REBUILD', '').lower() in {'1', 'true', 'yes'}\n",
    "missing_outputs = [name for name, path in expected_outputs.items() if not path.exists()]\n",
    "metadata_missing = not metadata_path.exists()\n",
    "\n",
    "if force_rebuild:\n",
    "    print('Forcing dataset rebuild because MEGACONTEXT_FORCE_DATA_REBUILD is set.')\n",
    "if missing_outputs:\n",
    "    print('Dataset outputs missing for splits: ' + ', '.join(missing_outputs))\n",
    "if metadata_missing and not (force_rebuild or missing_outputs):\n",
    "    print(f'Metadata not found at {metadata_path}; regenerating dataset summaries.')\n",
    "\n",
    "run_prepare = force_rebuild or bool(missing_outputs) or metadata_missing\n",
    "\n",
    "DATASET_RESULT = None\n",
    "if run_prepare:\n",
    "    progress = None\n",
    "    if tqdm is not None:\n",
    "        progress = tqdm(total=1, desc='Preparing dataset', leave=False, bar_format='{l_bar}{bar}| {elapsed}')\n",
    "    else:\n",
    "        print('Preparing dataset (tqdm unavailable)...')\n",
    "    try:\n",
    "        DATASET_RESULT = prepare_dataset_from_config(EXPERIMENT_CONFIG)\n",
    "    finally:\n",
    "        if tqdm is not None and progress is not None:\n",
    "            progress.update(1)\n",
    "            progress.close()\n",
    "else:\n",
    "    print('Dataset shards already present; skipping prepare_dataset_from_config.')\n",
    "    metadata = yaml.safe_load(metadata_path.read_text(encoding='utf-8')) or {}\n",
    "    DATASET_RESULT = {\n",
    "        'config_path': str(EXPERIMENT_CONFIG),\n",
    "        'metadata_path': str(metadata_path),\n",
    "        'teacher_dtype': metadata.get('teacher_dtype'),\n",
    "        'splits': metadata.get('splits', {}) or {},\n",
    "    }\n",
    "\n",
    "if not DATASET_RESULT['splits']:\n",
    "    print('Warning: dataset summaries are empty. Consider rebuilding the dataset.')\n",
    "\n",
    "display(Markdown(format_dataset_summary(DATASET_RESULT['splits'])))\n",
    "if callable(globals().get('refresh_training_settings')):\n",
    "    refresh_training_settings('Updated training configuration after dataset prep.')\n",
    "\n",
    "\n",
    "available_splits = list(CONFIG_MODEL.splits.keys())\n",
    "default_split = 'train' if 'train' in available_splits else available_splits[0]\n",
    "SPLIT_NAME = globals().get('SPLIT_NAME', default_split)\n",
    "if SPLIT_NAME not in available_splits:\n",
    "    SPLIT_NAME = default_split\n",
    "\n",
    "\n",
    "def _set_dataset_path(split_name: str) -> Path:\n",
    "    path = _resolve_dataset_path(split_name)\n",
    "    globals()['DATASET_PATH'] = path\n",
    "    DATASET_RESULT['dataset_path'] = str(path)\n",
    "    return path\n",
    "\n",
    "\n",
    "DATASET_PATH = _set_dataset_path(SPLIT_NAME)\n",
    "\n",
    "widgets_ready = widgets is not None\n",
    "\n",
    "if widgets_ready:\n",
    "    split_dropdown = widgets.Dropdown(\n",
    "        options=[(name, name) for name in available_splits],\n",
    "        value=SPLIT_NAME,\n",
    "        description='Split:',\n",
    "        layout=widgets.Layout(width='40%'),\n",
    "    )\n",
    "    split_message = widgets.Output()\n",
    "\n",
    "    def _sync_split(change):\n",
    "        if change.get('name') != 'value':\n",
    "            return\n",
    "        selected = change['new']\n",
    "        globals()['SPLIT_NAME'] = selected\n",
    "        path = _set_dataset_path(selected)\n",
    "        with split_message:\n",
    "            split_message.clear_output()\n",
    "            display(Markdown(f\"Using split **{selected}** → `{path}`\"))\n",
    "\n",
    "    split_dropdown.observe(_sync_split, names='value')\n",
    "    display(widgets.VBox([split_dropdown, split_message]))\n",
    "    with split_message:\n",
    "        split_message.clear_output()\n",
    "        display(Markdown(f\"Using split **{SPLIT_NAME}** → `{DATASET_PATH}`\"))\n",
    "else:\n",
    "    print(f\"Using split '{SPLIT_NAME}' at {DATASET_PATH}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b118ea5561624da68c537baed56e602f",
   "metadata": {
    "id": "b118ea5561624da68c537baed56e602f"
   },
   "source": [
    "### Sample Example\n",
    "\n",
    "Peek at the first prepared context to sanity-check tokens and horizons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938c804e27f84196a10c8828c723f798",
   "metadata": {
    "cellView": "form",
    "collapsed": true,
    "id": "938c804e27f84196a10c8828c723f798",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "import pyarrow.ipc as pa_ipc\n",
    "from pathlib import Path\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "if not Path(DATASET_PATH).exists():\n",
    "    missing_msg = (\n",
    "        f\"Dataset shard not found at {DATASET_PATH}. \"\n",
    "        \"Run the prep cell first.\"\n",
    "    )\n",
    "    raise FileNotFoundError(missing_msg)\n",
    "\n",
    "with pa_ipc.open_file(DATASET_PATH) as reader:\n",
    "    if reader.num_record_batches == 0:\n",
    "        print('Dataset is empty.')\n",
    "    else:\n",
    "        batch = reader.get_batch(0)\n",
    "        table_dict = batch.to_pydict()\n",
    "        context_col = 'context_input_ids'\n",
    "        future_col = 'future_input_ids'\n",
    "        missing = [col for col in (context_col, future_col) if col not in table_dict]\n",
    "        if missing:\n",
    "            display(Markdown(\n",
    "                f\"Dataset preview skipped; missing columns: {', '.join(missing)}\"\n",
    "            ))\n",
    "        else:\n",
    "            context_tokens = table_dict[context_col][0][:32]\n",
    "            future_tokens = table_dict[future_col][0][:16]\n",
    "            summary_html = (\n",
    "                f\"Context tokens (first 32): {context_tokens}<br><br>\"\n",
    "                f\"Future tokens (first 16): {future_tokens}\"\n",
    "            )\n",
    "            display(Markdown(summary_html))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8309879909854d7188b41380fd92a7c3",
   "metadata": {
    "id": "8309879909854d7188b41380fd92a7c3"
   },
   "source": [
    "## 2. GistNet Training\n",
    "\n",
    "Two training phases run back-to-back:\n",
    "\n",
    "- **Phase 1 – pooling MSE:** GistNet sees teacher hidden states for each 32-token block and learns to reproduce the block mean. This bootstraps the compression stack without querying the base LLM.\n",
    "- **Phase 2 – ΔNLL distillation:** GistNet replaces those 32-token blocks with a single gist, feeds the mixed gist/token context through the frozen base model, and minimizes the increase in negative log-likelihood. Lower ΔNLL means the compressed gist is substitutable for the original span.\n",
    "\n",
    "Check the live plot: the dashed 0.1 line marks the ΔNLL target for acceptable substitutability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed186c9a28b402fb0bc4494df01f08d",
   "metadata": {
    "cellView": "form",
    "collapsed": true,
    "id": "3ed186c9a28b402fb0bc4494df01f08d",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython.display import display\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, RichProgressBar  # type: ignore\n",
    "import torch\n",
    "\n",
    "from megacontext.gistnet import build_gistnet_experiment\n",
    "from megacontext.notebook import MetricsTracker\n",
    "\n",
    "print('Configured torch float32 matmul precision to high for Tensor Cores.')\n",
    "torch.set_float32_matmul_precision('high')\n",
    "try:\n",
    "    import ipywidgets as widgets  # type: ignore\n",
    "except ImportError:\n",
    "    widgets = None\n",
    "\n",
    "resume_path = globals().get('RESUME_CHECKPOINT')\n",
    "if isinstance(resume_path, str) and not resume_path:\n",
    "    resume_path = None\n",
    "if resume_path is not None and not isinstance(resume_path, Path):\n",
    "    resume_path = Path(resume_path)\n",
    "if resume_path is not None and not resume_path.exists():\n",
    "    raise FileNotFoundError(f'Resume checkpoint not found: {resume_path}')\n",
    "\n",
    "if resume_path:\n",
    "    print(f'Resuming from checkpoint: {resume_path}')\n",
    "else:\n",
    "    print('Starting a new training run.')\n",
    "\n",
    "live_metrics_output = None\n",
    "if widgets is not None:\n",
    "    try:\n",
    "        live_metrics_output = widgets.Output()\n",
    "        display(live_metrics_output)\n",
    "    except Exception as exc:\n",
    "        print('ipywidgets output unavailable; falling back to static plotting.', exc)\n",
    "        live_metrics_output = None\n",
    "\n",
    "plot_every = max(1, TRAINING_CONFIG.log_every_n_steps)\n",
    "metrics_callback = MetricsTracker(\n",
    "    metric_keys=(\n",
    "        'train/loss',\n",
    "        'train/delta_loss',\n",
    "        'train/gist_loss',\n",
    "        'train/baseline_loss',\n",
    "    ),\n",
    "    live_output=live_metrics_output,\n",
    "    plot_every=plot_every,\n",
    "    metric_labels={\n",
    "        'train/loss': 'ΔNLL',\n",
    "        'train/delta_loss': 'ΔNLL (explicit)',\n",
    "        'train/gist_loss': 'Gist loss',\n",
    "        'train/baseline_loss': 'Baseline loss',\n",
    "    },\n",
    "    reference_lines={\n",
    "        'train/loss': 0.01,\n",
    "        'train/delta_loss': 0.01,\n",
    "    },\n",
    "    y_label='Loss (nats)',\n",
    ")\n",
    "freeze_after_phase = len(TRAINING_CONFIG.phases) > 1\n",
    "\n",
    "RUNS_DIR = globals().get('RUNS_DIR', (ARTIFACT_ROOT / 'gistnet').resolve())\n",
    "RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "run_name = (LOGGER_STATE.get('run_name') or '').strip()\n",
    "if not run_name:\n",
    "    run_name = f\"{EXPERIMENT_CONFIG.stem}-{datetime.now():%Y%m%d-%H%M%S}\"\n",
    "run_name = run_name.replace(' ', '-')\n",
    "RUN_NAME = run_name\n",
    "RUN_DIR = (RUNS_DIR / run_name).resolve()\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_DIR = (RUN_DIR / 'checkpoints').resolve()\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f'Artifacts for run `{RUN_NAME}` stored under {RUN_DIR}')\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=str(CHECKPOINT_DIR),\n",
    "    filename='step-{step:06d}',\n",
    "    monitor='train/loss',\n",
    "    mode='min',\n",
    "    save_last=True,\n",
    "    save_top_k=3,\n",
    "    every_n_train_steps=max(1, TRAINING_CONFIG.log_every_n_steps),\n",
    "    auto_insert_metric_name=False,\n",
    ")\n",
    "\n",
    "callbacks = [metrics_callback, RichProgressBar(), checkpoint_callback]\n",
    "LOGGER = None\n",
    "try:\n",
    "    LOGGER = build_logger(\n",
    "        selection=LOGGER_STATE.get('selection', 'none'),\n",
    "        project=LOGGER_STATE.get('project'),\n",
    "        run_name=LOGGER_STATE.get('run_name'),\n",
    "        config={'config_path': str(EXPERIMENT_CONFIG)},\n",
    "    )\n",
    "except RuntimeError as exc:\n",
    "    print(exc)\n",
    "\n",
    "trainer_kwargs = {\n",
    "    'accelerator': 'auto',\n",
    "    'devices': 1,\n",
    "    'default_root_dir': str(RUN_DIR),\n",
    "    'enable_model_summary': False,\n",
    "}\n",
    "\n",
    "TRAINER, MODULE, DATA_MODULE = build_gistnet_experiment(\n",
    "    dataset_path=DATASET_PATH,\n",
    "    model_config=MODEL_CONFIG,\n",
    "    training=TRAINING_CONFIG,\n",
    "    callbacks=callbacks,\n",
    "    logger=LOGGER,\n",
    "    trainer_kwargs=trainer_kwargs,\n",
    ")\n",
    "\n",
    "globals()['TRAINER'] = TRAINER\n",
    "globals()['MODULE'] = MODULE\n",
    "globals()['DATA_MODULE'] = DATA_MODULE\n",
    "globals()['RUN_DIR'] = RUN_DIR\n",
    "globals()['CHECKPOINT_DIR'] = CHECKPOINT_DIR\n",
    "globals()['RUN_NAME'] = RUN_NAME\n",
    "\n",
    "TRAINER.fit(MODULE, DATA_MODULE, ckpt_path=str(resume_path) if resume_path else None)\n",
    "\n",
    "best_model_path = getattr(TRAINER.checkpoint_callback, 'best_model_path', None) or ''\n",
    "if best_model_path:\n",
    "    best_path = Path(best_model_path)\n",
    "else:\n",
    "    best_path = None\n",
    "last_model_path = getattr(TRAINER.checkpoint_callback, 'last_model_path', None) or ''\n",
    "if last_model_path:\n",
    "    last_path = Path(last_model_path)\n",
    "else:\n",
    "    last_path = None\n",
    "globals()['LATEST_CHECKPOINT'] = best_path\n",
    "globals()['LAST_CHECKPOINT'] = last_path\n",
    "\n",
    "if live_metrics_output is None:\n",
    "    metrics_callback.plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277c27b1587741f2af2001be3712ef0d",
   "metadata": {
    "id": "277c27b1587741f2af2001be3712ef0d"
   },
   "source": [
    "## 3. Summarise & Save\n",
    "Captures final metrics and writes a JSON summary under `artifacts/experiments/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7b79bc585a40fcaf58bf750017e135",
   "metadata": {
    "cellView": "form",
    "collapsed": true,
    "id": "db7b79bc585a40fcaf58bf750017e135",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "from pathlib import Path\n",
    "from IPython.display import Markdown\n",
    "\n",
    "metrics_callback.plot(figsize=(7, 4))\n",
    "\n",
    "final_metrics = {k: float(v) for k, v in TRAINER.callback_metrics.items()}\n",
    "if 'RUN_SEED' in globals():\n",
    "    final_metrics.setdefault('seed', RUN_SEED)\n",
    "display(Markdown(format_training_summary(final_metrics)))\n",
    "seed_value = globals().get('RUN_SEED')\n",
    "resume_path = globals().get('RESUME_CHECKPOINT')\n",
    "run_dir = globals().get('RUN_DIR')\n",
    "checkpoint_dir = globals().get('CHECKPOINT_DIR')\n",
    "latest_checkpoint = globals().get('LATEST_CHECKPOINT')\n",
    "last_checkpoint = globals().get('LAST_CHECKPOINT')\n",
    "summary_root = (ARTIFACT_ROOT / 'experiments').resolve()\n",
    "SUMMARY_PATH = save_experiment_summary(\n",
    "    output_dir=summary_root,\n",
    "    config_path=EXPERIMENT_CONFIG,\n",
    "    dataset_summary=DATASET_RESULT['splits'],\n",
    "    training_metrics=final_metrics,\n",
    "    artifacts={\n",
    "        'dataset_path': DATASET_RESULT.get('dataset_path'),\n",
    "        'default_root_dir': str(run_dir) if run_dir else None,\n",
    "        'checkpoint_dir': str(checkpoint_dir) if checkpoint_dir else None,\n",
    "        'resume_from': str(resume_path) if resume_path else None,\n",
    "        'latest_checkpoint': str(latest_checkpoint) if latest_checkpoint else None,\n",
    "        'last_checkpoint': str(last_checkpoint) if last_checkpoint else None,\n",
    "        'seed': seed_value,\n",
    "    },\n",
    ")\n",
    "SUMMARY_PATH\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "9ad6bc6d",
    "ygEM0yrdxIwB",
    "L2sJyG9dxIwB",
    "fQBMZYf7xIwD",
    "7cdc8c89c7104fffa095e18ddfef8986",
    "938c804e27f84196a10c8828c723f798",
    "59bbdb311c014d738909a11f9e486628",
    "8a65eabff63a45729fe45fb5ade58bdc",
    "4dd4641cc4064e0191573fe9c69df29b",
    "3ed186c9a28b402fb0bc4494df01f08d",
    "379cbbc1e968416e875cc15c1202d7eb",
    "db7b79bc585a40fcaf58bf750017e135"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7fb27b941602401d91542211134fc71a",
      "metadata": {},
      "source": [
        "# MegaContext Research Console\n",
        "\n",
        "Interactively prepare datasets, train GistNet with PyTorch Lightning, and capture experiment artifacts. Key docs: [GistNet](https://brandf.github.io/MegaContext/architecture/components/GistNet), [GistNet Training](https://brandf.github.io/MegaContext/architecture/components/GistNet%20Training), [Telemetry](https://brandf.github.io/MegaContext/ops/Telemetry), [Alternating Optimization](https://brandf.github.io/MegaContext/ops/Alternating%20Optimization).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick Start\n",
        "\n",
        "When running in Google Colab, execute the bootstrap cell below to clone the repo and install dependencies in the current runtime. Local Jupyter environments can skip it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import importlib\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from functools import lru_cache\n",
        "\n",
        "COLAB = 'google.colab' in sys.modules\n",
        "if COLAB:\n",
        "    repo_url = 'https://github.com/brandf/MegaContext.git'\n",
        "    workspace = Path('/content/MegaContext')\n",
        "    if not workspace.exists():\n",
        "        !git clone $repo_url $workspace\n",
        "    else:\n",
        "        !git -C $workspace pull --ff-only\n",
        "    %cd /content/MegaContext\n",
        "    print('Python executable:', sys.executable)\n",
        "    cuda_index = os.environ.get('PYTORCH_CUDA_INDEX', 'https://download.pytorch.org/whl/cu121')\n",
        "    %pip install --upgrade pip setuptools wheel\n",
        "    %pip install torch torchvision torchaudio --index-url $cuda_index\n",
        "    %pip install -e .[dev]\n",
        "    %pip install lightning\n",
        "    %pip install ipywidgets==7.7.1\n",
        "    src_path = workspace / 'src'\n",
        "    if str(src_path) not in sys.path:\n",
        "        sys.path.append(str(src_path))\n",
        "    importlib.invalidate_caches()\n",
        "    try:\n",
        "        import torch\n",
        "        import megacontext  # noqa: F401\n",
        "        import lightning  # noqa: F401\n",
        "    except Exception as exc:\n",
        "        print('Import check failed:', exc)\n",
        "        raise\n",
        "    else:\n",
        "        print('Torch version:', torch.__version__)\n",
        "        print('CUDA available:', torch.cuda.is_available())\n",
        "        print('Colab environment ready.')\n",
        "else:\n",
        "    print('Colab bootstrap skipped (not running in google.colab).')\n",
        "\n",
        "try:\n",
        "    import ipywidgets as widgets  # type: ignore\n",
        "except ImportError:  # pragma: no cover\n",
        "    widgets = None\n",
        "\n",
        "@lru_cache(maxsize=1)\n",
        "def ensure_widgets_ready() -> bool:\n",
        "    'Enable widget infrastructure when available.'\n",
        "    if widgets is None:\n",
        "        return False\n",
        "    if COLAB:\n",
        "        try:\n",
        "            from google.colab import output  # type: ignore\n",
        "        except ImportError:  # pragma: no cover\n",
        "            return True\n",
        "        output.enable_custom_widget_manager()\n",
        "    return True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acae54e37e7d407bbb7b55eff062a284",
      "metadata": {},
      "source": [
        "## 0. Environment Snapshot\n",
        "\n",
        "Verify your runtime (GPU, dependencies, disk space) before launching long jobs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a63283cbaf04dbcab1f6479b197f3a8",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import yaml\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "from megacontext.notebook import (\n",
        "    MetricsTracker,\n",
        "    build_logger,\n",
        "    collect_environment_report,\n",
        "    format_config_markdown,\n",
        "    format_dataset_summary,\n",
        "    format_training_config,\n",
        "    format_training_summary,\n",
        "    render_environment_report,\n",
        "    save_experiment_summary,\n",
        ")\n",
        "\n",
        "ENV_REPORT = collect_environment_report()\n",
        "display(Markdown(render_environment_report(ENV_REPORT)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from IPython.display import display\n",
        "\n",
        "try:\n",
        "    import ipywidgets as widgets  # type: ignore\n",
        "except ImportError:\n",
        "    widgets = None\n",
        "\n",
        "widgets_ready = False\n",
        "if widgets is not None:\n",
        "    try:\n",
        "        widgets_ready = ensure_widgets_ready()\n",
        "    except NameError:\n",
        "        widgets_ready = True\n",
        "\n",
        "if widgets_ready:\n",
        "    wandb_input = widgets.Text(\n",
        "        value=os.environ.get('WANDB_API_KEY', ''),\n",
        "        description='W&B API key:',\n",
        "        layout=widgets.Layout(width='70%'),\n",
        "        placeholder='Optional: paste your Weights & Biases API key'\n",
        "    )\n",
        "    hf_input = widgets.Text(\n",
        "        value=os.environ.get('HUGGINGFACE_HUB_TOKEN', ''),\n",
        "        description='HF token:',\n",
        "        layout=widgets.Layout(width='70%'),\n",
        "        placeholder='Optional: paste your Hugging Face token'\n",
        "    )\n",
        "\n",
        "    def _sync_wandb(change):\n",
        "        os.environ['WANDB_API_KEY'] = change['new'].strip()\n",
        "\n",
        "    def _sync_hf(change):\n",
        "        os.environ['HUGGINGFACE_HUB_TOKEN'] = change['new'].strip()\n",
        "\n",
        "    wandb_input.observe(_sync_wandb, names='value')\n",
        "    hf_input.observe(_sync_hf, names='value')\n",
        "\n",
        "    display(widgets.VBox([wandb_input, hf_input]))\n",
        "else:\n",
        "    print('Set WANDB_API_KEY and HUGGINGFACE_HUB_TOKEN manually if required.')\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "8dd0d8092fe74a7c96281538738b07e2",
      "metadata": {},
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "try:\n",
        "    import ipywidgets as widgets  # type: ignore\n",
        "except ImportError:\n",
        "    widgets = None\n",
        "\n",
        "CONFIG_ROOT = Path('configs')\n",
        "AVAILABLE_CONFIGS = sorted(CONFIG_ROOT.glob('*.yaml'))\n",
        "if not AVAILABLE_CONFIGS:\n",
        "    raise RuntimeError('No experiment configs found under `configs/`.')\n",
        "\n",
        "display(pd.DataFrame({'name': [cfg.stem for cfg in AVAILABLE_CONFIGS], 'path': [str(cfg) for cfg in AVAILABLE_CONFIGS]}))\n",
        "\n",
        "if 'EXPERIMENT_CONFIG' in globals():\n",
        "    current_config = Path(EXPERIMENT_CONFIG)\n",
        "    if current_config in AVAILABLE_CONFIGS:\n",
        "        default_config = current_config\n",
        "    else:\n",
        "        default_config = AVAILABLE_CONFIGS[0]\n",
        "else:\n",
        "    default_config = AVAILABLE_CONFIGS[0]\n",
        "\n",
        "EXPERIMENT_CONFIG = default_config\n",
        "\n",
        "widgets_ready = False\n",
        "if widgets is not None:\n",
        "    try:\n",
        "        widgets_ready = ensure_widgets_ready()\n",
        "    except NameError:\n",
        "        widgets_ready = True\n",
        "\n",
        "if widgets_ready:\n",
        "    config_dropdown = widgets.Dropdown(\n",
        "        options=[(cfg.stem, str(cfg)) for cfg in AVAILABLE_CONFIGS],\n",
        "        value=str(default_config),\n",
        "        description='Config:',\n",
        "        layout=widgets.Layout(width='60%'),\n",
        "    )\n",
        "    message_box = widgets.Output()\n",
        "\n",
        "    def _select_config(change):\n",
        "        if change['name'] != 'value':\n",
        "            return\n",
        "        globals()['EXPERIMENT_CONFIG'] = Path(change['new'])\n",
        "        with message_box:\n",
        "            message_box.clear_output()\n",
        "            display(Markdown(\n",
        "                f\"Selected **{EXPERIMENT_CONFIG.stem}** (`{EXPERIMENT_CONFIG}`)\"\n",
        "            ))\n",
        "\n",
        "    config_dropdown.observe(_select_config, names='value')\n",
        "    display(widgets.VBox([config_dropdown, message_box]))\n",
        "    with message_box:\n",
        "        display(Markdown(\n",
        "            f\"Selected **{default_config.stem}** (`{default_config}`)\"\n",
        "        ))\n",
        "    globals()['EXPERIMENT_CONFIG'] = Path(config_dropdown.value)\n",
        "else:\n",
        "    display(Markdown(\n",
        "        f\"Widgets unavailable; using **{EXPERIMENT_CONFIG.stem}** (`{EXPERIMENT_CONFIG}`)\"\n",
        "    ))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8edb47106e1a46a883d545849b8ab81b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "try:\n",
        "    import ipywidgets as widgets  # type: ignore\n",
        "except ImportError:\n",
        "    widgets = None\n",
        "\n",
        "if 'EXPERIMENT_CONFIG' not in globals():\n",
        "    CONFIG_ROOT = Path('configs')\n",
        "    configs = sorted(CONFIG_ROOT.glob('*.yaml'))\n",
        "    if not configs:\n",
        "        raise RuntimeError('No experiment configs found under `configs/`.')\n",
        "    EXPERIMENT_CONFIG = configs[0]\n",
        "\n",
        "widgets_ready = False\n",
        "if widgets is not None:\n",
        "    try:\n",
        "        widgets_ready = ensure_widgets_ready()\n",
        "    except NameError:\n",
        "        widgets_ready = True\n",
        "\n",
        "LOGGER_STATE = {\n",
        "    'selection': 'none',\n",
        "    'project': 'megacontext-poc',\n",
        "    'run_name': '',\n",
        "}\n",
        "\n",
        "if not widgets_ready:\n",
        "    print('ipywidgets not available; logging defaults to disabled.')\n",
        "    display(Markdown(\n",
        "        f\"Logger: **disabled**<br>Config: **{EXPERIMENT_CONFIG.stem}** \"\n",
        "        f\"(`{EXPERIMENT_CONFIG}`)\"))\n",
        "else:\n",
        "    summary = widgets.HTML()\n",
        "    logger_dropdown = widgets.Dropdown(\n",
        "        options=[('Disabled', 'none'), ('Weights & Biases', 'wandb')],\n",
        "        value=LOGGER_STATE['selection'],\n",
        "        description='Logger:',\n",
        "        layout=widgets.Layout(width='50%'),\n",
        "    )\n",
        "    project_text = widgets.Text(\n",
        "        value=LOGGER_STATE['project'],\n",
        "        description='Project:',\n",
        "        layout=widgets.Layout(width='50%'),\n",
        "    )\n",
        "    run_text = widgets.Text(\n",
        "        value=LOGGER_STATE['run_name'],\n",
        "        placeholder='auto',\n",
        "        description='Run name:',\n",
        "        layout=widgets.Layout(width='50%'),\n",
        "    )\n",
        "    wandb_box = widgets.VBox([project_text, run_text])\n",
        "    wandb_box.layout.display = 'none'\n",
        "\n",
        "    def refresh_summary() -> None:\n",
        "        summary.value = (\n",
        "            f\"<b>Logger:</b> {LOGGER_STATE['selection']}<br>\"\n",
        "            f\"<b>Config:</b> {EXPERIMENT_CONFIG.stem} \"\n",
        "            f\"(<code>{EXPERIMENT_CONFIG}</code>)\"\n",
        "        )\n",
        "\n",
        "    def _update_logger_state(change):\n",
        "        LOGGER_STATE['selection'] = logger_dropdown.value\n",
        "        LOGGER_STATE['project'] = project_text.value\n",
        "        LOGGER_STATE['run_name'] = run_text.value\n",
        "        wandb_box.layout.display = (\n",
        "            'flex' if LOGGER_STATE['selection'] == 'wandb' else 'none'\n",
        "        )\n",
        "        refresh_summary()\n",
        "\n",
        "    logger_dropdown.observe(_update_logger_state, names='value')\n",
        "    project_text.observe(_update_logger_state, names='value')\n",
        "    run_text.observe(_update_logger_state, names='value')\n",
        "\n",
        "    refresh_summary()\n",
        "    display(summary)\n",
        "    display(logger_dropdown)\n",
        "    display(wandb_box)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configure Storage\n",
        "\n",
        "Select where artifacts, checkpoints, and logs are written."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "DEFAULT_ARTIFACT_ROOT = Path(\n",
        "    os.environ.get('MEGACONTEXT_ARTIFACT_ROOT', Path.cwd() / 'artifacts')\n",
        ").expanduser().resolve()\n",
        "\n",
        "ARTIFACT_ROOT = Path(\n",
        "    globals().get('ARTIFACT_ROOT', DEFAULT_ARTIFACT_ROOT)\n",
        ").expanduser().resolve()\n",
        "ARTIFACT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "widgets_ready = False\n",
        "if 'widgets' in globals() and widgets is not None:\n",
        "    try:\n",
        "        widgets_ready = ensure_widgets_ready()\n",
        "    except NameError:\n",
        "        widgets_ready = True\n",
        "\n",
        "if widgets_ready:\n",
        "    artifact_input = widgets.Text(\n",
        "        value=str(ARTIFACT_ROOT),\n",
        "        description='Artifact root:',\n",
        "        layout=widgets.Layout(width='80%'),\n",
        "    )\n",
        "    artifact_status = widgets.Output()\n",
        "\n",
        "    def _sync_artifact(change):\n",
        "        if change.get('name') != 'value':\n",
        "            return\n",
        "        chosen = Path(change['new']).expanduser().resolve()\n",
        "        chosen.mkdir(parents=True, exist_ok=True)\n",
        "        globals()['ARTIFACT_ROOT'] = chosen\n",
        "        with artifact_status:\n",
        "            artifact_status.clear_output()\n",
        "            display(Markdown(f'Using artifact root `{chosen}`'))\n",
        "\n",
        "    artifact_input.observe(_sync_artifact, names='value')\n",
        "    display(widgets.VBox([artifact_input, artifact_status]))\n",
        "    with artifact_status:\n",
        "        artifact_status.clear_output()\n",
        "        display(Markdown(f'Using artifact root `{ARTIFACT_ROOT}`'))\n",
        "else:\n",
        "    print(f'Artifact root: {ARTIFACT_ROOT}')\n",
        "\n",
        "globals()['ARTIFACT_ROOT'] = ARTIFACT_ROOT\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10185d26023b46108eb7d9f57d49d2b3",
      "metadata": {},
      "source": [
        "## 2. Preview Configuration\n",
        "\n",
        "Review and, if needed, edit fields before running stages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8763a12b2bbd4a93a75aff182afb95dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "experiment_cfg = yaml.safe_load(EXPERIMENT_CONFIG.read_text(encoding='utf-8'))\n",
        "display(Markdown(format_config_markdown(experiment_cfg)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7623eae2785240b9bd12b16a66d81610",
      "metadata": {},
      "source": [
        "## 3. Dataset Preparation\n",
        "\n",
        "Runs `tools.prepare_dataset.prepare_dataset_from_config` with tqdm progress bars. Skip this if the shard already exists.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import glob\n",
        "import subprocess\n",
        "import yaml\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "def ensure_dataset_assets(config_path: Path) -> None:\n",
        "    raw_config = yaml.safe_load(config_path.read_text())\n",
        "    dataset_cfg = raw_config.get('dataset', raw_config)\n",
        "    base_dir = config_path.parent\n",
        "    splits = dataset_cfg.get('splits', {})\n",
        "    if isinstance(splits, dict):\n",
        "        iterable = splits.items()\n",
        "    else:\n",
        "        iterable = enumerate(splits or [])\n",
        "    missing_patterns = []\n",
        "    for split_name, split in tqdm(iterable, desc='Checking dataset assets'):\n",
        "        if not isinstance(split, dict):\n",
        "            continue\n",
        "        pattern = (base_dir / split['source']).expanduser()\n",
        "        matches = glob.glob(str(pattern), recursive=True)\n",
        "        if matches:\n",
        "            continue\n",
        "        missing_patterns.append((split_name, pattern))\n",
        "        if 'gutenberg' in str(pattern):\n",
        "            print('Downloading Gutenberg subset (one-time download)...')\n",
        "            subprocess.check_call(['bash', 'scripts/download_gutenberg.sh'])\n",
        "            matches = glob.glob(str(pattern), recursive=True)\n",
        "            if matches:\n",
        "                continue\n",
        "        print(f\"No files matched pattern {pattern}\")\n",
        "    if missing_patterns:\n",
        "        unresolved = '\\n - '.join(f\"{name}: {path}\" for name, path in missing_patterns)\n",
        "        raise FileNotFoundError(\n",
        "            'Dataset assets missing. Patterns without matches:\\n - ' + unresolved\n",
        "        )\n",
        "\n",
        "\n",
        "config_path_obj = EXPERIMENT_CONFIG if isinstance(EXPERIMENT_CONFIG, Path) else Path(EXPERIMENT_CONFIG)\n",
        "ensure_dataset_assets(config_path_obj)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cdc8c89c7104fffa095e18ddfef8986",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "from tools.prepare_dataset import load_dataset_config, prepare_dataset_from_config\n",
        "\n",
        "try:\n",
        "    from tqdm.auto import tqdm\n",
        "except ImportError:  # pragma: no cover\n",
        "    tqdm = None\n",
        "\n",
        "progress = None\n",
        "if tqdm is not None:\n",
        "    progress = tqdm(total=1, desc='Preparing dataset', leave=False, bar_format='{l_bar}{bar}| {elapsed}')\n",
        "else:\n",
        "    print('Preparing dataset (tqdm unavailable)...')\n",
        "try:\n",
        "    DATASET_RESULT = prepare_dataset_from_config(EXPERIMENT_CONFIG)\n",
        "finally:\n",
        "    if progress is not None:\n",
        "        progress.update(1)\n",
        "        progress.close()\n",
        "\n",
        "display(Markdown(format_dataset_summary(DATASET_RESULT['splits'])))\n",
        "\n",
        "CONFIG_MODEL = load_dataset_config(EXPERIMENT_CONFIG)\n",
        "available_splits = list(CONFIG_MODEL.splits.keys())\n",
        "default_split = 'train' if 'train' in available_splits else available_splits[0]\n",
        "SPLIT_NAME = globals().get('SPLIT_NAME', default_split)\n",
        "if SPLIT_NAME not in available_splits:\n",
        "    SPLIT_NAME = default_split\n",
        "\n",
        "\n",
        "def _resolve_dataset_path(split_name: str) -> Path:\n",
        "    split_cfg = CONFIG_MODEL.splits[split_name]\n",
        "    output_path_cfg = Path(split_cfg.output_path)\n",
        "    if output_path_cfg.is_absolute():\n",
        "        return output_path_cfg\n",
        "    base_dir = EXPERIMENT_CONFIG.parent\n",
        "    default_output = (base_dir / output_path_cfg).resolve()\n",
        "    data_root_override = (\n",
        "        Path(os.environ['MEGACONTEXT_DATA_ROOT']).expanduser().resolve()\n",
        "        if 'MEGACONTEXT_DATA_ROOT' in os.environ\n",
        "        else None\n",
        "    )\n",
        "    if data_root_override is not None:\n",
        "        resolved_base = base_dir.resolve()\n",
        "        repo_root = (\n",
        "            resolved_base.parents[1]\n",
        "            if len(resolved_base.parents) >= 2\n",
        "            else resolved_base\n",
        "        )\n",
        "        try:\n",
        "            relative = default_output.relative_to(repo_root)\n",
        "        except ValueError:\n",
        "            relative = default_output.name\n",
        "        return (data_root_override / relative).resolve()\n",
        "    return default_output\n",
        "\n",
        "\n",
        "def _set_dataset_path(split_name: str) -> Path:\n",
        "    path = _resolve_dataset_path(split_name)\n",
        "    globals()['DATASET_PATH'] = path\n",
        "    DATASET_RESULT['dataset_path'] = str(path)\n",
        "    return path\n",
        "\n",
        "\n",
        "DATASET_PATH = _set_dataset_path(SPLIT_NAME)\n",
        "\n",
        "widgets_ready = False\n",
        "if widgets is not None:\n",
        "    try:\n",
        "        widgets_ready = ensure_widgets_ready()\n",
        "    except NameError:\n",
        "        widgets_ready = True\n",
        "\n",
        "if widgets_ready:\n",
        "    split_dropdown = widgets.Dropdown(\n",
        "        options=[(name, name) for name in available_splits],\n",
        "        value=SPLIT_NAME,\n",
        "        description='Split:',\n",
        "        layout=widgets.Layout(width='40%'),\n",
        "    )\n",
        "    split_message = widgets.Output()\n",
        "\n",
        "    def _sync_split(change):\n",
        "        if change.get('name') != 'value':\n",
        "            return\n",
        "        selected = change['new']\n",
        "        globals()['SPLIT_NAME'] = selected\n",
        "        path = _set_dataset_path(selected)\n",
        "        with split_message:\n",
        "            split_message.clear_output()\n",
        "            display(Markdown(f\"Using split **{selected}** \u2192 `{path}`\"))\n",
        "\n",
        "    split_dropdown.observe(_sync_split, names='value')\n",
        "    display(widgets.VBox([split_dropdown, split_message]))\n",
        "    with split_message:\n",
        "        split_message.clear_output()\n",
        "        display(Markdown(f\"Using split **{SPLIT_NAME}** \u2192 `{DATASET_PATH}`\"))\n",
        "else:\n",
        "    print(f\"Using split '{SPLIT_NAME}' at {DATASET_PATH}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b118ea5561624da68c537baed56e602f",
      "metadata": {},
      "source": [
        "### Sample Example\n",
        "\n",
        "Peek at the first prepared context to sanity-check tokens and horizons.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "938c804e27f84196a10c8828c723f798",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pyarrow.ipc as pa_ipc\n",
        "from pathlib import Path\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "if not Path(DATASET_PATH).exists():\n",
        "    missing_msg = (\n",
        "        f\"Dataset shard not found at {DATASET_PATH}. \"\n",
        "        \"Run the prep cell first.\"\n",
        "    )\n",
        "    raise FileNotFoundError(missing_msg)\n",
        "\n",
        "with pa_ipc.open_file(DATASET_PATH) as reader:\n",
        "    if reader.num_record_batches == 0:\n",
        "        print('Dataset is empty.')\n",
        "    else:\n",
        "        batch = reader.get_batch(0)\n",
        "        table_dict = batch.to_pydict()\n",
        "        context_col = 'context_input_ids'\n",
        "        future_col = 'future_input_ids'\n",
        "        missing = [col for col in (context_col, future_col) if col not in table_dict]\n",
        "        if missing:\n",
        "            display(Markdown(\n",
        "                f\"Dataset preview skipped; missing columns: {', '.join(missing)}\"\n",
        "            ))\n",
        "        else:\n",
        "            context_tokens = table_dict[context_col][0][:32]\n",
        "            future_tokens = table_dict[future_col][0][:16]\n",
        "            summary_html = (\n",
        "                f\"Context tokens (first 32): {context_tokens}<br><br>\"\n",
        "                f\"Future tokens (first 16): {future_tokens}\"\n",
        "            )\n",
        "            display(Markdown(summary_html))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "504fb2a444614c0babb325280ed9130a",
      "metadata": {},
      "source": [
        "## 4. Configure GistNet Training\n",
        "\n",
        "Hidden size defaults to the teacher embedding width reported during dataset prep (set `gistnet.model.hidden_size` explicitly to override).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59bbdb311c014d738909a11f9e486628",
      "metadata": {},
      "outputs": [],
      "source": [
        "import dataclasses\n",
        "from copy import deepcopy\n",
        "\n",
        "from megacontext.gistnet import BaseModelSettings, GistNetConfig, GistNetTrainingConfig\n",
        "\n",
        "dataset_hidden_size = DATASET_RESULT['splits'][SPLIT_NAME]['teacher_hidden_size']\n",
        "gistnet_cfg = experiment_cfg.get('gistnet', {})\n",
        "model_dict = deepcopy(gistnet_cfg.get('model', {}))\n",
        "if model_dict.get('hidden_size') == 'auto':\n",
        "    if dataset_hidden_size:\n",
        "        model_dict['hidden_size'] = dataset_hidden_size\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            'Dataset summary did not report a teacher hidden size; '\n",
        "            'set gistnet.model.hidden_size explicitly.'\n",
        "        )\n",
        "MODEL_CONFIG = GistNetConfig(**model_dict)\n",
        "\n",
        "training_dict = deepcopy(gistnet_cfg.get('training', {}))\n",
        "TRAINING_CONFIG = GistNetTrainingConfig.from_dict(training_dict)\n",
        "if TRAINING_CONFIG.base_model is None and 'base_model' in experiment_cfg:\n",
        "    TRAINING_CONFIG = dataclasses.replace(\n",
        "        TRAINING_CONFIG,\n",
        "        base_model=BaseModelSettings.from_dict(experiment_cfg['base_model']),\n",
        "    )\n",
        "display(Markdown(format_training_config(TRAINING_CONFIG)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b43b363d81ae4b689946ece5c682cd59",
      "metadata": {},
      "source": [
        "### Optional Overrides\n",
        "\n",
        "Adjust batch size or per-phase learning rates/steps without editing YAML.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a65eabff63a45729fe45fb5ade58bdc",
      "metadata": {},
      "outputs": [],
      "source": [
        "if widgets is None:\n",
        "    print(\n",
        "        'ipywidgets not available; use dataclasses.replace(...) to '\n",
        "        'override training settings manually.'\n",
        "    )\n",
        "else:\n",
        "    batch_slider = widgets.IntSlider(\n",
        "        value=TRAINING_CONFIG.batch_size,\n",
        "        min=1,\n",
        "        max=128,\n",
        "        step=1,\n",
        "        description='Batch size:',\n",
        "    )\n",
        "    log_every_slider = widgets.IntSlider(\n",
        "        value=max(1, TRAINING_CONFIG.log_every_n_steps),\n",
        "        min=1,\n",
        "        max=max(1, TRAINING_CONFIG.log_every_n_steps * 2),\n",
        "        step=1,\n",
        "        description='Log every:',\n",
        "    )\n",
        "    phase_widgets = []\n",
        "    for phase in TRAINING_CONFIG.phases:\n",
        "        steps_slider = widgets.IntSlider(\n",
        "            value=phase.max_steps,\n",
        "            min=1,\n",
        "            max=max(phase.max_steps, 100),\n",
        "            step=1,\n",
        "            description='Steps',\n",
        "        )\n",
        "        lr_slider = widgets.FloatLogSlider(\n",
        "            value=phase.lr,\n",
        "            base=10,\n",
        "            min=-6,\n",
        "            max=0,\n",
        "            step=0.1,\n",
        "            description='LR',\n",
        "        )\n",
        "        window_slider = widgets.IntSlider(\n",
        "            value=phase.window_tokens,\n",
        "            min=MODEL_CONFIG.block_size,\n",
        "            max=max(phase.window_tokens, MODEL_CONFIG.block_size * 64),\n",
        "            step=MODEL_CONFIG.block_size,\n",
        "            description='Window',\n",
        "        )\n",
        "        phase_box = widgets.VBox([\n",
        "            widgets.HTML(f'<b>{phase.name}</b> ({phase.objective})'),\n",
        "            steps_slider,\n",
        "            lr_slider,\n",
        "            window_slider,\n",
        "        ])\n",
        "        phase_widgets.append((phase, phase_box))\n",
        "    apply_button = widgets.Button(description='Apply overrides', button_style='success')\n",
        "    overrides_output = widgets.Output()\n",
        "\n",
        "    def _apply_overrides(_):\n",
        "        global TRAINING_CONFIG\n",
        "        phases = []\n",
        "        for base_phase, phase_box in phase_widgets:\n",
        "            steps_slider, lr_slider, window_slider = phase_box.children[1:]\n",
        "            phases.append(\n",
        "                dataclasses.replace(\n",
        "                    base_phase,\n",
        "                    max_steps=int(steps_slider.value),\n",
        "                    lr=float(lr_slider.value),\n",
        "                    window_tokens=int(window_slider.value),\n",
        "                )\n",
        "            )\n",
        "        TRAINING_CONFIG = dataclasses.replace(\n",
        "            TRAINING_CONFIG,\n",
        "            batch_size=int(batch_slider.value),\n",
        "            log_every_n_steps=int(log_every_slider.value),\n",
        "            phases=tuple(phases),\n",
        "        )\n",
        "        overrides_output.clear_output()\n",
        "        with overrides_output:\n",
        "            display(\n",
        "                Markdown(\n",
        "                    format_training_config(\n",
        "                        TRAINING_CONFIG, heading='Updated Training Config'\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "\n",
        "    apply_button.on_click(_apply_overrides)\n",
        "    controls = widgets.VBox(\n",
        "        [batch_slider, log_every_slider]\n",
        "        + [box for _, box in phase_widgets]\n",
        "        + [apply_button, overrides_output]\n",
        "    )\n",
        "    display(controls)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reproducibility & Seeding\n",
        "\n",
        "Ensures deterministic behaviour where possible and records the seed in the run summary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "if 'RUN_SEED' not in globals():\n",
        "    RUN_SEED = int(os.environ.get('MEGACONTEXT_SEED', 42))\n",
        "\n",
        "random.seed(RUN_SEED)\n",
        "os.environ['PYTHONHASHSEED'] = str(RUN_SEED)\n",
        "np.random.seed(RUN_SEED)\n",
        "torch.manual_seed(RUN_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(RUN_SEED)\n",
        "    try:\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "    except AttributeError:\n",
        "        pass\n",
        "try:\n",
        "    torch.use_deterministic_algorithms(True)\n",
        "except (AttributeError, RuntimeError):\n",
        "    pass\n",
        "\n",
        "display(Markdown(f'Seeding complete with seed `{RUN_SEED}`.'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Checkpoints & Resume\n",
        "\n",
        "Choose whether to restart or continue from an existing checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "RUNS_DIR = (ARTIFACT_ROOT / 'gistnet').expanduser().resolve()\n",
        "RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "globals()['RUNS_DIR'] = RUNS_DIR\n",
        "\n",
        "\n",
        "def _discover_checkpoints(root: Path) -> list[Path]:\n",
        "    if not root.exists():\n",
        "        return []\n",
        "    return sorted(\n",
        "        root.glob('**/*.ckpt'),\n",
        "        key=lambda path: path.stat().st_mtime,\n",
        "        reverse=True,\n",
        "    )\n",
        "\n",
        "\n",
        "available_checkpoints = _discover_checkpoints(RUNS_DIR)\n",
        "default_resume = available_checkpoints[0] if available_checkpoints else None\n",
        "existing_resume = globals().get('RESUME_CHECKPOINT')\n",
        "if isinstance(existing_resume, str) and existing_resume:\n",
        "    existing_resume = Path(existing_resume)\n",
        "RESUME_CHECKPOINT = existing_resume or default_resume\n",
        "\n",
        "widgets_ready = False\n",
        "if 'widgets' in globals() and widgets is not None:\n",
        "    try:\n",
        "        widgets_ready = ensure_widgets_ready()\n",
        "    except NameError:\n",
        "        widgets_ready = True\n",
        "\n",
        "\n",
        "def _checkpoint_label(path: Path) -> str:\n",
        "    try:\n",
        "        relative = path.relative_to(RUNS_DIR)\n",
        "    except ValueError:\n",
        "        relative = path.name\n",
        "    return str(relative)\n",
        "\n",
        "\n",
        "if widgets_ready and available_checkpoints:\n",
        "    options = [('Do not resume', '')]\n",
        "    options.extend(((_checkpoint_label(path), str(path)) for path in available_checkpoints))\n",
        "    resume_dropdown = widgets.Dropdown(\n",
        "        options=options,\n",
        "        value=str(RESUME_CHECKPOINT) if RESUME_CHECKPOINT else '',\n",
        "        description='Resume from:',\n",
        "        layout=widgets.Layout(width='70%'),\n",
        "    )\n",
        "    resume_status = widgets.Output()\n",
        "\n",
        "    def _sync_resume(change):\n",
        "        if change.get('name') != 'value':\n",
        "            return\n",
        "        selected = change['new']\n",
        "        globals()['RESUME_CHECKPOINT'] = Path(selected) if selected else None\n",
        "        with resume_status:\n",
        "            resume_status.clear_output()\n",
        "            if selected:\n",
        "                display(Markdown(f'Resuming from `{_checkpoint_label(Path(selected))}`'))\n",
        "            else:\n",
        "                display(Markdown('Starting a new run.'))\n",
        "\n",
        "    resume_dropdown.observe(_sync_resume, names='value')\n",
        "    display(widgets.VBox([resume_dropdown, resume_status]))\n",
        "    with resume_status:\n",
        "        resume_status.clear_output()\n",
        "        if RESUME_CHECKPOINT:\n",
        "            display(Markdown(f'Resuming from `{_checkpoint_label(Path(RESUME_CHECKPOINT))}`'))\n",
        "        else:\n",
        "            display(Markdown('Starting a new run.'))\n",
        "else:\n",
        "    if RESUME_CHECKPOINT:\n",
        "        print(f'Resume checkpoint: {_checkpoint_label(Path(RESUME_CHECKPOINT))}')\n",
        "    elif available_checkpoints:\n",
        "        fallback = _checkpoint_label(available_checkpoints[0])\n",
        "        print(f'Checkpoints available (default `{fallback}`) \u2014 set RESUME_CHECKPOINT to resume.')\n",
        "    else:\n",
        "        print('No checkpoints found; starting a new run.')\n",
        "\n",
        "globals()['RESUME_CHECKPOINT'] = RESUME_CHECKPOINT\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3933fab20d04ec698c2621248eb3be0",
      "metadata": {},
      "source": [
        "## 5. Build Lightning Components\n",
        "\n",
        "Adds a rich progress bar and metrics tracker; enable WandB above to stream logs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dd4641cc4064e0191573fe9c69df29b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "from lightning.pytorch.callbacks import ModelCheckpoint, RichProgressBar  # type: ignore\n",
        "\n",
        "from megacontext.gistnet import build_gistnet_experiment\n",
        "\n",
        "metrics_callback = MetricsTracker(metric_keys=(\n",
        "    'train/loss',\n",
        "    'train/delta_loss',\n",
        "    'train/gist_loss',\n",
        "    'train/baseline_loss',\n",
        "))\n",
        "\n",
        "RUNS_DIR = globals().get('RUNS_DIR', (ARTIFACT_ROOT / 'gistnet').resolve())\n",
        "RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "run_name = (LOGGER_STATE.get('run_name') or '').strip()\n",
        "if not run_name:\n",
        "    run_name = f\"{EXPERIMENT_CONFIG.stem}-{datetime.now():%Y%m%d-%H%M%S}\"\n",
        "run_name = run_name.replace(' ', '-')\n",
        "RUN_NAME = run_name\n",
        "RUN_DIR = (RUNS_DIR / run_name).resolve()\n",
        "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
        "CHECKPOINT_DIR = (RUN_DIR / 'checkpoints').resolve()\n",
        "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print(f'Artifacts for run `{RUN_NAME}` stored under {RUN_DIR}')\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath=str(CHECKPOINT_DIR),\n",
        "    filename='step-{step:06d}',\n",
        "    monitor='train/loss',\n",
        "    mode='min',\n",
        "    save_last=True,\n",
        "    save_top_k=3,\n",
        "    every_n_train_steps=max(1, TRAINING_CONFIG.log_every_n_steps),\n",
        "    auto_insert_metric_name=False,\n",
        ")\n",
        "\n",
        "callbacks = [metrics_callback, RichProgressBar(), checkpoint_callback]\n",
        "LOGGER = None\n",
        "try:\n",
        "    LOGGER = build_logger(\n",
        "        selection=LOGGER_STATE.get('selection', 'none'),\n",
        "        project=LOGGER_STATE.get('project'),\n",
        "        run_name=LOGGER_STATE.get('run_name'),\n",
        "        config={'config_path': str(EXPERIMENT_CONFIG)},\n",
        "    )\n",
        "except RuntimeError as exc:\n",
        "    print(exc)\n",
        "\n",
        "trainer_kwargs = {\n",
        "    'accelerator': 'auto',\n",
        "    'devices': 1,\n",
        "    'default_root_dir': str(RUN_DIR),\n",
        "}\n",
        "\n",
        "TRAINER, MODULE, DATA_MODULE = build_gistnet_experiment(\n",
        "    dataset_path=DATASET_PATH,\n",
        "    model_config=MODEL_CONFIG,\n",
        "    training=TRAINING_CONFIG,\n",
        "    callbacks=callbacks,\n",
        "    logger=LOGGER,\n",
        "    trainer_kwargs=trainer_kwargs,\n",
        ")\n",
        "\n",
        "globals()['RUN_DIR'] = RUN_DIR\n",
        "globals()['CHECKPOINT_DIR'] = CHECKPOINT_DIR\n",
        "globals()['RUN_NAME'] = RUN_NAME\n",
        "TRAINER\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8309879909854d7188b41380fd92a7c3",
      "metadata": {},
      "source": [
        "## 6. Launch Training\n",
        "\n",
        "Run this cell to start the Lightning loop. Progress appears below and (optionally) in WandB.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ed186c9a28b402fb0bc4494df01f08d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "resume_path = globals().get('RESUME_CHECKPOINT')\n",
        "if isinstance(resume_path, str) and not resume_path:\n",
        "    resume_path = None\n",
        "if resume_path is not None and not isinstance(resume_path, Path):\n",
        "    resume_path = Path(resume_path)\n",
        "if resume_path is not None and not resume_path.exists():\n",
        "    raise FileNotFoundError(f'Resume checkpoint not found: {resume_path}')\n",
        "\n",
        "if resume_path:\n",
        "    print(f'Resuming from checkpoint: {resume_path}')\n",
        "else:\n",
        "    print('Starting a new training run.')\n",
        "\n",
        "TRAINER.fit(MODULE, DATA_MODULE, ckpt_path=str(resume_path) if resume_path else None)\n",
        "best_path = Path(TRAINER.checkpoint_callback.best_model_path) if TRAINER.checkpoint_callback.best_model_path else None\n",
        "last_path = Path(TRAINER.checkpoint_callback.last_model_path) if getattr(TRAINER.checkpoint_callback, 'last_model_path', None) else None\n",
        "globals()['LATEST_CHECKPOINT'] = best_path\n",
        "globals()['LAST_CHECKPOINT'] = last_path\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb1e1581032b452c9409d6c6813c49d1",
      "metadata": {},
      "source": [
        "## 7. Visualise Metrics\n",
        "\n",
        "Plots the captured metrics (requires matplotlib).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "379cbbc1e968416e875cc15c1202d7eb",
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics_callback.plot(figsize=(7, 4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "277c27b1587741f2af2001be3712ef0d",
      "metadata": {},
      "source": [
        "## 8. Summarise & Save\n",
        "\n",
        "Captures final metrics and writes a JSON summary under `artifacts/experiments/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db7b79bc585a40fcaf58bf750017e135",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from IPython.display import Markdown\n",
        "\n",
        "final_metrics = {k: float(v) for k, v in TRAINER.callback_metrics.items()}\n",
        "if 'RUN_SEED' in globals():\n",
        "    final_metrics.setdefault('seed', RUN_SEED)\n",
        "display(Markdown(format_training_summary(final_metrics)))\n",
        "seed_value = globals().get('RUN_SEED')\n",
        "resume_path = globals().get('RESUME_CHECKPOINT')\n",
        "run_dir = globals().get('RUN_DIR')\n",
        "checkpoint_dir = globals().get('CHECKPOINT_DIR')\n",
        "latest_checkpoint = globals().get('LATEST_CHECKPOINT')\n",
        "last_checkpoint = globals().get('LAST_CHECKPOINT')\n",
        "summary_root = (ARTIFACT_ROOT / 'experiments').resolve()\n",
        "SUMMARY_PATH = save_experiment_summary(\n",
        "    output_dir=summary_root,\n",
        "    config_path=EXPERIMENT_CONFIG,\n",
        "    dataset_summary=DATASET_RESULT['splits'],\n",
        "    training_metrics=final_metrics,\n",
        "    artifacts={\n",
        "        'dataset_path': DATASET_RESULT.get('dataset_path'),\n",
        "        'default_root_dir': str(run_dir) if run_dir else None,\n",
        "        'checkpoint_dir': str(checkpoint_dir) if checkpoint_dir else None,\n",
        "        'resume_from': str(resume_path) if resume_path else None,\n",
        "        'latest_checkpoint': str(latest_checkpoint) if latest_checkpoint else None,\n",
        "        'last_checkpoint': str(last_checkpoint) if last_checkpoint else None,\n",
        "        'seed': seed_value,\n",
        "    },\n",
        ")\n",
        "SUMMARY_PATH\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

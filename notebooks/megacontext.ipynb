{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7fb27b941602401d91542211134fc71a",
      "metadata": {},
      "source": [
        "# MegaContext Research Console\n",
        "\n",
        "Interactively prepare datasets, train GistNet with PyTorch Lightning, and capture experiment artifacts. Key docs: [GistNet](https://brandf.github.io/MegaContext/architecture/components/GistNet), [GistNet Training](https://brandf.github.io/MegaContext/architecture/components/GistNet%20Training), [Telemetry](https://brandf.github.io/MegaContext/ops/Telemetry), [Alternating Optimization](https://brandf.github.io/MegaContext/ops/Alternating%20Optimization).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick Start\n",
        "\n",
        "When running in Google Colab, execute the bootstrap cell below to clone the repo and install dependencies in the current runtime. Local Jupyter environments can skip it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import importlib\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from functools import lru_cache\n",
        "\n",
        "COLAB = 'google.colab' in sys.modules\n",
        "if COLAB:\n",
        "    repo_url = 'https://github.com/brandf/MegaContext.git'\n",
        "    workspace = Path('/content/MegaContext')\n",
        "    if not workspace.exists():\n",
        "        !git clone $repo_url $workspace\n",
        "    else:\n",
        "        !git -C $workspace pull --ff-only\n",
        "    %cd /content/MegaContext\n",
        "    print('Python executable:', sys.executable)\n",
        "    cuda_index = os.environ.get('PYTORCH_CUDA_INDEX', 'https://download.pytorch.org/whl/cu121')\n",
        "\n",
        "    from IPython import get_ipython\n",
        "\n",
        "    shell = get_ipython()\n",
        "    if shell is None:\n",
        "        raise RuntimeError('IPython shell not found; cannot run pip installs.')\n",
        "\n",
        "    def pip_install(args: str) -> None:\n",
        "        shell.run_line_magic('pip', args)\n",
        "\n",
        "    ready_flag = workspace / '.megacontext_bootstrapped'\n",
        "    force_reinstall = os.environ.get('MEGACONTEXT_FORCE_REINSTALL', '').lower() in {'1', 'true', 'yes'}\n",
        "    needs_install = force_reinstall or not ready_flag.exists()\n",
        "\n",
        "    def install_dependencies() -> None:\n",
        "        print('Installing MegaContext dependencies...')\n",
        "        for module_name in list(sys.modules):\n",
        "            if module_name.startswith('megacontext') or module_name.startswith('lightning'):\n",
        "                sys.modules.pop(module_name, None)\n",
        "        pip_install('install --upgrade pip setuptools wheel')\n",
        "        pip_install(f'install torch torchvision torchaudio --index-url {cuda_index}')\n",
        "        pip_install('install lightning')\n",
        "        pip_install('install -e .[dev]')\n",
        "        pip_install('install ipywidgets==7.7.1')\n",
        "        ready_flag.write_text('ok', encoding='utf-8')\n",
        "\n",
        "    if needs_install:\n",
        "        install_dependencies()\n",
        "    else:\n",
        "        print(\n",
        "            f'Using existing environment at {workspace} '\n",
        "            '(set MEGACONTEXT_FORCE_REINSTALL=1 to rebuild).'\n",
        "        )\n",
        "\n",
        "    src_path = workspace / 'src'\n",
        "    if str(src_path) not in sys.path:\n",
        "        sys.path.append(str(src_path))\n",
        "    importlib.invalidate_caches()\n",
        "    try:\n",
        "        import torch\n",
        "        import megacontext  # noqa: F401\n",
        "        import lightning  # noqa: F401\n",
        "    except ModuleNotFoundError as exc:\n",
        "        print(f'Missing dependency ({exc}); reinstalling...')\n",
        "        install_dependencies()\n",
        "        import torch\n",
        "        import megacontext  # noqa: F401\n",
        "        import lightning  # noqa: F401\n",
        "    print('Torch version:', torch.__version__)\n",
        "    print('CUDA available:', torch.cuda.is_available())\n",
        "    print('Colab environment ready.')\n",
        "else:\n",
        "    print('Colab bootstrap skipped (not running in google.colab).')\n",
        "\n",
        "try:\n",
        "    import ipywidgets as widgets  # type: ignore\n",
        "except ImportError:  # pragma: no cover\n",
        "    widgets = None\n",
        "\n",
        "@lru_cache(maxsize=1)\n",
        "def ensure_widgets_ready() -> bool:\n",
        "    'Enable widget infrastructure when available.'\n",
        "    if widgets is None:\n",
        "        return False\n",
        "    if COLAB:\n",
        "        try:\n",
        "            from google.colab import output  # type: ignore\n",
        "        except ImportError:  # pragma: no cover\n",
        "            return True\n",
        "        output.enable_custom_widget_manager()\n",
        "    return True\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Setup Console\n",
        "\n",
        "Configure experiment settings, storage, logging, and reproducibility before running the pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import yaml\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "from megacontext.notebook import (\n",
        "    MetricsTracker,\n",
        "    build_logger,\n",
        "    collect_environment_report,\n",
        "    format_config_markdown,\n",
        "    format_dataset_summary,\n",
        "    format_training_config,\n",
        "    format_training_summary,\n",
        "    render_environment_report,\n",
        "    save_experiment_summary,\n",
        ")\n",
        "\n",
        "try:\n",
        "    import ipywidgets as widgets  # type: ignore\n",
        "except ImportError:  # pragma: no cover\n",
        "    widgets = None\n",
        "\n",
        "\n",
        "def _resolve_path(value):\n",
        "    if value in (None, \"\"):\n",
        "        return None\n",
        "    return Path(value).expanduser().resolve()\n",
        "\n",
        "\n",
        "def launch_setup_console() -> None:\n",
        "    global LOGGER_STATE\n",
        "    if widgets is None:\n",
        "        print('ipywidgets not available \u2014 falling back to defaults.')\n",
        "        config_root = Path('configs')\n",
        "        configs = sorted(config_root.glob('*.yaml'))\n",
        "        if not configs:\n",
        "            raise RuntimeError('No experiment configs found under `configs/`.')\n",
        "        selected_config = configs[0]\n",
        "        globals()['EXPERIMENT_CONFIG'] = selected_config\n",
        "        globals()['experiment_cfg'] = yaml.safe_load(selected_config.read_text(encoding='utf-8'))\n",
        "        env_report = collect_environment_report()\n",
        "        print(render_environment_report(env_report))\n",
        "        artifact_root = _resolve_path(os.environ.get('MEGACONTEXT_ARTIFACT_ROOT')) or (Path.cwd() / 'artifacts').resolve()\n",
        "        artifact_root.mkdir(parents=True, exist_ok=True)\n",
        "        globals()['ARTIFACT_ROOT'] = artifact_root\n",
        "        os.environ['MEGACONTEXT_ARTIFACT_ROOT'] = str(artifact_root)\n",
        "        LOGGER_STATE = {\n",
        "            'selection': os.environ.get('MEGACONTEXT_LOGGER', 'none'),\n",
        "            'project': os.environ.get('MEGACONTEXT_LOGGER_PROJECT', 'megacontext-poc'),\n",
        "            'run_name': os.environ.get('MEGACONTEXT_LOGGER_RUN', ''),\n",
        "        }\n",
        "        seed = int(os.environ.get('MEGACONTEXT_SEED', 42))\n",
        "        globals()['RUN_SEED'] = seed\n",
        "        print(f'Selected config: {selected_config}')\n",
        "        print(f'Artifact root: {artifact_root}')\n",
        "        print(f'Seed: {seed}')\n",
        "        return\n",
        "\n",
        "    ensure_widgets_ready()\n",
        "\n",
        "    config_root = Path('configs')\n",
        "    configs = sorted(config_root.glob('*.yaml'))\n",
        "    if not configs:\n",
        "        raise RuntimeError('No experiment configs found under `configs/`.')\n",
        "\n",
        "    selected_config = globals().get('EXPERIMENT_CONFIG', configs[0])\n",
        "    if isinstance(selected_config, str):\n",
        "        selected_config = Path(selected_config)\n",
        "    if selected_config not in configs:\n",
        "        selected_config = configs[0]\n",
        "\n",
        "    artifact_default = _resolve_path(os.environ.get('MEGACONTEXT_ARTIFACT_ROOT')) or (Path.cwd() / 'artifacts').resolve()\n",
        "    artifact_root = _resolve_path(globals().get('ARTIFACT_ROOT')) or artifact_default\n",
        "    artifact_root.mkdir(parents=True, exist_ok=True)\n",
        "    os.environ['MEGACONTEXT_ARTIFACT_ROOT'] = str(artifact_root)\n",
        "    globals()['ARTIFACT_ROOT'] = artifact_root\n",
        "\n",
        "    data_root_initial = _resolve_path(os.environ.get('MEGACONTEXT_DATA_ROOT'))\n",
        "\n",
        "    LOGGER_STATE = globals().get('LOGGER_STATE', {\n",
        "        'selection': 'none',\n",
        "        'project': 'megacontext-poc',\n",
        "        'run_name': '',\n",
        "    })\n",
        "    LOGGER_STATE = {\n",
        "        'selection': LOGGER_STATE.get('selection', 'none'),\n",
        "        'project': LOGGER_STATE.get('project', 'megacontext-poc'),\n",
        "        'run_name': LOGGER_STATE.get('run_name', ''),\n",
        "    }\n",
        "\n",
        "    env_report = collect_environment_report()\n",
        "    env_output = widgets.Output()\n",
        "    with env_output:\n",
        "        display(Markdown(render_environment_report(env_report)))\n",
        "\n",
        "    seed_initial = int(globals().get('RUN_SEED', os.environ.get('MEGACONTEXT_SEED', 42)))\n",
        "    state = {\n",
        "        'artifact_root': str(artifact_root),\n",
        "        'data_root': str(data_root_initial) if data_root_initial else '',\n",
        "        'resume_checkpoint': globals().get('RESUME_CHECKPOINT'),\n",
        "        'seed': seed_initial,\n",
        "    }\n",
        "\n",
        "    config_dropdown = widgets.Dropdown(\n",
        "        options=[(cfg.stem, str(cfg)) for cfg in configs],\n",
        "        value=str(selected_config),\n",
        "        description='Experiment:',\n",
        "        layout=widgets.Layout(width='60%'),\n",
        "    )\n",
        "    config_preview = widgets.Output()\n",
        "\n",
        "    def _load_config(path: Path) -> None:\n",
        "        cfg = yaml.safe_load(path.read_text(encoding='utf-8'))\n",
        "        globals()['EXPERIMENT_CONFIG'] = path\n",
        "        globals()['experiment_cfg'] = cfg\n",
        "        config_preview.clear_output()\n",
        "        with config_preview:\n",
        "            display(Markdown(format_config_markdown(cfg)))\n",
        "\n",
        "    _load_config(selected_config)\n",
        "\n",
        "    def update_summary(status: str | None = None) -> None:\n",
        "        summary_lines = [\n",
        "            f\"- **Config:** `{globals().get('EXPERIMENT_CONFIG')}`\",\n",
        "            f\"- **Artifact root:** `{state['artifact_root']}`\",\n",
        "        ]\n",
        "        if state['data_root']:\n",
        "            summary_lines.append(f\"- **Data root:** `{state['data_root']}`\")\n",
        "        else:\n",
        "            summary_lines.append('- **Data root:** config-relative')\n",
        "        summary_lines.extend([\n",
        "            f\"- **Logger:** {LOGGER_STATE.get('selection', 'none')}\",\n",
        "            f\"- **Resume checkpoint:** `{state['resume_checkpoint']}`\" if state['resume_checkpoint'] else '- **Resume checkpoint:** none',\n",
        "            f\"- **WANDB token:** {'set' if os.environ.get('WANDB_API_KEY') else 'not set'}\",\n",
        "            f\"- **HF token:** {'set' if os.environ.get('HUGGINGFACE_HUB_TOKEN') else 'not set'}\",\n",
        "            f\"- **Seed:** `{state['seed']}`\",\n",
        "        ])\n",
        "        summary_output.clear_output()\n",
        "        with summary_output:\n",
        "            if status:\n",
        "                display(Markdown(status))\n",
        "            display(Markdown('\\n'.join(summary_lines)))\n",
        "\n",
        "    def _on_config_change(change):\n",
        "        if change['name'] != 'value':\n",
        "            return\n",
        "        _load_config(Path(change['new']))\n",
        "        update_summary('Experiment config updated.')\n",
        "\n",
        "    config_dropdown.observe(_on_config_change, names='value')\n",
        "\n",
        "    wandb_input = widgets.Text(\n",
        "        value=os.environ.get('WANDB_API_KEY', ''),\n",
        "        description='W&B token:',\n",
        "        layout=widgets.Layout(width='70%'),\n",
        "        placeholder='Optional',\n",
        "    )\n",
        "    hf_input = widgets.Text(\n",
        "        value=os.environ.get('HUGGINGFACE_HUB_TOKEN', ''),\n",
        "        description='HF token:',\n",
        "        layout=widgets.Layout(width='70%'),\n",
        "        placeholder='Optional',\n",
        "    )\n",
        "\n",
        "    def _sync_wandb(change):\n",
        "        if change['name'] != 'value':\n",
        "            return\n",
        "        os.environ['WANDB_API_KEY'] = change['new'].strip()\n",
        "        update_summary()\n",
        "\n",
        "    def _sync_hf(change):\n",
        "        if change['name'] != 'value':\n",
        "            return\n",
        "        os.environ['HUGGINGFACE_HUB_TOKEN'] = change['new'].strip()\n",
        "        update_summary()\n",
        "\n",
        "    wandb_input.observe(_sync_wandb, names='value')\n",
        "    hf_input.observe(_sync_hf, names='value')\n",
        "\n",
        "    artifact_input = widgets.Text(\n",
        "        value=str(artifact_root),\n",
        "        description='Artifact root:',\n",
        "        layout=widgets.Layout(width='70%'),\n",
        "    )\n",
        "    artifact_apply = widgets.Button(description='Apply', button_style='info', icon='save')\n",
        "    artifact_message = widgets.Output()\n",
        "\n",
        "    data_input = widgets.Text(\n",
        "        value=str(data_root_initial) if data_root_initial else '',\n",
        "        description='Data root:',\n",
        "        layout=widgets.Layout(width='70%'),\n",
        "        placeholder='Optional \u2014 defaults to config-relative paths',\n",
        "    )\n",
        "    data_apply = widgets.Button(description='Apply', button_style='info', icon='save')\n",
        "    data_message = widgets.Output()\n",
        "\n",
        "    resume_status = widgets.HTML('Starting a new run.')\n",
        "    resume_dropdown = widgets.Dropdown(\n",
        "        description='Resume from:',\n",
        "        layout=widgets.Layout(width='70%'),\n",
        "    )\n",
        "\n",
        "    summary_output = widgets.Output()\n",
        "\n",
        "    def refresh_resume_options() -> None:\n",
        "        runs_dir = Path(state['artifact_root']) / 'gistnet'\n",
        "        runs_dir.mkdir(parents=True, exist_ok=True)\n",
        "        globals()['RUNS_DIR'] = runs_dir\n",
        "        checkpoints = sorted(\n",
        "            runs_dir.glob('**/*.ckpt'),\n",
        "            key=lambda p: p.stat().st_mtime,\n",
        "            reverse=True,\n",
        "        )\n",
        "        options = [('Start fresh', '')]\n",
        "        for path in checkpoints:\n",
        "            try:\n",
        "                label = str(path.relative_to(Path(state['artifact_root'])))\n",
        "            except ValueError:\n",
        "                label = str(path)\n",
        "            options.append((label, str(path)))\n",
        "        current = str(state['resume_checkpoint']) if state['resume_checkpoint'] else ''\n",
        "        values = [value for _, value in options]\n",
        "        if current not in values:\n",
        "            current = ''\n",
        "        resume_dropdown.options = options\n",
        "        resume_dropdown.value = current\n",
        "        if current:\n",
        "            resume_status.value = f'Resuming from <code>{current}</code>'\n",
        "        else:\n",
        "            resume_status.value = 'Starting a new run.'\n",
        "\n",
        "    def _apply_artifact_path(_=None) -> None:\n",
        "        raw = artifact_input.value.strip()\n",
        "        if not raw:\n",
        "            artifact_input.value = state['artifact_root']\n",
        "            update_summary('Artifact root cannot be empty; keeping current path.')\n",
        "            return\n",
        "        new_path = Path(raw).expanduser().resolve()\n",
        "        if str(new_path) == state['artifact_root']:\n",
        "            return\n",
        "        new_path.mkdir(parents=True, exist_ok=True)\n",
        "        state['artifact_root'] = str(new_path)\n",
        "        globals()['ARTIFACT_ROOT'] = new_path\n",
        "        os.environ['MEGACONTEXT_ARTIFACT_ROOT'] = str(new_path)\n",
        "        with artifact_message:\n",
        "            artifact_message.clear_output()\n",
        "            display(Markdown(f'Artifacts stored under `{new_path}`.'))\n",
        "        refresh_resume_options()\n",
        "        update_summary('Artifact root updated.')\n",
        "\n",
        "    artifact_apply.on_click(_apply_artifact_path)\n",
        "    artifact_input.on_submit(lambda _: _apply_artifact_path())\n",
        "\n",
        "    def _apply_data_path(_=None) -> None:\n",
        "        raw = data_input.value.strip()\n",
        "        if not raw:\n",
        "            os.environ.pop('MEGACONTEXT_DATA_ROOT', None)\n",
        "            state['data_root'] = ''\n",
        "            with data_message:\n",
        "                data_message.clear_output()\n",
        "                display(Markdown('Using dataset paths relative to the experiment config.'))\n",
        "            update_summary('Data root cleared.')\n",
        "            return\n",
        "        path = Path(raw).expanduser().resolve()\n",
        "        if str(path) == state['data_root']:\n",
        "            return\n",
        "        path.mkdir(parents=True, exist_ok=True)\n",
        "        os.environ['MEGACONTEXT_DATA_ROOT'] = str(path)\n",
        "        state['data_root'] = str(path)\n",
        "        with data_message:\n",
        "            data_message.clear_output()\n",
        "            display(Markdown(f'Dataset root override set to `{path}`.'))\n",
        "        update_summary('Data root updated.')\n",
        "\n",
        "    data_apply.on_click(_apply_data_path)\n",
        "    data_input.on_submit(lambda _: _apply_data_path())\n",
        "\n",
        "    def _on_resume_change(change):\n",
        "        if change['name'] != 'value':\n",
        "            return\n",
        "        selected = change['new']\n",
        "        state['resume_checkpoint'] = Path(selected) if selected else None\n",
        "        globals()['RESUME_CHECKPOINT'] = state['resume_checkpoint']\n",
        "        if selected:\n",
        "            resume_status.value = f'Resuming from <code>{selected}</code>'\n",
        "        else:\n",
        "            resume_status.value = 'Starting a new run.'\n",
        "        update_summary()\n",
        "\n",
        "    resume_dropdown.observe(_on_resume_change, names='value')\n",
        "\n",
        "    logger_dropdown = widgets.Dropdown(\n",
        "        options=[('Disabled', 'none'), ('Weights & Biases', 'wandb')],\n",
        "        value=LOGGER_STATE['selection'],\n",
        "        description='Logger:',\n",
        "        layout=widgets.Layout(width='50%'),\n",
        "    )\n",
        "    project_text = widgets.Text(\n",
        "        value=LOGGER_STATE['project'],\n",
        "        description='Project:',\n",
        "        layout=widgets.Layout(width='50%'),\n",
        "    )\n",
        "    run_text = widgets.Text(\n",
        "        value=LOGGER_STATE['run_name'],\n",
        "        placeholder='auto',\n",
        "        description='Run name:',\n",
        "        layout=widgets.Layout(width='50%'),\n",
        "    )\n",
        "    logger_summary = widgets.HTML()\n",
        "\n",
        "    def update_logger_state(_=None):\n",
        "        LOGGER_STATE['selection'] = logger_dropdown.value\n",
        "        LOGGER_STATE['project'] = project_text.value.strip()\n",
        "        LOGGER_STATE['run_name'] = run_text.value.strip()\n",
        "        logger_summary.value = (\n",
        "            f\"<b>Logger:</b> {LOGGER_STATE['selection']}<br>\"\n",
        "            f\"<b>Project:</b> {LOGGER_STATE['project'] or '\u2014'}<br>\"\n",
        "            f\"<b>Run name:</b> {LOGGER_STATE['run_name'] or 'auto'}\"\n",
        "        )\n",
        "        update_summary()\n",
        "\n",
        "    for widget in (logger_dropdown, project_text, run_text):\n",
        "        widget.observe(update_logger_state, names='value')\n",
        "    update_logger_state()\n",
        "\n",
        "    seed_input = widgets.IntText(\n",
        "        value=seed_initial,\n",
        "        description='Global seed:',\n",
        "        layout=widgets.Layout(width='30%'),\n",
        "    )\n",
        "    apply_button = widgets.Button(description='Apply settings', button_style='success', icon='check')\n",
        "\n",
        "    def apply_settings(_=None):\n",
        "        try:\n",
        "            seed = int(seed_input.value)\n",
        "        except (TypeError, ValueError):\n",
        "            seed_input.value = state['seed']\n",
        "            update_summary('Seed must be an integer; reverting to the previous value.')\n",
        "            return\n",
        "        state['seed'] = seed\n",
        "        globals()['RUN_SEED'] = seed\n",
        "        os.environ['MEGACONTEXT_SEED'] = str(seed)\n",
        "        random.seed(seed)\n",
        "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "        np.random.seed(seed)\n",
        "        try:\n",
        "            import torch\n",
        "            torch.manual_seed(seed)\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.manual_seed_all(seed)\n",
        "                try:\n",
        "                    torch.backends.cudnn.deterministic = True\n",
        "                    torch.backends.cudnn.benchmark = False\n",
        "                except AttributeError:\n",
        "                    pass\n",
        "            try:\n",
        "                torch.use_deterministic_algorithms(True)\n",
        "            except (AttributeError, RuntimeError):\n",
        "                pass\n",
        "        except ImportError:\n",
        "            pass\n",
        "        update_summary(f'Seeded runtime with `{seed}`.')\n",
        "\n",
        "    apply_button.on_click(apply_settings)\n",
        "\n",
        "    refresh_resume_options()\n",
        "    update_summary()\n",
        "\n",
        "    overview_box = widgets.VBox([env_output, summary_output])\n",
        "    experiment_box = widgets.VBox([config_dropdown, config_preview])\n",
        "    storage_box = widgets.VBox([\n",
        "        widgets.HBox([artifact_input, artifact_apply]),\n",
        "        artifact_message,\n",
        "        widgets.HBox([data_input, data_apply]),\n",
        "        data_message,\n",
        "        resume_dropdown,\n",
        "        resume_status,\n",
        "    ])\n",
        "    logging_box = widgets.VBox([logger_dropdown, project_text, run_text, logger_summary])\n",
        "    credentials_box = widgets.VBox([wandb_input, hf_input])\n",
        "    reproducibility_box = widgets.VBox([seed_input, apply_button])\n",
        "\n",
        "    tabs = widgets.Tab(children=[overview_box, experiment_box, storage_box, logging_box, credentials_box, reproducibility_box])\n",
        "    titles = ['Overview', 'Experiment', 'Storage', 'Logging', 'Credentials', 'Reproducibility']\n",
        "    for idx, title in enumerate(titles):\n",
        "        tabs.set_title(idx, title)\n",
        "\n",
        "    apply_settings()\n",
        "    globals()['SETUP_CONSOLE'] = tabs\n",
        "    return tabs\n",
        "\n",
        "\n",
        "console_widget = launch_setup_console()\n",
        "if console_widget is not None:\n",
        "    display(console_widget)\n",
        "    print('Setup console ready \u2014 adjust settings via the tabs above.')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7623eae2785240b9bd12b16a66d81610",
      "metadata": {},
      "source": [
        "## 1. Dataset Preparation\n\nRuns `tools.prepare_dataset.prepare_dataset_from_config` with tqdm progress bars. Skip this if the shard already exists.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import glob\n",
        "import subprocess\n",
        "import yaml\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "def ensure_dataset_assets(config_path: Path) -> None:\n",
        "    raw_config = yaml.safe_load(config_path.read_text())\n",
        "    dataset_cfg = raw_config.get('dataset', raw_config)\n",
        "    base_dir = config_path.parent\n",
        "    splits = dataset_cfg.get('splits', {})\n",
        "    if isinstance(splits, dict):\n",
        "        iterable = splits.items()\n",
        "    else:\n",
        "        iterable = enumerate(splits or [])\n",
        "    missing_patterns = []\n",
        "    for split_name, split in tqdm(iterable, desc='Checking dataset assets'):\n",
        "        if not isinstance(split, dict):\n",
        "            continue\n",
        "        pattern = (base_dir / split['source']).expanduser()\n",
        "        matches = glob.glob(str(pattern), recursive=True)\n",
        "        if matches:\n",
        "            continue\n",
        "        if 'gutenberg' in str(pattern):\n",
        "            print('Downloading Gutenberg subset (one-time download)...')\n",
        "            script_path = Path('tools/download_gutenberg.sh')\n",
        "            if not script_path.exists():\n",
        "                raise FileNotFoundError(\n",
        "                    'Expected tools/download_gutenberg.sh to exist. '\n",
        "                    'Populate the Gutenberg corpus manually or restore the helper script.'\n",
        "                )\n",
        "            glob_str = str(pattern)\n",
        "            stop_idx = len(glob_str)\n",
        "            for token in ('*', '?', '['):\n",
        "                idx = glob_str.find(token)\n",
        "                if idx != -1:\n",
        "                    stop_idx = min(stop_idx, idx)\n",
        "            target_dir = Path(glob_str[:stop_idx]).resolve()\n",
        "            if target_dir.suffix:\n",
        "                target_dir = target_dir.parent\n",
        "            target_dir.mkdir(parents=True, exist_ok=True)\n",
        "            try:\n",
        "                subprocess.check_call(['bash', str(script_path), str(target_dir)])\n",
        "            except FileNotFoundError as err:\n",
        "                raise RuntimeError('`bash` not found while running Gutenberg download script.') from err\n",
        "            except subprocess.CalledProcessError as err:\n",
        "                raise RuntimeError(\n",
        "                    'Gutenberg download script failed. Inspect the output above or run \"bash tools/download_gutenberg.sh\" manually to diagnose.'\n",
        "                ) from err\n",
        "            matches = glob.glob(str(pattern), recursive=True)\n",
        "            if matches:\n",
        "                continue\n",
        "        missing_patterns.append((split_name, pattern))\n",
        "        print(f\"No files matched pattern {pattern}\")\n",
        "    if missing_patterns:\n",
        "        unresolved = '\\n - '.join(f\"{name}: {path}\" for name, path in missing_patterns)\n",
        "        raise FileNotFoundError(\n",
        "            'Dataset assets missing. Patterns without matches:\\n - ' + unresolved\n",
        "        )\n",
        "\n",
        "\n",
        "config_path_obj = EXPERIMENT_CONFIG if isinstance(EXPERIMENT_CONFIG, Path) else Path(EXPERIMENT_CONFIG)\n",
        "ensure_dataset_assets(config_path_obj)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cdc8c89c7104fffa095e18ddfef8986",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import yaml\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "from megacontext.notebook import format_dataset_summary\n",
        "from tools.prepare_dataset import load_dataset_config, prepare_dataset_from_config\n",
        "\n",
        "try:\n",
        "    from tqdm.auto import tqdm\n",
        "except ImportError:  # pragma: no cover\n",
        "    tqdm = None\n",
        "\n",
        "CONFIG_MODEL = load_dataset_config(EXPERIMENT_CONFIG)\n",
        "\n",
        "\n",
        "def _resolve_dataset_path(split_name: str) -> Path:\n",
        "    split_cfg = CONFIG_MODEL.splits[split_name]\n",
        "    output_path_cfg = Path(split_cfg.output_path)\n",
        "    if output_path_cfg.is_absolute():\n",
        "        return output_path_cfg\n",
        "    base_dir = EXPERIMENT_CONFIG.parent\n",
        "    default_output = (base_dir / output_path_cfg).resolve()\n",
        "    data_root_override = os.environ.get('MEGACONTEXT_DATA_ROOT')\n",
        "    if data_root_override:\n",
        "        resolved_base = base_dir.resolve()\n",
        "        repo_root = (\n",
        "            resolved_base.parents[1]\n",
        "            if len(resolved_base.parents) >= 2\n",
        "            else resolved_base\n",
        "        )\n",
        "        try:\n",
        "            relative = default_output.relative_to(repo_root)\n",
        "        except ValueError:\n",
        "            relative = default_output.name\n",
        "        return (Path(data_root_override).expanduser().resolve() / relative).resolve()\n",
        "    return default_output\n",
        "\n",
        "\n",
        "expected_outputs = {name: _resolve_dataset_path(name) for name in CONFIG_MODEL.splits}\n",
        "metadata_path = CONFIG_MODEL.metadata_path()\n",
        "if metadata_path.suffix not in {'.yaml', '.yml'}:\n",
        "    metadata_path = metadata_path.with_suffix('.yaml')\n",
        "metadata_path = metadata_path.resolve()\n",
        "force_rebuild = os.environ.get('MEGACONTEXT_FORCE_DATA_REBUILD', '').lower() in {'1', 'true', 'yes'}\n",
        "missing_outputs = [name for name, path in expected_outputs.items() if not path.exists()]\n",
        "metadata_missing = not metadata_path.exists()\n",
        "\n",
        "if force_rebuild:\n",
        "    print('Forcing dataset rebuild because MEGACONTEXT_FORCE_DATA_REBUILD is set.')\n",
        "if missing_outputs:\n",
        "    print('Dataset outputs missing for splits: ' + ', '.join(missing_outputs))\n",
        "if metadata_missing and not (force_rebuild or missing_outputs):\n",
        "    print(f'Metadata not found at {metadata_path}; regenerating dataset summaries.')\n",
        "\n",
        "run_prepare = force_rebuild or bool(missing_outputs) or metadata_missing\n",
        "\n",
        "DATASET_RESULT = None\n",
        "if run_prepare:\n",
        "    progress = None\n",
        "    if tqdm is not None:\n",
        "        progress = tqdm(total=1, desc='Preparing dataset', leave=False, bar_format='{l_bar}{bar}| {elapsed}')\n",
        "    else:\n",
        "        print('Preparing dataset (tqdm unavailable)...')\n",
        "    try:\n",
        "        DATASET_RESULT = prepare_dataset_from_config(EXPERIMENT_CONFIG)\n",
        "    finally:\n",
        "        if tqdm is not None and progress is not None:\n",
        "            progress.update(1)\n",
        "            progress.close()\n",
        "else:\n",
        "    print('Dataset shards already present; skipping prepare_dataset_from_config.')\n",
        "    metadata = yaml.safe_load(metadata_path.read_text(encoding='utf-8')) or {}\n",
        "    DATASET_RESULT = {\n",
        "        'config_path': str(EXPERIMENT_CONFIG),\n",
        "        'metadata_path': str(metadata_path),\n",
        "        'teacher_dtype': metadata.get('teacher_dtype'),\n",
        "        'splits': metadata.get('splits', {}) or {},\n",
        "    }\n",
        "\n",
        "if not DATASET_RESULT['splits']:\n",
        "    print('Warning: dataset summaries are empty. Consider rebuilding the dataset.')\n",
        "\n",
        "display(Markdown(format_dataset_summary(DATASET_RESULT['splits'])))\n",
        "\n",
        "available_splits = list(CONFIG_MODEL.splits.keys())\n",
        "default_split = 'train' if 'train' in available_splits else available_splits[0]\n",
        "SPLIT_NAME = globals().get('SPLIT_NAME', default_split)\n",
        "if SPLIT_NAME not in available_splits:\n",
        "    SPLIT_NAME = default_split\n",
        "\n",
        "\n",
        "def _set_dataset_path(split_name: str) -> Path:\n",
        "    path = _resolve_dataset_path(split_name)\n",
        "    globals()['DATASET_PATH'] = path\n",
        "    DATASET_RESULT['dataset_path'] = str(path)\n",
        "    return path\n",
        "\n",
        "\n",
        "DATASET_PATH = _set_dataset_path(SPLIT_NAME)\n",
        "\n",
        "widgets_ready = False\n",
        "if widgets is not None:\n",
        "    try:\n",
        "        widgets_ready = ensure_widgets_ready()\n",
        "    except NameError:\n",
        "        widgets_ready = True\n",
        "\n",
        "if widgets_ready:\n",
        "    split_dropdown = widgets.Dropdown(\n",
        "        options=[(name, name) for name in available_splits],\n",
        "        value=SPLIT_NAME,\n",
        "        description='Split:',\n",
        "        layout=widgets.Layout(width='40%'),\n",
        "    )\n",
        "    split_message = widgets.Output()\n",
        "\n",
        "    def _sync_split(change):\n",
        "        if change.get('name') != 'value':\n",
        "            return\n",
        "        selected = change['new']\n",
        "        globals()['SPLIT_NAME'] = selected\n",
        "        path = _set_dataset_path(selected)\n",
        "        with split_message:\n",
        "            split_message.clear_output()\n",
        "            display(Markdown(f\"Using split **{selected}** \u2192 `{path}`\"))\n",
        "\n",
        "    split_dropdown.observe(_sync_split, names='value')\n",
        "    display(widgets.VBox([split_dropdown, split_message]))\n",
        "    with split_message:\n",
        "        split_message.clear_output()\n",
        "        display(Markdown(f\"Using split **{SPLIT_NAME}** \u2192 `{DATASET_PATH}`\"))\n",
        "else:\n",
        "    print(f\"Using split '{SPLIT_NAME}' at {DATASET_PATH}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b118ea5561624da68c537baed56e602f",
      "metadata": {},
      "source": [
        "### Sample Example\n",
        "\n",
        "Peek at the first prepared context to sanity-check tokens and horizons.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "938c804e27f84196a10c8828c723f798",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pyarrow.ipc as pa_ipc\n",
        "from pathlib import Path\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "if not Path(DATASET_PATH).exists():\n",
        "    missing_msg = (\n",
        "        f\"Dataset shard not found at {DATASET_PATH}. \"\n",
        "        \"Run the prep cell first.\"\n",
        "    )\n",
        "    raise FileNotFoundError(missing_msg)\n",
        "\n",
        "with pa_ipc.open_file(DATASET_PATH) as reader:\n",
        "    if reader.num_record_batches == 0:\n",
        "        print('Dataset is empty.')\n",
        "    else:\n",
        "        batch = reader.get_batch(0)\n",
        "        table_dict = batch.to_pydict()\n",
        "        context_col = 'context_input_ids'\n",
        "        future_col = 'future_input_ids'\n",
        "        missing = [col for col in (context_col, future_col) if col not in table_dict]\n",
        "        if missing:\n",
        "            display(Markdown(\n",
        "                f\"Dataset preview skipped; missing columns: {', '.join(missing)}\"\n",
        "            ))\n",
        "        else:\n",
        "            context_tokens = table_dict[context_col][0][:32]\n",
        "            future_tokens = table_dict[future_col][0][:16]\n",
        "            summary_html = (\n",
        "                f\"Context tokens (first 32): {context_tokens}<br><br>\"\n",
        "                f\"Future tokens (first 16): {future_tokens}\"\n",
        "            )\n",
        "            display(Markdown(summary_html))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "504fb2a444614c0babb325280ed9130a",
      "metadata": {},
      "source": [
        "## 2. Configure GistNet Training\n\nHidden size defaults to the teacher embedding width reported during dataset prep (set `gistnet.model.hidden_size` explicitly to override).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59bbdb311c014d738909a11f9e486628",
      "metadata": {},
      "outputs": [],
      "source": [
        "import dataclasses\n",
        "from copy import deepcopy\n",
        "\n",
        "from megacontext.gistnet import BaseModelSettings, GistNetConfig, GistNetTrainingConfig\n",
        "\n",
        "dataset_hidden_size = DATASET_RESULT['splits'][SPLIT_NAME]['teacher_hidden_size']\n",
        "gistnet_cfg = experiment_cfg.get('gistnet', {})\n",
        "model_dict = deepcopy(gistnet_cfg.get('model', {}))\n",
        "if model_dict.get('hidden_size') == 'auto':\n",
        "    if dataset_hidden_size:\n",
        "        model_dict['hidden_size'] = dataset_hidden_size\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            'Dataset summary did not report a teacher hidden size; '\n",
        "            'set gistnet.model.hidden_size explicitly.'\n",
        "        )\n",
        "MODEL_CONFIG = GistNetConfig(**model_dict)\n",
        "\n",
        "training_dict = deepcopy(gistnet_cfg.get('training', {}))\n",
        "TRAINING_CONFIG = GistNetTrainingConfig.from_dict(training_dict)\n",
        "if TRAINING_CONFIG.base_model is None and 'base_model' in experiment_cfg:\n",
        "    TRAINING_CONFIG = dataclasses.replace(\n",
        "        TRAINING_CONFIG,\n",
        "        base_model=BaseModelSettings.from_dict(experiment_cfg['base_model']),\n",
        "    )\n",
        "display(Markdown(format_training_config(TRAINING_CONFIG)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b43b363d81ae4b689946ece5c682cd59",
      "metadata": {},
      "source": [
        "### Optional Overrides\n",
        "\n",
        "Adjust batch size or per-phase learning rates/steps without editing YAML.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a65eabff63a45729fe45fb5ade58bdc",
      "metadata": {},
      "outputs": [],
      "source": [
        "if widgets is None:\n",
        "    print(\n",
        "        'ipywidgets not available; use dataclasses.replace(...) to '\n",
        "        'override training settings manually.'\n",
        "    )\n",
        "else:\n",
        "    batch_slider = widgets.IntSlider(\n",
        "        value=TRAINING_CONFIG.batch_size,\n",
        "        min=1,\n",
        "        max=128,\n",
        "        step=1,\n",
        "        description='Batch size:',\n",
        "    )\n",
        "    log_every_slider = widgets.IntSlider(\n",
        "        value=max(1, TRAINING_CONFIG.log_every_n_steps),\n",
        "        min=1,\n",
        "        max=max(1, TRAINING_CONFIG.log_every_n_steps * 2),\n",
        "        step=1,\n",
        "        description='Log every:',\n",
        "    )\n",
        "    phase_widgets = []\n",
        "    for phase in TRAINING_CONFIG.phases:\n",
        "        steps_slider = widgets.IntSlider(\n",
        "            value=phase.max_steps,\n",
        "            min=1,\n",
        "            max=max(phase.max_steps, 100),\n",
        "            step=1,\n",
        "            description='Steps',\n",
        "        )\n",
        "        lr_slider = widgets.FloatLogSlider(\n",
        "            value=phase.lr,\n",
        "            base=10,\n",
        "            min=-6,\n",
        "            max=0,\n",
        "            step=0.1,\n",
        "            description='LR',\n",
        "        )\n",
        "        window_slider = widgets.IntSlider(\n",
        "            value=phase.window_tokens,\n",
        "            min=MODEL_CONFIG.block_size,\n",
        "            max=max(phase.window_tokens, MODEL_CONFIG.block_size * 64),\n",
        "            step=MODEL_CONFIG.block_size,\n",
        "            description='Window',\n",
        "        )\n",
        "        phase_box = widgets.VBox([\n",
        "            widgets.HTML(f'<b>{phase.name}</b> ({phase.objective})'),\n",
        "            steps_slider,\n",
        "            lr_slider,\n",
        "            window_slider,\n",
        "        ])\n",
        "        phase_widgets.append((phase, phase_box))\n",
        "    apply_button = widgets.Button(description='Apply overrides', button_style='success')\n",
        "    overrides_output = widgets.Output()\n",
        "\n",
        "    def _apply_overrides(_):\n",
        "        global TRAINING_CONFIG\n",
        "        phases = []\n",
        "        for base_phase, phase_box in phase_widgets:\n",
        "            steps_slider, lr_slider, window_slider = phase_box.children[1:]\n",
        "            phases.append(\n",
        "                dataclasses.replace(\n",
        "                    base_phase,\n",
        "                    max_steps=int(steps_slider.value),\n",
        "                    lr=float(lr_slider.value),\n",
        "                    window_tokens=int(window_slider.value),\n",
        "                )\n",
        "            )\n",
        "        TRAINING_CONFIG = dataclasses.replace(\n",
        "            TRAINING_CONFIG,\n",
        "            batch_size=int(batch_slider.value),\n",
        "            log_every_n_steps=int(log_every_slider.value),\n",
        "            phases=tuple(phases),\n",
        "        )\n",
        "        overrides_output.clear_output()\n",
        "        with overrides_output:\n",
        "            display(\n",
        "                Markdown(\n",
        "                    format_training_config(\n",
        "                        TRAINING_CONFIG, heading='Updated Training Config'\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "\n",
        "    apply_button.on_click(_apply_overrides)\n",
        "    controls = widgets.VBox(\n",
        "        [batch_slider, log_every_slider]\n",
        "        + [box for _, box in phase_widgets]\n",
        "        + [apply_button, overrides_output]\n",
        "    )\n",
        "    display(controls)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3933fab20d04ec698c2621248eb3be0",
      "metadata": {},
      "source": [
        "## 3. Build Lightning Components\n\nAdds a rich progress bar and metrics tracker; enable WandB above to stream logs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dd4641cc4064e0191573fe9c69df29b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "from lightning.pytorch.callbacks import ModelCheckpoint, RichProgressBar  # type: ignore\n",
        "\n",
        "from megacontext.gistnet import build_gistnet_experiment\n",
        "\n",
        "metrics_callback = MetricsTracker(metric_keys=(\n",
        "    'train/loss',\n",
        "    'train/delta_loss',\n",
        "    'train/gist_loss',\n",
        "    'train/baseline_loss',\n",
        "))\n",
        "\n",
        "RUNS_DIR = globals().get('RUNS_DIR', (ARTIFACT_ROOT / 'gistnet').resolve())\n",
        "RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "run_name = (LOGGER_STATE.get('run_name') or '').strip()\n",
        "if not run_name:\n",
        "    run_name = f\"{EXPERIMENT_CONFIG.stem}-{datetime.now():%Y%m%d-%H%M%S}\"\n",
        "run_name = run_name.replace(' ', '-')\n",
        "RUN_NAME = run_name\n",
        "RUN_DIR = (RUNS_DIR / run_name).resolve()\n",
        "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
        "CHECKPOINT_DIR = (RUN_DIR / 'checkpoints').resolve()\n",
        "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print(f'Artifacts for run `{RUN_NAME}` stored under {RUN_DIR}')\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath=str(CHECKPOINT_DIR),\n",
        "    filename='step-{step:06d}',\n",
        "    monitor='train/loss',\n",
        "    mode='min',\n",
        "    save_last=True,\n",
        "    save_top_k=3,\n",
        "    every_n_train_steps=max(1, TRAINING_CONFIG.log_every_n_steps),\n",
        "    auto_insert_metric_name=False,\n",
        ")\n",
        "\n",
        "callbacks = [metrics_callback, RichProgressBar(), checkpoint_callback]\n",
        "LOGGER = None\n",
        "try:\n",
        "    LOGGER = build_logger(\n",
        "        selection=LOGGER_STATE.get('selection', 'none'),\n",
        "        project=LOGGER_STATE.get('project'),\n",
        "        run_name=LOGGER_STATE.get('run_name'),\n",
        "        config={'config_path': str(EXPERIMENT_CONFIG)},\n",
        "    )\n",
        "except RuntimeError as exc:\n",
        "    print(exc)\n",
        "\n",
        "trainer_kwargs = {\n",
        "    'accelerator': 'auto',\n",
        "    'devices': 1,\n",
        "    'default_root_dir': str(RUN_DIR),\n",
        "}\n",
        "\n",
        "TRAINER, MODULE, DATA_MODULE = build_gistnet_experiment(\n",
        "    dataset_path=DATASET_PATH,\n",
        "    model_config=MODEL_CONFIG,\n",
        "    training=TRAINING_CONFIG,\n",
        "    callbacks=callbacks,\n",
        "    logger=LOGGER,\n",
        "    trainer_kwargs=trainer_kwargs,\n",
        ")\n",
        "\n",
        "globals()['RUN_DIR'] = RUN_DIR\n",
        "globals()['CHECKPOINT_DIR'] = CHECKPOINT_DIR\n",
        "globals()['RUN_NAME'] = RUN_NAME\n",
        "TRAINER\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8309879909854d7188b41380fd92a7c3",
      "metadata": {},
      "source": [
        "## 4. Launch Training\n\nRun this cell to start the Lightning loop. Progress appears below and (optionally) in WandB.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ed186c9a28b402fb0bc4494df01f08d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "resume_path = globals().get('RESUME_CHECKPOINT')\n",
        "if isinstance(resume_path, str) and not resume_path:\n",
        "    resume_path = None\n",
        "if resume_path is not None and not isinstance(resume_path, Path):\n",
        "    resume_path = Path(resume_path)\n",
        "if resume_path is not None and not resume_path.exists():\n",
        "    raise FileNotFoundError(f'Resume checkpoint not found: {resume_path}')\n",
        "\n",
        "if resume_path:\n",
        "    print(f'Resuming from checkpoint: {resume_path}')\n",
        "else:\n",
        "    print('Starting a new training run.')\n",
        "\n",
        "TRAINER.fit(MODULE, DATA_MODULE, ckpt_path=str(resume_path) if resume_path else None)\n",
        "best_model_path = getattr(TRAINER.checkpoint_callback, 'best_model_path', None) or ''\n",
        "if best_model_path:\n",
        "    best_path = Path(best_model_path)\n",
        "else:\n",
        "    best_path = None\n",
        "last_model_path = getattr(TRAINER.checkpoint_callback, 'last_model_path', None) or ''\n",
        "if last_model_path:\n",
        "    last_path = Path(last_model_path)\n",
        "else:\n",
        "    last_path = None\n",
        "globals()['LATEST_CHECKPOINT'] = best_path\n",
        "globals()['LAST_CHECKPOINT'] = last_path\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb1e1581032b452c9409d6c6813c49d1",
      "metadata": {},
      "source": [
        "## 5. Visualise Metrics\n\nPlots the captured metrics (requires matplotlib).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "379cbbc1e968416e875cc15c1202d7eb",
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics_callback.plot(figsize=(7, 4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "277c27b1587741f2af2001be3712ef0d",
      "metadata": {},
      "source": [
        "## 6. Summarise & Save\n\nCaptures final metrics and writes a JSON summary under `artifacts/experiments/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db7b79bc585a40fcaf58bf750017e135",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from IPython.display import Markdown\n",
        "\n",
        "final_metrics = {k: float(v) for k, v in TRAINER.callback_metrics.items()}\n",
        "if 'RUN_SEED' in globals():\n",
        "    final_metrics.setdefault('seed', RUN_SEED)\n",
        "display(Markdown(format_training_summary(final_metrics)))\n",
        "seed_value = globals().get('RUN_SEED')\n",
        "resume_path = globals().get('RESUME_CHECKPOINT')\n",
        "run_dir = globals().get('RUN_DIR')\n",
        "checkpoint_dir = globals().get('CHECKPOINT_DIR')\n",
        "latest_checkpoint = globals().get('LATEST_CHECKPOINT')\n",
        "last_checkpoint = globals().get('LAST_CHECKPOINT')\n",
        "summary_root = (ARTIFACT_ROOT / 'experiments').resolve()\n",
        "SUMMARY_PATH = save_experiment_summary(\n",
        "    output_dir=summary_root,\n",
        "    config_path=EXPERIMENT_CONFIG,\n",
        "    dataset_summary=DATASET_RESULT['splits'],\n",
        "    training_metrics=final_metrics,\n",
        "    artifacts={\n",
        "        'dataset_path': DATASET_RESULT.get('dataset_path'),\n",
        "        'default_root_dir': str(run_dir) if run_dir else None,\n",
        "        'checkpoint_dir': str(checkpoint_dir) if checkpoint_dir else None,\n",
        "        'resume_from': str(resume_path) if resume_path else None,\n",
        "        'latest_checkpoint': str(latest_checkpoint) if latest_checkpoint else None,\n",
        "        'last_checkpoint': str(last_checkpoint) if last_checkpoint else None,\n",
        "        'seed': seed_value,\n",
        "    },\n",
        ")\n",
        "SUMMARY_PATH\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

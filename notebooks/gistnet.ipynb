{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GistNet Training Guide\n",
        "\n",
        "This notebook consolidates the minimal data and training pipeline for the Phase 2 gist compressor. It replaces the standalone `docs/gistnet.md` so the dataset prep, trainer commands, and logging tips live alongside the executable steps.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/brandf/MegaContext/blob/main/notebooks/gistnet.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Fields\n",
        "\n",
        "`tools/prepare_dataset.py` emits Arrow shards with the following columns:\n",
        "\n",
        "- `input_ids` — L0 token ids for the block being compressed (`block_size` tokens).\n",
        "- `attention_mask` — mask for the block (currently all ones).\n",
        "- `context_input_ids` — flattened horizon window (`horizon` tokens) used when the teacher model produced cached embeddings.\n",
        "- `context_attention_mask` — mask for the horizon window.\n",
        "- `teacher_hidden` — cached teacher embeddings with shape `[block_size, teacher_hidden_size]` stored using the configured dtype (auto → float16 on T4, bfloat16 on bf16-capable GPUs, otherwise float32).\n",
        "- `gist_target` — pooled target vector (mean of the teacher hidden states) emitted in the same dtype as `teacher_hidden`.\n",
        "\n",
        "The metadata stored in `data/<dataset>/metadata.yaml` records the tokenizer, block size, horizon, teacher model/dtype, and per-split statistics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Minimal sample dataset\n",
        "\n",
        "If you just need a smoke test, generate the sample Arrow shard with:\n",
        "\n",
        "```bash\n",
        "uv run python -m tools.prepare_dataset --config configs/data/sample_text.yaml\n",
        "```\n",
        "\n",
        "This uses `sshleifer/tiny-gpt2` as the teacher and produces `data/sample_text/train.arrow`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Clone the repository & install dependencies\n",
        "\n",
        "Run these commands when starting from a fresh Colab runtime; skip them if the repository is already checked out locally.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/brandf/MegaContext.git\n",
        "%cd MegaContext\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f19afbd3",
      "metadata": {},
      "outputs": [],
      "source": [
        "!git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36efafea",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt\n",
        "!pip install -e .[dev]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Download the Gutenberg subset\n",
        "\n",
        "The helper script grabs a curated <1 GB slice of Project Gutenberg titles. Tweak `tools/download_gutenberg.sh` if you want a different reading list before running the cell below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8154686a",
      "metadata": {
        "id": "fetch-data"
      },
      "outputs": [],
      "source": [
        "!bash tools/download_gutenberg.sh data/raw/gutenberg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Prepare the dataset shard\n",
        "\n",
        "The `configs/data/gutenberg_sample.yaml` configuration reuses the same `sshleifer/tiny-gpt2` teacher with `block_size=32` and `horizon=64`. Adjust `teacher_device`, the horizon length, or dataset paths to match your hardware before executing the next cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6856f90d",
      "metadata": {},
      "outputs": [],
      "source": [
        "!rm -f data/gutenberg_sample/train.arrow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34953497",
      "metadata": {
        "id": "prepare-dataset"
      },
      "outputs": [],
      "source": [
        "%run tools/prepare_dataset.py --config configs/data/gutenberg_sample.yaml\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Larger corpus option\n",
        "\n",
        "For more realistic experiments (still under 1 GB total), rerun the download and prep steps manually:\n",
        "\n",
        "```bash\n",
        "bash tools/download_gutenberg.sh data/raw/gutenberg\n",
        "uv run python -m tools.prepare_dataset --config configs/data/gutenberg_sample.yaml\n",
        "```\n",
        "\n",
        "The Gutenberg subset feeds into the same pipeline and produces `data/gutenberg_sample/train.arrow` for training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7867be0",
      "metadata": {},
      "source": [
        "## 4. Train the GistNet model\n",
        "\n",
        "`tools/train_gistnet.py` now supports multi-phase schedules. The default example runs two phases:\n",
        "\n",
        "- `pooling-pretrain` uses the fast `pooling_mse` objective for 200 steps to match the mean teacher hidden state per block.\n",
        "- `delta-finetune` switches to `delta_nll` for another 200 steps, measuring how well gists preserve the frozen base model’s loss.\n",
        "\n",
        "You can add, remove, or reorder phases in `training.phases`, adjusting `max_steps`, learning rates, or window sizes to taste. If you omit `phases`, the script falls back to a single objective selected via the config or `--objective` flag. When switching machines or the virtualenv, run `uv sync --extra dev` to ensure `pytest`, `pydantic`, and friends install into `.venv` before invoking training or tests.\n",
        "\n",
        "Run the trainer from the repository root:\n",
        "\n",
        "```bash\n",
        "uv run python -m tools.train_gistnet     --dataset data/sample_text/train.arrow     --config configs/runs/gistnet_example.yaml\n",
        "```\n",
        "\n",
        "The sample configuration enables sequential pooling + ΔNLL training, matching the Gutenberg shard defaults (`block_size=32`, hidden size 960) and targeting the MobileLLM teacher. Toggle individual phases or override the objective at the CLI (e.g., `--objective pooling_mse`) when you want a quicker baseline run. Throttle `max_steps` or `batch_size` if you need a faster smoke run on smaller GPUs. In this notebook we invoke the same script with extra flags so metrics and plots persist automatically.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train-model"
      },
      "outputs": [],
      "source": [
        "%run tools/train_gistnet.py \\\n",
        "    --dataset data/gutenberg_sample/train.arrow \\\n",
        "    --config configs/runs/gistnet_example.yaml \\\n",
        "    --metrics-path artifacts/gistnet/metrics.json \\\n",
        "    --save-plot artifacts/gistnet/loss.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Logging & visualisation\n",
        "\n",
        "- Progress defaults to a `tqdm` bar in notebook environments; disable it with `--no-tqdm` if you prefer plain logs.\n",
        "- Add `--metrics-path artifacts/gistnet/metrics.json` to dump raw losses for custom plotting or notebooks.\n",
        "- Use `--save-plot artifacts/gistnet/loss.png` to emit a ready-made curve (falls back gracefully if `matplotlib` is missing).\n",
        "- CLI runs on infra like Novita can pass `--use-wandb --wandb-project <name>` to stream metrics without notebook changes.\n",
        "- When the script detects a notebook runtime (Colab, JupyterLab), it renders the loss curve inline and still saves the PNG so headless runs can inspect it later.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Next steps\n",
        "\n",
        "* Run the ΔNLL smoke eval (coming in Task 2.4).\n",
        "* Push metrics & checkpoints to W&B or Novita storage with `--use-wandb`.\n",
        "* Swap `configs/runs/gistnet_example.yaml` for a larger hidden size when you have a bigger teacher.\n",
        "* Benchmark `pooling_mse` vs `delta_nll` objectives to understand substitutability trade-offs.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "gistnet.ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

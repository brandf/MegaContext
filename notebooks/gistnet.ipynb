{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GistNet Training Guide\n",
        "\n",
        "This notebook consolidates the minimal data and training pipeline for the Phase 2 gist compressor. It replaces the standalone `docs/gistnet.md` so the dataset prep, trainer commands, and logging tips live alongside the executable steps.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/<your-org>/MegaContext/blob/main/notebooks/gistnet.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Fields\n",
        "\n",
        "`tools/prepare_dataset.py` emits Arrow shards with the following columns:\n",
        "\n",
        "- `input_ids` \u2014 L0 token ids for the block being compressed (`block_size` tokens).\n",
        "- `attention_mask` \u2014 mask for the block (currently all ones).\n",
        "- `context_input_ids` \u2014 flattened horizon window (`horizon` tokens) used when the teacher model produced cached embeddings.\n",
        "- `context_attention_mask` \u2014 mask for the horizon window.\n",
        "- `teacher_hidden` \u2014 cached teacher embeddings with shape `[block_size, teacher_hidden_size]` stored using the configured dtype (auto \u2192 float16 on T4, bfloat16 on bf16-capable GPUs, otherwise float32).\n",
        "- `gist_target` \u2014 pooled target vector (mean of the teacher hidden states) emitted in the same dtype as `teacher_hidden`.\n",
        "\n",
        "The metadata stored in `data/<dataset>/metadata.yaml` records the tokenizer, block size, horizon, teacher model/dtype, and per-split statistics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Minimal sample dataset\n",
        "\n",
        "If you just need a smoke test, generate the sample Arrow shard with:\n",
        "\n",
        "```bash\n",
        "uv run python -m tools.prepare_dataset --config configs/data/sample_text.yaml\n",
        "```\n",
        "\n",
        "This uses `sshleifer/tiny-gpt2` as the teacher and produces `data/sample_text/train.arrow`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Clone the repository & install dependencies\n",
        "\n",
        "Run these commands when starting from a fresh Colab runtime; skip them if the repository is already checked out locally.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/brandf/MegaContext.git\n",
        "%cd MegaContext\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f19afbd3",
      "metadata": {},
      "outputs": [],
      "source": [
        "!git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36efafea",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt\n",
        "!pip install -e .[dev]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Download the Gutenberg subset\n",
        "\n",
        "The helper script grabs a curated <1 GB slice of Project Gutenberg titles. Tweak `tools/download_gutenberg.sh` if you want a different reading list before running the cell below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8154686a",
      "metadata": {
        "id": "fetch-data"
      },
      "outputs": [],
      "source": [
        "!bash tools/download_gutenberg.sh data/raw/gutenberg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Prepare the dataset shard\n",
        "\n",
        "The `configs/data/gutenberg_sample.yaml` configuration reuses the same `sshleifer/tiny-gpt2` teacher with `block_size=32` and `horizon=64`. Adjust `teacher_device`, the horizon length, or dataset paths to match your hardware before executing the next cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6856f90d",
      "metadata": {},
      "outputs": [],
      "source": [
        "!rm -f data/gutenberg_sample/train.arrow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34953497",
      "metadata": {
        "id": "prepare-dataset"
      },
      "outputs": [],
      "source": [
        "%run tools/prepare_dataset.py --config configs/data/gutenberg_sample.yaml\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Larger corpus option\n",
        "\n",
        "For more realistic experiments (still under 1 GB total), rerun the download and prep steps manually:\n",
        "\n",
        "```bash\n",
        "bash tools/download_gutenberg.sh data/raw/gutenberg\n",
        "uv run python -m tools.prepare_dataset --config configs/data/gutenberg_sample.yaml\n",
        "```\n",
        "\n",
        "The Gutenberg subset feeds into the same pipeline and produces `data/gutenberg_sample/train.arrow` for training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train the GistNet model\n",
        "\n",
        "`tools/train_gistnet.py` provides a thin training loop that:\n",
        "\n",
        "1. Loads a single Arrow shard into memory.\n",
        "2. Instantiates `GistNet` from a YAML/JSON config block.\n",
        "3. Runs an MSE reconstruction objective between the predicted gist and the cached target vector.\n",
        "4. Saves a checkpoint to `artifacts/gistnet/gistnet.pt`.\n",
        "\n",
        "Run the trainer from the repository root:\n",
        "\n",
        "```bash\n",
        "uv run python -m tools.train_gistnet     --dataset data/sample_text/train.arrow     --config configs/runs/gistnet_example.yaml\n",
        "```\n",
        "\n",
        "The sample configuration matches the Gutenberg shard defaults (`block_size=32`, hidden size 960) and targets the MobileLLM teacher. Throttle `max_steps` or `batch_size` if you need a faster smoke run on smaller GPUs. In this notebook we invoke the same script with extra flags so metrics and plots persist automatically.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train-model"
      },
      "outputs": [],
      "source": [
        "%run tools/train_gistnet.py \\\n",
        "    --dataset data/gutenberg_sample/train.arrow \\\n",
        "    --config configs/runs/gistnet_example.yaml \\\n",
        "    --metrics-path artifacts/gistnet/metrics.json \\\n",
        "    --save-plot artifacts/gistnet/loss.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Logging & visualisation\n",
        "\n",
        "- Progress defaults to a `tqdm` bar in notebook environments; disable it with `--no-tqdm` if you prefer plain logs.\n",
        "- Add `--metrics-path artifacts/gistnet/metrics.json` to dump raw losses for custom plotting or notebooks.\n",
        "- Use `--save-plot artifacts/gistnet/loss.png` to emit a ready-made curve (falls back gracefully if `matplotlib` is missing).\n",
        "- CLI runs on infra like Novita can pass `--use-wandb --wandb-project <name>` to stream metrics without notebook changes.\n",
        "- When the script detects a notebook runtime (Colab, JupyterLab), it renders the loss curve inline and still saves the PNG so headless runs can inspect it later.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Next steps\n",
        "\n",
        "* Run the \u0394NLL smoke eval (coming in Task 2.4).\n",
        "* Push metrics & checkpoints to W&B or Novita storage with `--use-wandb`.\n",
        "* Swap `configs/runs/gistnet_example.yaml` for a larger hidden size when you have a bigger teacher.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "gistnet.ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

# MegaContext â€” Learned Context Compression & Focus for Frozen LLMs

*A system architecture for virtualized LLM memory. This document is both a conceptual overview and a technical design spec for an initial proof-of-concept (POC).*

---
## TL;DR â€” MegaContext
MegaContext is a proposed system architecture for virtualized LLM context - think â€œMegaTexture for text.â€, if you're familiar with this graphics concept.

It separates a modelâ€™s context into a lifetime context (a hierarchical gist tree stored on disk) and a working context (a fixed-size mix of tokens and gists on GPU).  A standard (even pre-trained) LLM then operates on the working context.

A lightweight learned LensNet (and streaming Allocator) continuously/incrementally refocus the full lifetime context onto the working context, giving the model effectively infinite memory at constant compute.

---

## What is MegaContext?

Large language models are constrained by a fixed context window.  
MegaContext removes this limit by separating:

- **Lifetime context** â€” the complete interaction or document history (potentially millions or billions of tokens) stored as a *hierarchical gist tree* on disk or in RAM.  
- **Working context** â€” a small, fixed-size slice of that history (e.g., 8kâ€“32k tokens) mixed from raw tokens and learned gists, fed to the frozen LLM for each decoding step.

### Analogy: MegaTexture â†’ MegaContext
This is not required to understand MegaContext, but for those that are interested in learning about the inspiration [this video](https://www.youtube.com/watch?v=BiQCz2NjPR8) provides a good overview of the problems Mega Texture solves.
- In graphics, **MegaTexture** streams the visible portions of a vast texture map into GPU memory at appropriate resolution.  
- **MegaContext** does the same for text: only the high-resolution â€œtilesâ€ (recent or relevant spans) are loaded into the modelâ€™s working memory, while distant regions stay represented by coarse gists.

### Intuitions / Motivation
The core intuition that's motivating this work is that long context is only useful if the model can focus on the relevant parts and ignore distractors (efficiently).  
- "Relevant parts" is inherently non-causal (something that wasn't previously relevant can become relevant), so this implies dynamic focusing/defocusing.  One-way compression/summarization schemes are fundamentally flawed.
- Exciting new future LLM scenarios will be unlocked at 100M+ context lengths, and at this scale both memory and compute requirements must be sub-linear to be practical for widespread consumer applications.

---

## Grand vision: why this matters

The POC will prove the mechanism, but the broader implications are transformative:

### â™¾ï¸ Virtually infinite memory
Lifetime context can grow unbounded while per-step compute and GPU RAM remain constant.  A conversation could persist for years without retraining or forgetting.

### ğŸ§© Smaller, smarter models
An LLM trained end-to-end with MegaContext could shift parameter budget away from memorized facts toward reasoning, abstraction, and planning.  
Knowledge lives in the *lifetime memory* instead of the weights.

### ğŸ’» Agentic coding & persistent tasks
Today, agents rely on brittle, lossy context management (manual summarization, sub-agents, RAG hacks).  
MegaContext treats context management as a **first-class architectural component**, allowing seamless long-term reasoning and creative iteration.

### ğŸŒ Core knowledge as dynamic system prompt
Shipping LLMs with a **core lifetime context** transforms in-context learning:  
the model boots with a massive â€œsystem promptâ€ of structured world knowledge that updates externally and without retraining weights.  
- A cloud-hosted MegaContext model could refresh its understanding of the world continually, combining retrieval and reasoning in a unified pipeline.
- An agentic coding system could provide an entire codebase as a system prompt (lifetime context), eliminating the expensive / error prone processes of reading parts of the projects code.

---

## Components

### 1ï¸âƒ£ Lifetime gist tree
Built incrementally as text streams in:
- Every 32 tokens â†’ 1 gist (Level 1).  
- Every 32 Level-1 gists â†’ 1 Level-2 gist.  
- etc.

### 2ï¸âƒ£ Working context
A fixed-size mixture of raw tokens and gists forming a contiguous window over the lifetime tree.  
- At any step, its total token cost â‰¤ `W_max` (e.g., 8 k).
- The base LLM operates only on this context

---
## System overview

```
Streaming text  â”€â”€â–º 1ï¸âƒ£ Lifetime Gist Tree  â”€â”€â–º  Allocator â”€â”€â–º 2ï¸âƒ£ Working Context  â”€â”€â–º  Frozen Base LLM â”€â”€â–º Next Token Prediction
                               â–²                    â–²                â”‚   â”‚  â–²                                  â”‚
                               â”‚                    â”•â”â”â”â”â”LensNetâ”â”â”â”â”›   â”‚  â”•â”â”â”â”â”â”â”â”â”â”Auto Regressionâ”â”â”â”â”â”â”â”â”â”›
                               â”•â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”GistNetâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›
                          
```

---

## POC scope & constraints

- **Frozen base LLM** no fine-tuning initially, with LoRA finetuning as a follow up  
- **Two-level Lifetime gist tree:** The POC will be limited to moderate sized contexts so only 2 laters should be sufficient   
- **Synchronous updates.** Lifetime tree lives in RAM/GPU for POC (rather than disk); updates happen between autoregressive steps.
  
---

## Performance sketch

| Setup | Lifetime tokens | Active tokens | KV-cache | Disk I/O / step | Notes |
|-------|-----------------|----------------|-----------|-----------------|-------|
| **Vanilla LLM** | 32 k | 32 k | ~2 GB | n/a | context-limited |
| **MegaContext (POC)** | ~1 M | 8 k | ~0.5 GB | few MB | constant compute per step |
| **MegaContext (Future)** | 1 B+ | 32 k | ~2 GB | 10â€“50 MB/s | fully trained base model |

Per-step compute â‰ˆ base decode cost; gist extraction and LensNet overhead < 1 %.

### Long-term storage example: lifetime memory for a 24/7 robot (10 years)

**Assumptions**

- **Sampling:** 500 feature vectors / sec  
- **Feature size:** 4,096-dim; stored as fp16 (2 bytes) unless noted  
- **Duration:** 10 years â‰ˆ 3.1536Ã—10â¸ seconds â‡’ **N = 500 Ã— 3.1536Ã—10â¸ â‰ˆ 1.5768Ã—10Â¹Â¹** leaf vectors  
- **Tree arity:** 32â†’1 at each level (no overlap)  
- **Tree depth:** logâ‚ƒâ‚‚(N) â‰ˆ **8 levels** (root near level 8)  
- **Node payload:** one vector per node (same width as leaves, different precision per scenario)

#### Storage breakdown

| Scenario | Estimated storage | How to read this |
|---|---:|---|
| **Raw leaves only (fp16)** | **~1.29 PB** | N Ã— 4096 Ã— 2 bytes = 1.5768e11 Ã— 8192 B |
| **Full 32-ary tree at fp16 (leaves + *all* internal levels)** | **~1.33 PB** | Geometric factor for all nodes: (1 + 1/32 + 1/32Â² + â€¦) = 32/31 â‰ˆ **1.032Ã—** overhead over leaves |
| **Full tree, 8-bit everywhere** | **~667 TB** | Leaves 8-bit: ~646 TB; internal nodes count = N/31 â‰ˆ 5.09e9; internal 8-bit adds ~20.8 TB; total â‰ˆ 646 + 20.8 |
| **Pruned: keep only 1% of leaves @ 8-bit; keep *all* internal nodes @ 8-bit** | **~27 TB** | Leaves: 0.01 Ã— 646 TB â‰ˆ 6.46 TB; internal 8-bit â‰ˆ 20.8 TB; total â‰ˆ 27.3 TB |
| **Pruned + compressed: 1% leaves @ 8-bit with entropy coding (~Ã—0.5); internal @ 8-bit with entropy coding (~Ã—0.5)** | **~13â€“14 TB** | Leaves â‰ˆ 3.2 TB + internal â‰ˆ 10.4 TB |
| **More aggressive: 0.5% leaves @ 8-bit + entropy (~Ã—0.5); internal @ 4-bit + entropy (~Ã—0.5)** | **~6â€“8 TB** | Leaves: 0.005 Ã— 646 TB Ã— 0.5 â‰ˆ **1.6 TB**; internal: (20.8 TB Ã— 0.5 for 4-bit) Ã— 0.5 entropy â‰ˆ **5.2 TB**; total ~6.8 TB |

**Key takeaways**

- A **full 32-ary tree** only adds ~**3.2%** storage over leaves when stored at the **same precision** (factor 32/31), so multilevel LOD itself is cheap; **precision and pruning dominate** total footprint.  
- With **8-bit quantization** and **reasonable pruning** of raw leaves (e.g., keep only salient 0.5â€“1%), plus straightforward **entropy coding**, **a decade of continuous 500 Hz, 4k-dim features** compresses to **single-digit TBs**â€”practical for local SSD arrays.  
- This makes a **lifelong, high-bandwidth memory** feasible: raw details can be recovered where preserved; elsewhere, multilevel gists maintain global context with the **working context** handling on-demand re-expansion.

---

## GistNet â€” local gist extraction (32â†’1, two-layer tree)

### Purpose
GistNet replaces short, fixed-length token sequences with compact **gist embeddings** ("gists") that can stand in for their original tokens inside the base LLMâ€™s context.  
Each gist preserves the meaning of its 32-token span while freeing token budget for new information.  
Stacking two 32â†’1 layers provides **1024Ã— compression** in the proof of concept (POC).

---

### Inputs & outputs
| Symbol | Shape | Meaning |
|---------|--------|---------|
| `E âˆˆ R[32, d]` | 32 raw token embeddings (no contextualization) |
| `Gâ‚€ âˆˆ R[K, d]` | learnable slot query (`K=1` for 32â†’1) |
| `g* âˆˆ R[d]` | single gist vector aligned with base LLM embedding dim |

---

### POC architecture (32â†’32â†’1â†’32â†’32â†’1)

GistNet alternates **self-attention** and **cross-attention** to gradually compress and refine each 32-token span.

#### Stage 1 â€” Local token self-attention (32 â†’ 32)
- Apply 1â€“2 standard self-attention + MLP blocks within the 32-token window.  
- Add RoPE or sinusoidal positional encodings for local ordering.  
- Output is `E1`, a locally contextualized version of the raw embeddings.

#### Stage 2 â€” Compression (32 â†’ 1)
- Introduce a single learned slot query `Gâ‚€`.  
- Perform cross-attention where the slot reads from the tokens:  

```
G1 = CrossAttn(query=G0, key=E1, value=E1)
G1 = G1 + MLP(LN(G1)) # residual + feedforward
```

- `G1` is the first gist embedding for this 32-token span.

#### Stage 3 â€” Expansion (1 â†’ 32)
- Expand information back into the 32-token space for refinement:  

```
E2 = CrossAttn(query=E1, key=G1, value=G1)
E2 = E1 + MLP(LN(E2))
```

- Optionally run one self-attention block over `E2` to diffuse the gist info across tokens.

#### Stage 4 â€” Final compression (32 â†’ 1)
- Run a second cross-attention with a fresh slot query derived from `G1`:  

```
G2 = G1 + Îµ
g_star = CrossAttn(query=G2, key=E2, value=E2)
g_star = LN(MLP(g_star))
```
- The result `g_star` is the final gist vector for the span and becomes a node in the lifetime gist tree.

#### Stage 5 â€” Hierarchical stacking
- Two 32â†’1 layers are stacked hierarchically (32Â² = 1024 tokens per top-level gist).  
- The lower layer runs directly on token embeddings; the upper operates on lower-layer outputs.


### Architectural properties
| Property | Description |
|-----------|--------------|
| **Causal scope** | Operates strictly within 32-token windows; no long-range attention. |
| **Parameter sharing** | Shared weights across all spans; upper and lower layers may share or specialize. |
| **Complexity** | O(32Â²Â·d) per span â€” negligible compared to the base LLM. |
| **Dimensionality** | Outputs match the base modelâ€™s embedding size `d`. |
| **Positioning** | Gist inserted at the central token index for RoPE alignment. |
| **Precision** | bf16 or fp16; supports later quantization for storage. |


### Training objectives

#### 1. Substitutability (primary)
Train the model so that replacing a span with its gist minimally changes the base LLMâ€™s predictions.

For each training example:

```
Loss_subst = KL( P_base(x_{t+1:T} | E)
|| P_base(x_{t+1:T} | E_replaced) )
```
or equivalently minimize the Î”NLL between the full and gist-replaced context over a short horizon (H = 32â€“128 tokens).

#### 2. Reconstruction (optional)
Encourage the gist to preserve enough information to approximate its original tokens:

```
Loss_recon = || E_reconstructed - E ||^2
```

#### 3. Contrastive span separation (optional)
Discourage neighboring spans from collapsing to identical gists:

```
Loss_contrast = max(0, margin - cosine_similarity(g_i*, g_j*))
```
for adjacent spans (margin â‰ˆ 0.2).

Total loss:
```
Loss = Loss_subst + 0.1 * Loss_recon + 0.05 * Loss_contrast
```

### Implementation details (POC)
| Item | Setting |
|------|----------|
| Window size | 32 tokens |
| Slots | 1 (single learnable query) |
| Layers per 32â†’1 block | 2 self + 2 cross |
| Refinement stack | 32â†’1â†’32â†’1 |
| Embedding dim | same as base LLM (e.g., 4096) |
| Internal hidden width | 512 |
| Attention heads | 8 |
| RoPE | applied to token positions only (slots omit it) |
| Activation | GELU |
| Norm | Pre-LayerNorm |
| Parameters | ~0.5M per layer |
| Output | single `g*` vector per span |
| Runtime | <1 ms per 32-token span on GPU |

### Training pipeline (POC)
1. **Dataset:** long-form text (4kâ€“16k tokens), chunked into 32-token spans.  
2. **Teacher:** frozen base LLM used for Î”NLL@H computation.  
3. **Objective:** minimize Î”NLL@H between original and gist-replaced contexts.  
4. **Curriculum:** start with contiguous text, then include structured data (lists, code, tables).  
5. **Optimizer:** AdamW, lr = 1e-4, cosine decay, bf16 precision.  
6. **Output:** store 32â†’1 and 1024â†’1 gists in the lifetime gist tree for later use by LensNet and Allocator.

### Recap
GistNet is a **local autoencoder for token spans** that learns to produce substitutable embeddings aligned with the base modelâ€™s token space.  
It uses **self- and cross-attention refinement (32â†’1â†’32â†’1)** to compress meaning while remaining directly compatible with the base LLMâ€™s embedding layer.  
Stacked hierarchically, GistNet forms the **Lifetime Gist Tree** that supports scalable, virtualized context in MegaContext.

---

## LensNet â€” how focus is decided

### Why â€œLensâ€?
LensNet acts like an optical lens that dynamically **focuses** and **defocuses** regions within the lifetime context while keeping total compute constant.  
It predicts where to spend detail (expand gists into raw tokens) and where to blur (collapse raw tokens into gists), ensuring that the **fixed-size working context** maintains maximal relevance.

### What it operates on
- LensNet reads the **working context** (not the lifetime tree).  
  It analyzes the embeddings currently fed into the base LLM â€” the only state that resides on GPU.
- It outputs one **focus score** per feature (token span or gist).

### Why non-causal is essential
LensNet must understand *future queries* to know which past facts matter.

**Example**
```
C1: "My shirt is red. My pants are green."
C2: "My shirt is red. My pants are green. What color hat would match my shirt?"
```


Because the base LLM is causal, the hidden states for â€œshirtâ€ and â€œpantsâ€ are identical in C1 and C2; they never see the question.  
A non-causal LensNet can look at the full working context (including the query) and boost focus on the â€œshirtâ€ fact.

### Conceptual overview
- LensNet runs independently of the frozen base LLM.  
- It operates directly on the **working context embeddings** (`~8k features`), not on live LLM hidden states.  
- It conditions on a small **gist set** (`L2 + last 5 L1` gists, total â‰ˆ 6) taken from the end of the context, which implicitly encodes the upcoming query/task.  
- The model outputs one **signed focus score** `u_i` per feature:
  - `u_i > 0`: expand / focus (increase detail, go one level down)
  - `u_i < 0`: collapse / defocus (reduce detail, go one level up)

At runtime, the **Allocator** interprets these scores to expand and collapse spans while keeping the working context within its token budget.

### Why dynamic LOD matters
Traditional compression methods summarize once and lose detail forever.  
MegaContext continually re-evaluates importance: if a previously collapsed region becomes relevant again, it can be expanded back into its children gists or raw tokens.  
Note that this expansion is NOT a lossy decoding of the gist latent - the lifetime context preserves the full token-level details on disk (or in RAM for the POC), so the LLM has full access to the whole lifetime context, just not all at once.
This enables the modelâ€™s effective memory to **evolve over time** as new information arrives.  Similar to how you're now thinking about your first kiss ğŸ˜˜

### Architecture (POC: dual cross-attention LensNet)

1. **Inputs**
   - `X âˆˆ R^{NÃ—d}` â€” embeddings of all features in the working context (â‰ˆ 8 000 tokens or gists).  
   - `G âˆˆ R^{KÃ—d}` â€” six gist embeddings from the tail of the context (L2 + 5 L1).  
   - `Ï†_i` â€” per-feature metadata (level, width, distance to cursor, system/user flags, etc.).  
   - All embeddings are first **down-projected** to a compact LensNet width `d_lens â‰ˆ 512`.

2. **Stage 1 â€” Gists read the context (8k â†’ 6)**  
   Each gist slot attends across all working-context features to build a condensed, query-conditioned representation:

```
tilde_G = Softmax(((G W_Q)(X W_K)^T) / sqrt(d)) * (X W_V)
```

3. **Stage 2 â€” Context reads refined gists (6 â†’ 8k)**  
   The 8 k context features query the six updated gists to broadcast relevance back:

```
tilde_X = Softmax(((X W'_Q)(tilde_G W'_K)^T) / sqrt(d)) * (tilde_G W'_V)
```

4. **Stage 3 â€” Per-feature scoring head**  
   Concatenate each updated context vector `tilde_X_i` with its metadata `Ï†_i` and predict a scalar score:

```
u_i = MLP(LN([tilde_X_i, Ï†_i])) -> R
```

5. **Stacks / refinement**  
   1â€“3 stacked dual-attn blocks may be used for iterative refinement; parameters `(W_Q,K,V)` are the only learned weights.

**Complexity:** O(N Ã— K Ã— d) per pass.  
With N = 8 000, K = 6, d = 512 â‡’ ~25 M mult-adds â€” trivial compared to the base model.

### Update cadence (block-wise refocus)

LensNet runs **once every K tokens** (POC: K = 32).  
During each block update:

1. Gather the latest gists `G`.  
2. Run LensNet to produce signed scores `u_i`.  
3. The Allocator executes expansions/collapses subject to the working-context budget.  
4. The updated context is frozen for the next K tokens.

This matches the intended inference cadence (no per-token recompute).

### Training objectives

#### 1ï¸âƒ£ Signed focus supervision
Each feature receives a **signed target utility** `y_i` derived from counterfactual NLL deltas:

- Expandable items (L1/L2 children) â‡’ positive `y_i > 0`  
- Collapsible spans â‡’ negative `y_i < 0`  
- Others â‡’ 0 / masked.

LensNet learns to regress and rank these utilities.

```
L_reg  = (1 / |M|) * sum_{i in M} (u_i - y_i)^2
L_rank = softplus(-(u_i - u_j))  # for ordered pairs
```

#### 2ï¸âƒ£ Zero-sum budget regularizer
To maintain constant working-context size:

```
P = sum_i c_i_plus * ReLU(u_i)
N = sum_i c_i_minus * ReLU(-u_i)
L_budget = ((P - N) / (eps + P + N))^2
```
(`c_i^+` / `c_i^-` = token cost / refund.)  
This encourages net-zero expand/defocus mass per block.

#### 3ï¸âƒ£ Legality penalties
Prevent impossible actions:

```
L_illegal = alpha * sum_{L0} ReLU(u_i) + beta * sum_{L2} ReLU(-u_i)
```
(alpha, beta â‰ˆ 0.3).  
At inference, invalid directions are hard-masked to 0.

#### 4ï¸âƒ£ Total loss

```
L_total = L_reg + 0.5 * L_rank + 0.1 * L_budget + L_illegal
```

### Inference procedure

1. **Mask** illegal sides (L0 canâ€™t expand; L2 canâ€™t collapse).  
2. **Optional rebalance**: rescale positive/negative masses to match before sending to the Allocator.  
3. **Allocator** greedily applies expand/collapse actions within the token budget, honoring hysteresis rules.

### Summary of POC parameters

| Item | Value / Notes |
|------|----------------|
| Input embeddings | 8 k features (mixed L0/L1/L2) |
| Conditioning gists | 6 (L2 + 5 L1) |
| Down-projection width | 512 |
| Attention heads | 8 |
| Stacks | 1â€“3 |
| Update cadence | every 32 tokens |
| Output | signed focus score `u_i` per feature |
| Runtime | < 3 ms per update @ 8 k tokens |
| Params | â‰ˆ 100 k â€“ 200 k total |


**In short:**  
LensNet is a compact, non-causal controller built as a dual cross-attention network (`8k â†’ 6 â†’ 8k`).  
It runs once per block, predicts balanced signed focus scores for every feature, and guides the Allocator to keep the working context sharp, legal, and budget-neutral.

---

## Joint Training (Alternating / â€œEM-styleâ€): GistNet + LensNet + Base-LoRA

**Goal:** Let all three modules co-adapt without full end-to-end backprop through the discrete Allocator or long unrolls.  
**Method:** Short alternating phases where some modules are frozen while others learn from on-policy signals produced by the frozen parts. Repeat for a few cycles.

### What â€œEM-styleâ€ means here
We alternate optimization across modules:
- **E-like step:** hold policy parts fixed to produce supervision/targets (e.g., counterfactual utilities).
- **M-like step:** update another module to better fit those targets.
Itâ€™s not exact EM; itâ€™s an **alternating optimization schedule** that stabilizes joint training.

### Modules
- **GistNet** `Gist` (32â†’1, two levels; substitutability objective)
- **LensNet** `LensNet` (dual cross-attn 8kâ†’6â†’8k; signed focus scores)
- **Base-LoRA** `LoRA` (tiny adapters on the base LLM to improve gist compatibility)
- **Allocator** is always **discrete greedy** (no relaxation needed)

### Phase B1 â€” Update GistNet (fix LensNet + LoRA)
**Fix:** `LensNet`, `LoRA`, `Allocator`  
**Update:** `Gist`

**Procedure (on-policy):**
1. Build/refresh lifetime trees with current `Gist`.
2. For each training block (size K=32): run `LensNet` + `Allocator` to pick expands/collapses; form the working context used by the base LLM.
3. Optimize **GistNet** on spans touched in this block using:
   - **Substitutability loss**: KL(full || replaced) or Î”NLL@H (H=32â€“128) for the gist that *was actually* inserted.
   - **Stability loss** (optional): L2 between current gist and previous checkpoint to avoid drift.
   - **Boundary aux** (optional): light reconstruction on edge tokens.

**Intuition:** With the current focusing policy fixed, make gists better drop-in replacements for *exactly the places the policy cares about*.

### Phase B2 â€” Update LensNet (fix GistNet + LoRA)
**Fix:** `Gist`, `LoRA`  
**Update:** `LensNet`

**Procedure:**
1. Using the fixed `Gist`, generate **counterfactual labels** on on-policy snapshots:
   - For candidate expands/collapses in the current working context, compute Î”NLL/Î”KL (batched).
   - Convert to **signed utility per token** (expand positive; collapse negative).
2. Train `LensNet` with:
   - **Signed regression + ranking** (within snapshot)
   - **Zero-sum budget** regularizer (token-cost weighted)
   - **Legality** penalties; keep runtime masking
   - **Update-every-K** cadence (Lens runs once per block)

**Intuition:** Given the current gists, learn a better focusing policy.


### Phase B3 â€” Update Base-LoRA (fix GistNet + LensNet)
**Fix:** `Gist`, `LensNet`  
**Update:** `LoRA` (small ranks; keep it tiny)

**Where to place LoRA (recommended):**
- Input embedding projection
- QKV/O of the **first 2 attention blocks** *or* the **last 2** (pick one set; not both)

**Losses:**
- **Task NLL@H** with the *discrete* working context produced by `LensNet+Allocator`
- **Substitutability keep-alive** (weak): prevents gist semantics drifting away from what the base understands
- (Optional) **KL to teacher** if you have a larger teacher-with-MegaContext

**Intuition:** Slightly adapt the base to â€œlikeâ€ gist tokens and the current WC geometry (positional anchoring, variance, etc.).


### Schedule & Hyperparameters

- **Cycle length:** B1 â†’ B2 â†’ B3 = **one cycle**. Repeat **3â€“5 cycles**.
- **Step counts per phase (per cycle):**  
  - B1 (GistNet): 2â€“4k steps  
  - B2 (LensNet): 2â€“4k steps  
  - B3 (LoRA): 1â€“2k steps  
- **Batching:** mixed long-context tasks; block size K=32; horizon H=64.
- **Optimizers:** AdamW (bf16), cosine LR with warmup per phase.
- **Checkpoints:** save after each phase; early-stop on validation **Loss@H vs. token-budget**.


### Data flow per cycle (pseudo)

1. **B1:**  
   - Freeze `LensNet`, `LoRA`.  
   - Decode blocks with current WC (from LensNet+Allocator).  
   - Update `Gist` using on-policy substitutability losses on the replaced spans.

2. **B2:**  
   - Freeze `Gist`, `LoRA`.  
   - From the same blocks, compute counterfactual utilities (expand/collapse candidates).  
   - Update `LensNet` with signed utilities + budget/legality losses.

3. **B3:**  
   - Freeze `Gist`, `LensNet`.  
   - Run normal blocks (LensNet+Allocator active) and update `LoRA` on Task NLL@H (+ weak substitutability keep-alive).


### Stability & efficiency tips

- **Warm starts:** Do a short **sequential pretrain** (GistNet then LensNet) before the first B1; it reduces early oscillations.
- **Small LoRA ranks:** r=4â€“16, low LR; the goal is interface alignment, not knowledge injection.
- **Hysteresis in Allocator:** min residency steps to prevent expand/collapse thrash during B2/B3.
- **On-policy labeling:** Always regenerate Î”NLL labels *after* the last B1 so LensNet trains on current gists.
- **Curriculum:** start with narrative/doc tasks; add lists/tables/code once stable.
- **Telemetry:** track (a) Loss@H vs budget, (b) swap rate, (c) residency time, (d) non-causal C1/C2 tests.


### When to stop
- **Validation Loss@H vs budget** improves then plateaus across cycles.
- **Swap rate** stabilizes; no ping-pong.
- **Ablations:** freezing any one of {GistNet, LensNet, LoRA} now causes a measurable drop.

**Outcome:** All three modules co-learn: **GistNet** encodes what the policy needs, **LensNet** chooses expansions that actually help, and **LoRA** nudges the base LLM to be friendlier to mixed-LOD inputsâ€”without the cost/fragility of full end-to-end training.


---

## Comparison: MegaContext vs. RAG

| Aspect | RAG | MegaContext |
|---------|-----|-------------|
| **Storage** | External documents, often text chunks in a vector DB | Hierarchical learned gists (vectors) directly aligned to the modelâ€™s lifetime |
| **Retrieval trigger** | Query-time semantic search | Continuous, learned focus from LensNet |
| **Integration** | Concatenate retrieved text to prompt | Replace/expand in working context with proper positional encoding |
| **Training** | Separate retriever / generator | Single substitutability & focus training |
| **Memory type** | Stateless look-up | Persistent evolving memory with reversible summarization |

MegaContext is *structurally* similar to RAG in that both pull relevant data into a fixed context, but differs fundamentally: it treats compression and focus as an **integrated learned process** rather than retrieval over external text.

---

## Training data & streaming behavior

- **GistNet training:** any long-form corpus; each 32-token window provides (full vs gist) pairs.  
- **LensNet training:** logged working-context snapshots from real LLM runs.  Counterfactual losses (`expand`/`collapse`) computed offline.  
- **Streaming:** as new tokens arrive, the system:  
  1. Buffers 32 tokens â†’ creates new L1 gist.  
  2. When 32 L1s exist â†’ create L2 gist.  
  3. LensNet+Allocator decide which regions to expand/collapse before the next decode step.

Gists can be serialized as fp16 or quantized vectors (e.g., 8-bit) with metadata JSON.

---

## Evaluation plan

- **Perplexity vs. token budget** (loss @ Horizon).  
- **Causal vs. non-causal LensNet** on C1/C2-style tests.  
- **Boundary artifacts** (information split across spans).  
- **Stress test** at 1024Ã— compression.  
- **Memory & compute traces** verifying constant per-step cost.

---

## Related work

| Concept | Reference | Relevance |
|----------|------------|------------|
| MegaTexture (id Software, 2007) | Virtualized textures | Direct analogy |
| Perceiver / Perceiver IO (DeepMind 2021-22) | Latent cross-attention | Architectural similarity |
| Slot Attention (Locatello 2020) | Object-like latent slots | GistNet inspiration |
| Compressive Transformer (Rae 2019) | Long-term compressed memory | Temporal analog |
| Gist tokens / LLMLingua 2 (2023-24) | Prompt compression | Substitutability idea |
| RAG / Retrieval-Augmented Generation | Search-based retrieval | Conceptual cousin |
| MegaContext (this work) | â€” | Unified learned compression + focus over frozen LLMs |

---

## Implementation roadmap

1. **32â†’1 GistNet** â€” implement & train substitutability.  
2. **Lifetime Tree Builder** â€” streaming, 2-level hierarchy in RAM.  
3. **LensNet v1 (non-causal)** â€” implement query-conditioned scorer, train on offline labels.  
4. **Allocator** â€” greedy expand/collapse, hysteresis.  
5. **E2E POC** â€” run step-loop (score â†’ allocate â†’ update â†’ decode).  
6. **Evaluate** â€” loss vs budget, C1/C2 relevance, stress tests.

---

## Future directions

- Async disk streaming of the lifetime tree.  
- RL-trained allocator optimizing accuracy Ã— latency.  
- Multi-token gists for structured data.  
- Joint training of LLM + MegaContext from scratch.  
- Shared or federated lifetime memories between agents.

---

## License & contributions

MIT License (suggested).  
PRs welcome â€” please include reproducible tests for GistNet, LensNet, allocator, and end-to-end demos.

---

*MegaContext virtualizes sequence memory just as MegaTexture virtualized textures â€” focusing detailed computation only where needed.  
It opens a path to persistent, updatable, and truly lifelong language models.*

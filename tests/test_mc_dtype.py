import pytest

torch = pytest.importorskip("torch", reason="PyTorch is required for MC tests")
import torch.nn as nn

from mc.config import MCConfig, WorkingContextConfig
from mc import runtime as mc_runtime
from mc.runtime import MCController, WorkingContextVariant
from mc.working_context import WorkingContext


class DummyTransformer(nn.Module):
    def __init__(self, vocab_size: int, embed_dim: int) -> None:
        super().__init__()
        self.wte = nn.Embedding(vocab_size, embed_dim)


class DummyModel(nn.Module):
    def __init__(self, vocab_size: int, embed_dim: int) -> None:
        super().__init__()
        self.transformer = DummyTransformer(vocab_size, embed_dim)
        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)

    def forward(self, idx, targets=None, **kwargs):
        x = self.transformer.wte(idx) if kwargs.get("inputs_embeds") is None else kwargs["inputs_embeds"]
        return self.lm_head(x)

class DummyReport:
    def log(self, *args, **kwargs):
        return


def test_variant_forward_handles_bf16_embeddings(monkeypatch):
    vocab = 32
    embed_dim = 8
    model = DummyModel(vocab, embed_dim).float()
    cfg = MCConfig(
        embed_dim=embed_dim,
        max_seq_len=16,
        block_size=2,
        device="cpu",
        allocator_recent_tokens=0,
        num_heads=1,
    )
    monkeypatch.setattr(mc_runtime, "get_report", lambda: DummyReport())
    controller = MCController(model, cfg)
    # Force embeddings generated by the controller to be bf16 even though the model is fp32.
    controller.embed.weight.data = controller.embed.weight.data.to(torch.bfloat16)
    wc_config = WorkingContextConfig(embed_dim=embed_dim, max_length=8, device="cpu")
    embeddings = torch.randn(1, 4, embed_dim, dtype=torch.bfloat16)
    positions = torch.arange(4, dtype=torch.long).unsqueeze(0)
    wc = WorkingContext(embeddings, positions, wc_config)
    controller._configure_wc_positional(wc)
    variant = WorkingContextVariant(working_context=wc, source="test", lod_hint=0, batch_index=0)
    tokens = torch.randint(0, vocab, (1, 4))
    loss = controller._run_variant_forward(variant, tokens)
    assert isinstance(loss, torch.Tensor)
    assert loss.dtype == controller._target_dtype
